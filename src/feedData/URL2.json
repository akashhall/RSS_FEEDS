{
  "status": "ok",
  "feed": {
    "url": "https://aws.amazon.com/blogs/big-data/feed/",
    "title": "AWS Big Data Blog",
    "link": "https://aws.amazon.com/blogs/big-data/",
    "author": "",
    "description": "Official Big Data Blog of Amazon Web Services",
    "image": ""
  },
  "items": [
    {
      "title": "Visualize over 200 years of global climate data using Amazon Athena and Amazon QuickSight",
      "pubDate": "2019-02-13 17:42:30",
      "link": "https://aws.amazon.com/blogs/big-data/visualize-over-200-years-of-global-climate-data-using-amazon-athena-and-amazon-quicksight/",
      "guid": "3788d34011b4db8192171e0ffb5e5461e194fb2a",
      "author": "Joe Flasher",
      "thumbnail": "https://d2908q01vomqb2.cloudfront.net/b6692ea5df920cad691c20319a6fffd7a4a766b8/2019/02/07/VisualizeClimateDataAthenaQuickSight1.png",
      "description": "Climate Change continues to have a profound effect on our quality of life. As a result, the investigation into sustainability is growing. Researchers in both the public and private sector are planning for the future by studying recorded climate history and using climate forecast models. To help explain these concepts, this post introduces the Global […]",
      "content": "\n<p>Climate Change continues to have a profound effect on our quality of life. As a result, the investigation into sustainability is growing. Researchers in both the public and private sector are planning for the future by studying recorded climate history and using climate forecast models.</p> \n<p>To help explain these concepts, this post introduces the <a href=\"https://registry.opendata.aws/?search=ghcn\" target=\"_blank\" rel=\"noopener\">Global Historical Climatology Network Daily (GHCN-D).</a> This registry is used by the global climate change research community.</p> \n<p>This post also provides a step-by-step demonstration of how Amazon Web Services (AWS) services improve access to this data for climate change research. Data scientists and engineers previously had to access hundreds of nodes on high-performance computers to query this data. Now they can get the same data by using a few steps on AWS.</p> \n<h2>Background</h2> \n<p>Global climate analysis is essential for researchers to assess the implications of climate change on the Earth’s natural capital and ecosystem resources. This activity requires high-quality climate datasets, which can be challenging to work with because of their scale and complexity. To have confidence in their findings, researchers must be confident about the provenance of the climate datasets that they work with. For example, researchers may be trying to answer questions like: has the climate of a particular food producing area changed in a way that impacts food security? They must be able to easily query authoritative and curated datasets.</p> \n<p>The <a href=\"https://www.ncei.noaa.gov/\" target=\"_blank\" rel=\"noopener\">National Centers for Environmental Information (NCEI)</a> in the U.S. maintains a dataset of climate data that is based on observations from weather stations around the globe. It’s the Global Historical Climatology Network Daily (GHCN-D) — a central repository for daily weather summaries from ground-based stations. It is comprised of millions of quality-assured observations that are updated daily.</p> \n<p>The most common parameters recorded are daily temperatures, rainfall, and snowfall. These are useful parameters for assessing risks for drought, flooding, and extreme weather.</p> \n<h2>The challenge</h2> \n<p>The NCEI makes the GHCN_D data available in CSV format through an FTP server, organized by year. Organizing the data by year means that a complete copy of the archive requires over 255 files (the first year in the archive is 1763). Traditionally, if a researcher wants to work on this dataset they must download it and work on it locally. For a researcher to be sure of using the latest data for their analysis, they must repeat this download every day.</p> \n<p>For researchers, deriving insight from this data can be a challenge. They must be able to fully engage with the data, because that requires technical skill, computing resources, and subject matter expertise.</p> \n<h2>A new efficient approach</h2> \n<p>Through AWS’s collaboration with the NOAA Big Data Project, a daily snapshot of the GHCN_D dataset is now available on AWS. The data is publically accessible through an Amazon S3 bucket. For more information, see the <a href=\"https://registry.opendata.aws/noaa-ghcn/\" target=\"_blank\" rel=\"noopener\">Registry of Open Data on AWS</a>.</p> \n<p>Having the data available in this way offers several advantages:</p> \n<ul>\n<li>\n<strong>The data is globally available to a community of users</strong>. Users no longer must download data to work on it. Everyone can work with the same, authoritative copy.</li> \n <li>\n<strong>Time to insight is reduced</strong>. By taking advantage of AWS services, researchers can immediately start to perform analysis.</li> \n <li>\n<strong>The cost of research is reduced</strong>. Researchers can switch off resources as soon as their analysis is finished.</li> \n</ul>\n<p>This blog post illustrates a workflow using <a href=\"https://aws.amazon.com/s3/?nc2=h_m1\" target=\"_blank\" rel=\"noopener\">Amazon S3</a>, <a href=\"https://aws.amazon.com/athena/?nc2=h_m1\" target=\"_blank\" rel=\"noopener\">Amazon Athena</a>, <a href=\"https://aws.amazon.com/glue/?hp=tile&amp;so-exp=below\" target=\"_blank\" rel=\"noopener\">AWS Glue</a>, and <a href=\"https://aws.amazon.com/quicksight/?nc2=h_m1\" target=\"_blank\" rel=\"noopener\">Amazon QuickSight</a> that demonstrates how quickly one can derive insights from this dataset.</p> \n<p>The workflow presented in this post follows these general steps:</p> \n<ul>\n<li>Extract data files from the NOAA bucket and make the data available as tables.</li> \n <li>Use SQL to query the data contained in the tables.</li> \n <li>Show how to speed up analysis by creating tables from queries and storing those tables in a private Amazon S3 bucket.</li> \n <li>Visualize the data to gain insight.</li> \n</ul>\n<h2>Overview of the GHCN_D dataset</h2> \n<p>The GHCN-D is a quality-assured dataset that contains daily weather summaries from weather stations across global land areas. It has the following properties:</p> \n<ul>\n<li>Data is integrated from approximately 30 sources that provide weather observations from various national and international networks.</li> \n <li>A comprehensive dataset for the US and good coverage for many parts of the world.</li> \n <li>There are many types of daily weather observations in this dataset, but the majority are maximum temperature, minimum temperature, precipitation, snow fall, and snow depth. These observations include: \n  <ul>\n<li>Over 35,000 temperature stations.</li> \n   <li>Over 100,000 precipitation stations.</li> \n   <li>Over 50,000 snowfall or snow depth stations</li> \n  </ul>\n</li> \n <li>The source of each datum, the term used for a single record, is contained in the dataset. Each datum has a quality control flag associated with it.</li> \n <li>The dataset is updated daily. The historic sources are reprocessed weekly.</li> \n</ul>\n<p>You can see in the graphic below how the data volume has grown in recent decades.</p> \n<p><img class=\"alignnone size-full wp-image-6462\" src=\"https://d2908q01vomqb2.cloudfront.net/b6692ea5df920cad691c20319a6fffd7a4a766b8/2019/02/07/VisualizeClimateDataAthenaQuickSight1.png\" alt=\"\" width=\"800\" height=\"481\"></p> \n<p><em>Figure 1: 1763 to 2018. For 1763 there are less than a thousand observations. For 2017 there are over 34 million observations.</em></p> \n<h2>Organization of the data on Amazon S3</h2> \n<p>As previously mentioned, the GHCN-D dataset is accessible through an Amazon S3 bucket. The details of the dataset are on the Registry of Open Data on AWS (<a href=\"https://registry.opendata.aws/noaa-ghcn/\" target=\"_blank\" rel=\"noopener\">RODA</a>). The landing page for the dataset on RODA contains a link to a comprehensive <a href=\"https://docs.opendata.aws/noaa-ghcn-pds/readme.html\" target=\"_blank\" rel=\"noopener\">readme</a> file for the dataset. This <a href=\"https://docs.opendata.aws/noaa-ghcn-pds/readme.html\" target=\"_blank\" rel=\"noopener\">readme</a> contains all of the lookup tables and variable definitions.</p> \n<p>This section shows the pertinent information required to start working with the dataset.</p> \n<p>The data is in a text, or comma-separated values (CSV), format and is contained in the Amazon S3 bucket called noaa-ghcn-pds.</p> \n<p>The noaa-ghcn-pds bucket contains virtual folders, and is structured like this:</p> \n<ul>\n<li>\n<strong>noaa-ghcn-pds</strong>. This is the root of the bucket with two subdirectories and a number of useful files. For the purposes of this exercise, we use only the ghcnd-stations.txt file. This file contains information about the observation stations that produced the data for the GHCN_D dataset. You must download the <a href=\"http://noaa-ghcn-pds.s3.amazonaws.com/ghcnd-stations.txt\" target=\"_blank\" rel=\"noopener\">ghcnd-stations.txt</a> file.</li> \n <li>\n<strong>noaa-ghcn-pds/csv/</strong>. This virtual folder contains all of the observations from 1763 to the present organized in .csv files, one file for every year. For this exercise, we’ll collate this data into a single table.</li> \n</ul>\n<p>Also for the purpose of this exercise, the data from ‘ghcnd-stations.txt’ and the data contained in noaa-ghcn-pds/csv/ are extracted and added to two separate tables. These tables are the basis of the analysis.</p> \n<p>The tables are labeled as:</p> \n<ul>\n<li>\n<strong>tblallyears</strong>. This table contains all the records stored in the yearly .csv files from 1763 to present.</li> \n <li>\n<strong>tblghcnd_stations</strong>. This table contains information for over 106,430 weather stations.</li> \n</ul>\n<p>Point of interest: the .csv file from the year 1763 contains the data for one weather station. That station was located in the center of Milan, Italy.</p> \n<h2>The tools</h2> \n<p>To implement the general workflow in this exercise, we’re using the following tools:</p> \n<ul>\n<li>Amazon Simple Storage Service (Amazon S3) to stage the data for analysis. The GHCN_D dataset is stored in a bucket on Amazon S3. We also use a private bucket to store new tables created from queries.</li> \n <li>Amazon Athena to query data stored on Amazon S3 using standard SQL.</li> \n <li>AWS Glue to extract and load data into Athena from the Amazon S3 buckets in which it is stored. AWS Glue is a fully managed extract, transform, and load (ETL) service.</li> \n <li>AWS Glue Data Catalog to catalog the data that we query with Athena.</li> \n <li>Amazon QuickSight to build visualizations, perform ad hoc analyses, and get insights from the dataset. Queries and tables from Athena can be read directly from Amazon QuickSight. Amazon QuickSight can also run queries against tables in Athena.</li> \n</ul>\n<p>To implement the processes outlined in this post, you need an AWS Account. For more information about creating an AWS account, see <a href=\"https://aws.amazon.com/start-now/?sc_ichannel=ha&amp;sc_icampaign=start-now&amp;sc_icontent=2235\" target=\"_blank\" rel=\"noopener\">Getting Started with AWS</a>. You also must create a private Amazon S3 bucket located in the N. Virginia AWS Region. For more information, see <a href=\"https://docs.aws.amazon.com/AmazonS3/latest/gsg/CreatingABucket.html\" target=\"_blank\" rel=\"noopener\">Create a Bucket</a>.</p> \n<p>When you create the bucket, it must contain the following empty directories:</p> \n<ol>\n<li>[your_bucket_name]/stations_raw/</li> \n <li>[your_bucket_name]/ghcnblog/</li> \n <li>[your_bucket_name]/ghcnblog/stations/</li> \n <li>[your_bucket_name]/ghcnblog/allyears/</li> \n <li>[your_bucket_name]/ghcnblog/1836usa/</li> \n</ol>\n<p>The following is an overview of how the various AWS services interact in this workflow.</p> \n<p><strong>Note</strong></p> \n<p>The AWS services are in the same AWS Region. One of the Amazon S3 buckets is the existing one that stores the GHCN_D data. The other Amazon S3 bucket is the bucket that you use for storing tables.</p> \n<p><img class=\"alignnone size-full wp-image-6463\" src=\"https://d2908q01vomqb2.cloudfront.net/b6692ea5df920cad691c20319a6fffd7a4a766b8/2019/02/07/VisualizeClimateDataAthenaQuickSight2.png\" alt=\"\" width=\"433\" height=\"505\"></p> \n<p><em>Figure 2: How the AWS services work together to compose this workflow.</em></p> \n<h2>The workflow</h2> \n<p>Now that we have the tools and the data, we are ready to:</p> \n<ol>\n<li>Extract the yearly .csv files and add them to a table in Amazon Athena.</li> \n <li>Extract the stations text file and add it to a separate table in Amazon Athena.</li> \n</ol>\n<h3>Extract the yearly .csv files and add it to a table in Amazon Athena</h3> \n<p>The complete set of daily weather observations is organized by year in one folder of the Amazon S3 bucket in .csv format. The path to the data is <strong>s3://noaa-ghcn-pds/csv</strong><strong>/</strong>.</p> \n<p>Each file is named by year beginning with 1763.csv and progressing one year at a time up to the present.</p> \n<p>From the AWS console, click on AWS Athena. This takes you to the main dashboard for Athena. From here, click on AWS Glue Data Catalog. This brings you to AWS Glue.</p> \n<p>In AWS Glue, choose the <strong>Tables</strong> section on the left side. Then, in the <strong>Add table</strong> drop-down menu, choose <strong>Add table manually</strong>. A series of forms displays for you to add the following information:</p> \n<ul>\n<li>Set up your table’s properties: \n  <ul>\n<li>Give the new table a name, for example, tblallyears</li> \n   <li>Create a database and name it ghcnblog.</li> \n  </ul>\n</li> \n</ul>\n<p>The database then appears in the Athena dashboard.</p> \n<ul>\n<li>Add a data store: \n  <ul>\n<li>Choose the <strong>Specified path in another account</strong> option, and enter the following path in the text box: <strong>s3://noaa-ghcn-pds/csv/</strong>\n</li> \n  </ul>\n</li> \n <li>Choose a data format: \n  <ul>\n<li>Select <strong>CSV</strong>, then select <strong>Comma</strong> as the delimiter.</li> \n  </ul>\n</li> \n <li>Define a schema: \n  <ul>\n<li>Add the following columns as string variables: \n    <ul>\n<li>id</li> \n     <li>year_date</li> \n     <li>element</li> \n     <li>data_value</li> \n     <li>m_flag</li> \n     <li>q_flag</li> \n     <li>s_flag</li> \n     <li>obs_time</li> \n    </ul>\n</li> \n  </ul>\n</li> \n</ul>\n<p>For a full description of the variables and data structures, see the <a href=\"https://docs.opendata.aws/noaa-ghcn-pds/readme.html\" target=\"_blank\" rel=\"noopener\">readme</a> file.</p> \n<ul>\n<li>Choose <strong>OK</strong>, then <strong>Finish</strong>.</li> \n</ul>\n<p>Now return to the Athena dashboard, and choose the database that you created. The table will appear in the list of tables on the left. You can now preview the data by choosing the ‘Preview table’ option to the right of the table.</p> \n<h3>Use CTAS to speed up queries</h3> \n<p>As a final step, create a table using the SQL statement called CREATE TABLE AS SELECT (CTAS). Store the table in a private Amazon S3 bucket.</p> \n<p>This step dramatically speeds up queries. The reason is because in this process we extract the data once and store the extracted data in a columnar format (Parquet) in the private Amazon S3 bucket.</p> \n<p>To illustrate the improvement in speed, here are two examples:</p> \n<ul>\n<li>A query that counts all of the distinct IDs, meaning unique weather stations, takes around 55 seconds and scans around 88 GB of data.</li> \n <li>The same query on the converted data takes around 13 seconds and scans about 5 GB of data.</li> \n</ul>\n<p>To create the table for this final step:</p> \n<ol>\n<li>Open the Athena console.</li> \n <li>In the dashboard, select <strong>New query</strong>, then enter the query as shown in the following example. Make sure to enter the information that’s applicable to your particular situation, such as your bucket name.<img class=\"alignnone size-full wp-image-6464\" src=\"https://d2908q01vomqb2.cloudfront.net/b6692ea5df920cad691c20319a6fffd7a4a766b8/2019/02/07/VisualizeClimateDataAthenaQuickSight3.png\" alt=\"\" width=\"800\" height=\"326\"><br><em>Figure 3: Query to create a table data converting the data into Parquet and storing it in your S3 bucket.</em>\n</li> \n <li>Make sure that the data format is Parquet.</li> \n <li>Name your table <strong>tblallyears_qa</strong>.</li> \n <li>Add this path to this folder in the private Amazon S3 bucket: <strong>[your_bucket_name]/ghcnblog/allyearsqa/</strong>. Replace your_bucket_name with your specific bucket name.</li> \n</ol>\n<p>The new table appears in your database, listed on the left side of the Athena dashboard. This is the table that we work with going forward.</p> \n<h3>Extract the stations text file and add it to a separate table in Amazon Athena</h3> \n<p>The stations text file contains information about the weather stations, such as location, nationality, and ID. This data is kept in a separate file from the yearly observations. We need to import this data to look at the geographical spread of weather observations. While dealing with this file is a bit more complicated, the steps to importing this data into Athena are similar to what we have already done.</p> \n<p>To import this data into Athena:</p> \n<ol>\n<li>Download the <a href=\"http://noaa-ghcn-pds.s3.amazonaws.com/ghcnd-stations.txt\" target=\"_blank\" rel=\"noopener\">ghcnd-stations text file</a>.</li> \n <li>Open the file in a spreadsheet program and use the fixed width-delimited data import function. The fixed widths of the columns are described in the <a href=\"https://docs.opendata.aws/noaa-ghcn-pds/readme.html\" target=\"_blank\" rel=\"noopener\">readme file</a> in the section called <strong>FORMAT OF “ghcnd-stations.txt” file</strong>.</li> \n <li>After you successfully import the data, save the spreadsheet as a .csv text file.</li> \n <li>Copy the new .csv file to <strong>[your_bucket_name]/stations_raw/</strong>. Replace your_bucket_name with your specific bucket name.</li> \n <li>Using this new .csv file, follow the <strong>Add table process</strong> steps in AWS Glue, as described earlier in this post. \n  <ul>\n<li>Use the following field names: \n    <ul>\n<li>id</li> \n     <li>latitude</li> \n     <li>longitude</li> \n     <li>elevation</li> \n     <li>state</li> \n     <li>name</li> \n     <li>gsn_flag</li> \n     <li>hcn_flag</li> \n     <li>wmo_id</li> \n    </ul>\n</li> \n   <li>Name the table <strong>tblghcnd_stations</strong>.</li> \n  </ul>\n</li> \n <li>After the table is created, follow the CREATE TABLE AS SELECT (CTAS) steps for this table as described earlier in this post. \n  <ul>\n<li>Name the new table <strong>tblghcnd_stations_qa</strong>.</li> \n   <li>Store the new table in [your_bucket_name]/ghcnblog/stations/. Replace your_bucket_name with your specific bucket name.</li> \n  </ul>\n</li> \n</ol>\n<p>The two most important datasets of GHCN_D are now in Athena.</p> \n<p>In the next section, we run queries against these tables and visualize the results using Amazon QuickSight.</p> \n<h2>Exploratory data analysis and visualization</h2> \n<p>With our two tables created, we are now ready to query and visualize to gain insight.</p> \n<h3>Exploratory data analysis</h3> \n<p>In the Athena query window, run the following queries to get an idea of the size of the dataset.</p> \n<p>Query #1: the total number of observations since 1763:</p> \n<p><img class=\"alignnone size-full wp-image-6465\" src=\"https://d2908q01vomqb2.cloudfront.net/b6692ea5df920cad691c20319a6fffd7a4a766b8/2019/02/07/VisualizeClimateDataAthenaQuickSight4.png\" alt=\"\" width=\"800\" height=\"342\"></p> \n<p><em>Figure 4: Query for total number of observations. This was run in autumn 2018. The dataset is continuingly growing over time.</em></p> \n<p>Query #2: the number of stations since 1763:</p> \n<p><img class=\"alignnone size-full wp-image-6466\" src=\"https://d2908q01vomqb2.cloudfront.net/b6692ea5df920cad691c20319a6fffd7a4a766b8/2019/02/07/VisualizeClimateDataAthenaQuickSight5.png\" alt=\"\" width=\"800\" height=\"342\"></p> \n<p><em>Figure 5: Query for total number of stations that have made observations since 1763. Deactivated stations are included.</em></p> \n<h3>Average weather parameters for the Earth</h3> \n<p>The following figure shows a query that calculates the average maximum temperature (Celsius), average minimum temperature (Celsius), and average rainfall (mm) for the Earth since 1763.</p> \n<p>In the query, we must convert the data_value from a String variable to a Real variable. We also must divide by 10, because the temperature and precipitation measurements are in tenths of their respective units. For more information about these details and the element codes (TMIB, TMAX and PRCP), see the <a href=\"https://docs.opendata.aws/noaa-ghcn-pds/readme.html\" target=\"_blank\" rel=\"noopener\">readme file</a>.</p> \n<p><img class=\"alignnone size-full wp-image-6467\" src=\"https://d2908q01vomqb2.cloudfront.net/b6692ea5df920cad691c20319a6fffd7a4a766b8/2019/02/07/VisualizeClimateDataAthenaQuickSight6.png\" alt=\"\" width=\"800\" height=\"398\"></p> \n<p><em>Figure 6. Querying for global averages to get to Earth’s pulse.</em></p> \n<p>It would be convenient if we could just run simple queries, such as this one, on this dataset and accept that the results are correct.</p> \n<p>The previous query is assuming an even and equal spread of weather stations around the world since 1763. In fact, the number and spread of weather stations varied over time.</p> \n<h3>Visualizing the growth in numbers of weather stations over time</h3> \n<p>The following query visualizes the number of weather stations for each year since 1763, by using <a href=\"https://docs.aws.amazon.com/quicksight/latest/user/welcome.html\" target=\"_blank\" rel=\"noopener\">Amazon QuickSight</a>.</p> \n<p><strong>Note: </strong>You must be signed up for Amazon QuickSight to complete these steps. During the sign-up process, you are prompted to manage your Amazon QuickSight data source permissions. At this time, use step 3 in the following procedure to grant access to the Amazon S3 buckets and to Athena.</p> \n<p>The steps are as follows:</p> \n<ol>\n<li>Open the Amazon QuickSight console.</li> \n <li>On the far right of the dashboard, choose <strong>Manage QuickSight</strong>.</li> \n <li>Choose <strong>Account Setting</strong>, then <strong>Manage QuickSight permissions</strong>. Give Amazon QuickSight permission to access Athena, and to read the Amazon S3 bucket that contains the new tables.</li> \n <li>Return to Amazon QuickSight by choosing the logo on the top left side of the screen.</li> \n <li>From the Amazon QuickSight dashboard, choose <strong>New analysis</strong>, then <strong>New data set</strong>.</li> \n <li>From the <strong>Create a Data Set</strong> tiles, choose <strong>Athena</strong>. Name the data source, for example ghcnblog, then choose <strong>Create data source</strong>.</li> \n <li>Choose the option to add a custom SQL, then add the SQL, as shown in the following example:</li> \n</ol>\n<p><img class=\"alignnone size-full wp-image-6468\" src=\"https://d2908q01vomqb2.cloudfront.net/b6692ea5df920cad691c20319a6fffd7a4a766b8/2019/02/07/VisualizeClimateDataAthenaQuickSight7.png\" alt=\"\" width=\"527\" height=\"430\"></p> \n<p><em>Figure 7: Location to add a custom SQL query.</em></p> \n<ol start=\"8\">\n<li>Choose <strong>Confirm query</strong>.</li> \n <li>Choose <strong>Directly query your data</strong>.</li> \n <li>Choose <strong>Visualize</strong>.</li> \n <li>To make the graph, choose the line chart graph. Add <strong>year</strong> to the X-axis and <strong>number_of_stations</strong> to the Value field wells. The options appear to the left of the visualization dashboard.<img class=\"alignnone size-full wp-image-6469\" src=\"https://d2908q01vomqb2.cloudfront.net/b6692ea5df920cad691c20319a6fffd7a4a766b8/2019/02/07/VisualizeClimateDataAthenaQuickSight8.png\" alt=\"\" width=\"800\" height=\"487\">\n</li> \n</ol>\n<p><em>Figure 8. The number of global weather stations used by GHCN_D over time.</em></p> \n<p>The resulting graph shows that the number and spread of stations around the world has varied over time.</p> \n<h3>A look at the development of observation in the US</h3> \n<p>1836 is the year of the first US observation station in the data set. To get an insight into the development of observations the US, we extracted a subset of US data from the main data source (tblallyears_qa). This dataset features annual data every 30<sup>th</sup> year from 1836 to 2016.</p> \n<p>This query generates a large dataset. To improve performance, save the query as a table stored in an Amazon S3 bucket using the previously described procedure.</p> \n<p>The query to do this in one step is shown in the following figure.</p> \n<p><img class=\"alignnone size-full wp-image-6470\" src=\"https://d2908q01vomqb2.cloudfront.net/b6692ea5df920cad691c20319a6fffd7a4a766b8/2019/02/07/VisualizeClimateDataAthenaQuickSight9.png\" alt=\"\" width=\"800\" height=\"333\"></p> \n<p><em>Figure 9: The SQL to create a table from a query and store it in Parquet format in a user-specified Amazon S3 bucket.</em></p> \n<p>The table appears in the Amazon Athena dashboard as <strong>tbl1836every30thyear</strong> and it forms the basis for our analysis.</p> \n<p>In the Amazon QuickSight console, use the follow SQL to generate a new dataset.</p> \n<p><img class=\"alignnone size-full wp-image-6471\" src=\"https://d2908q01vomqb2.cloudfront.net/b6692ea5df920cad691c20319a6fffd7a4a766b8/2019/02/07/VisualizeClimateDataAthenaQuickSight10.png\" alt=\"\" width=\"492\" height=\"394\"></p> \n<p><em>Figure 10: The SQL to create a dataset for viewing USA data in Amazon QuickSight.</em></p> \n<ol>\n<li>Choose <strong>Confirm query</strong>.</li> \n <li>Choose <strong>Directly query your data</strong>.</li> \n <li>Choose <strong>Visualize</strong>.</li> \n</ol>\n<p>This brings you back to the visualization dashboard. From this dashboard, chose the <strong>Points on a map</strong> visualization, and set up the fields as follows:</p> \n<ul>\n<li>\n<strong>Geospatial</strong>: state</li> \n <li>\n<strong>Size</strong>: number_of_stations, aggregate by count.</li> \n <li>\n<strong>Color</strong>: year</li> \n</ul>\n<p>The results should be the following map of the US showing the growth of weather stations used by GHCN_D from 1836 to 2016 at 30-year increments. In 1836, there was one station. By 2016, there were thousands.</p> \n<p><img class=\"alignnone size-full wp-image-6472\" src=\"https://d2908q01vomqb2.cloudfront.net/b6692ea5df920cad691c20319a6fffd7a4a766b8/2019/02/07/VisualizeClimateDataAthenaQuickSight11.png\" alt=\"\" width=\"800\" height=\"390\"></p> \n<p><em>Figure 11: The growth of the number of observations stations in the US.</em></p> \n<p>Interestingly, some states had more stations in 1956 than they did in 1986. This is also illustrated in the following figure. The data for the figure was derived from the previous dataset.</p> \n<p><img class=\"alignnone size-full wp-image-6473\" src=\"https://d2908q01vomqb2.cloudfront.net/b6692ea5df920cad691c20319a6fffd7a4a766b8/2019/02/07/VisualizeClimateDataAthenaQuickSight12.png\" alt=\"\" width=\"800\" height=\"405\"></p> \n<p><em>Figure 12: This heat map illustrates the number of stations per state over time. This is a 30th year snapshot.</em></p> \n<h3>A look at more data</h3> \n<p>We have now a data lake of GHN_D data. By using the tools that we have assembled, we are free to experiment with the data. It is now possible to construct queries and visualization on this huge dataset to gain insights.</p> \n<p>The following figure shows the heat map that we created. It shows the average minimum temperature in US states over time. As before, we are looking at 30-year snapshots; that is to say, every 30th year we take a yearly average.</p> \n<p><img class=\"alignnone size-full wp-image-6474\" src=\"https://d2908q01vomqb2.cloudfront.net/b6692ea5df920cad691c20319a6fffd7a4a766b8/2019/02/07/VisualizeClimateDataAthenaQuickSight13.png\" alt=\"\" width=\"800\" height=\"388\"></p> \n<p><em>Figure 13: This heat map illustrates the minimum temperate for each state over time. A yearly average every 30th year starting at 1836.</em></p> \n<h2>Summary</h2> \n<p>Our headlines are full of Climate Change and Sustainability stories, and research and analysis has become more crucial than ever.</p> \n<p>We showed researchers, analysts, and scientists how AWS services have reduced the level of technical skills required to fully use the GHCN_D dataset.</p> \n<p>This GHCN-D is available on AWS. The details can be found on the <a href=\"https://registry.opendata.aws/noaa-ghcn/\" target=\"_blank\" rel=\"noopener\">Registry of Open Data</a> on AWS. This data is available to researchers studying climate change and weather impacts.</p> \n<p>This blog post demonstrated a typical workflow that a researcher could use to engage with and analyze this important data by using Amazon Athena, AWS Glue, and Amazon S3, and how they can visualize insights by using Amazon QuickSight.</p> \n<p>By making this data available, Amazon has removed the heavy lifting that was traditionally required to work with the GHCN_D, thus expanding the opportunity for new research and new discoveries.</p> \n<p> </p> \n<hr>\n<h3>About the Authors</h3> \n<p><img class=\"size-full wp-image-6487 alignleft\" src=\"https://d2908q01vomqb2.cloudfront.net/b6692ea5df920cad691c20319a6fffd7a4a766b8/2019/02/11/JFlasher.png\" alt=\"\" width=\"113\" height=\"146\"><strong>Joe Flasher is the Open Geospatial Data Lead at Amazon Web Services</strong>, helping organizations most effectively make data available for analysis in the cloud. Joe has been working with geospatial data and open source projects for the past decade, both as a contributor and maintainer. He has been a member of the Landsat Advisory Group, and has worked on projects, ranging from building GIS software to making the space shuttle fly. Joe’s background is in astrophysics, but he kindly requests you don’t ask him any questions about constellations.</p> \n<p> </p> \n<p> </p> \n<p><img class=\"size-full wp-image-6488 alignleft\" src=\"https://d2908q01vomqb2.cloudfront.net/b6692ea5df920cad691c20319a6fffd7a4a766b8/2019/02/11/ConorDel.png\" alt=\"\" width=\"113\" height=\"146\"><strong>Conor Delaney, PhD. is an environmental data scientist.</strong></p>\n",
      "enclosure": {},
      "categories": [
        "Amazon Athena",
        "Amazon QuickSight",
        "Amazon Quicksight"
      ]
    },
    {
      "title": "Create real-time clickstream sessions and run analytics with Amazon Kinesis Data Analytics, AWS Glue, and Amazon Athena",
      "pubDate": "2019-02-07 20:56:46",
      "link": "https://aws.amazon.com/blogs/big-data/create-real-time-clickstream-sessions-and-run-analytics-with-amazon-kinesis-data-analytics-aws-glue-and-amazon-athena/",
      "guid": "06aeeb49bfe1b4ff6c4284a03bd2171fc5de04a6",
      "author": "Hugo Rozestraten",
      "thumbnail": "https://d2908q01vomqb2.cloudfront.net/b6692ea5df920cad691c20319a6fffd7a4a766b8/2019/02/01/ClickstreamSessionsKinesisGlueAthena1.png",
      "description": "Clickstream events are small pieces of data that are generated continuously with high speed and volume. Often, clickstream events are generated by user actions, and it is useful to analyze them. For example, you can detect user behavior in a website or application by analyzing the sequence of clicks a user makes, the amount of […]",
      "content": "\n<p>Clickstream events are small pieces of data that are generated continuously with high speed and volume. Often, clickstream events are generated by user actions, and it is useful to analyze them.</p> \n<p>For example, you can detect user behavior in a website or application by analyzing the sequence of clicks a user makes, the amount of time the user spends, where they usually begin the navigation, and how it ends. By tracking this user behavior in real time, you can update recommendations, perform advanced A/B testing, push notifications based on session length, and much more. To track and analyze these events, you need to identify and create sessions from them. The process of identifying events in the data and creating sessions is known as <em>sessionization</em>.</p> \n<p>Capturing and processing data clickstream events in real time can be difficult. As the number of users and web and mobile assets you have increases, so does the volume of data. <a href=\"https://aws.amazon.com/kinesis/\" target=\"_blank\" rel=\"noopener\">Amazon Kinesis</a> provides you with the capabilities necessary to ingest this data in real time and generate useful statistics immediately so that you can take action.</p> \n<p>When you run sessionization on clickstream data, you identify events and assign them to a session with a specified key and lag period. After each event has a key, you can perform analytics on them. The use cases for sessionization vary widely, and have different requirements. For example, you might need to identify and create sessions from events in web analytics to track user actions. Sessionization is also broadly used across many different areas, such as log data and IoT.</p> \n<p>This blog post demonstrates how to identify and create sessions from real-time clickstream events and then analyze them using <a href=\"https://aws.amazon.com/kinesis/data-analytics/\" target=\"_blank\" rel=\"noopener\">Amazon Kinesis Data Analytics</a>.</p> \n<h2>Why did we choose Kinesis Data Analytics?</h2> \n<p>Clickstream data arrives continuously as thousands of messages per second receiving new events. When you analyze the effectiveness of new application features, site layout, or marketing campaigns, it is important to analyze them in real time so that you can take action faster.</p> \n<p>To perform the sessionization in batch jobs, you could use a tool such as <a href=\"https://aws.amazon.com/glue/\" target=\"_blank\" rel=\"noopener\">AWS Glue</a> or <a href=\"https://aws.amazon.com/emr/\" target=\"_blank\" rel=\"noopener\">Amazon EMR</a>. But with daily schedules, queries and aggregation, it can take more resources and time because each aggregation involves working with large amounts of data. Performing sessionization in Kinesis Data Analytics takes less time and gives you a lower latency between the sessions generation. You can trigger real-time alerts with <a href=\"https://aws.amazon.com/lambda/\" target=\"_blank\" rel=\"noopener\">AWS Lambda</a> functions based on conditions, such as session time that is shorter than 20 seconds, or a machine learning endpoint.</p> \n<h2>Identifying a session among thousands of clicks</h2> \n<p>A <em>session</em> is a short-lived and interactive exchange between two or more devices and/or users. For example, it can be a user browsing and then exiting your website, or an IoT device waking up to perform a job and then going back to sleep. These interactions result in a series of events that occur in sequence that start and end, or a session. A start and an end of a session can be difficult to determine, and are often defined by a time period without a relevant event associated with a user or device. A session starts when a new event arrives after a specified “lag” time period has passed without an event arriving. A session ends in a similar manner, when a new event does not arrive within the specified lag period.</p> \n<p>This blog post relies on several other posts about performing batch analytics on SQL data with sessions. My two favorite posts on this subject are <a href=\"https://www.dataiku.com/learn/guide/code/reshaping_data/sessionization.html\" target=\"_blank\" rel=\"noopener\">Sessionization in SQL, Hive, Pig and Python</a> from Dataiku and <a href=\"https://blog.modeanalytics.com/finding-user-sessions-sql/\" target=\"_blank\" rel=\"noopener\">Finding User Session with SQL</a> by Benn Stancil at Mode. Both posts take advantage of SQL window functions to identify and build sessions from clickstream events.</p> \n<p>ANSI added SQL window functions to the SQL standard in 2003 and has since expanded them. Window functions work naturally with streaming data and enable you to easily translate batch SQL examples to Kinesis Data Analytics.</p> \n<p>In this use case, I group the events of a specific user as described in the following simplified example. In this example, I use distinct navigation patterns from three users to analyze user behavior. To begin, I group events by user ID to obtain some statistics from data, as shown following:</p> \n<p><img class=\"alignnone size-full wp-image-6416\" src=\"https://d2908q01vomqb2.cloudfront.net/b6692ea5df920cad691c20319a6fffd7a4a766b8/2019/02/01/ClickstreamSessionsKinesisGlueAthena1.png\" alt=\"\" width=\"800\" height=\"455\"></p> \n<p>In this example, for “User ID 20,” the minimum timestamp is 2018-11-29 23:35:10 and the maximum timestamp is 2018-11-29 23:35:44. This provides a 34 seconds-long session, starting with action “B_10” and ending with action “A_02.” These “actions” are identification of the application’s buttons in this example.</p> \n<p>Suppose that after several minutes, new “User ID 20” actions arrive. Would you consider them as running in the same session? A user can abort a navigation or start a new one. Also, applications often have timeouts. You have to decide what is the maximum session length to consider it a new session. A session can run anywhere from 20 to 50 seconds, or from 1 to 5 minutes.</p> \n<p><img class=\"alignnone size-full wp-image-6417\" src=\"https://d2908q01vomqb2.cloudfront.net/b6692ea5df920cad691c20319a6fffd7a4a766b8/2019/02/01/ClickstreamSessionsKinesisGlueAthena2.png\" alt=\"\" width=\"800\" height=\"404\"></p> \n<p>There are other elements that you might want to consider, such as a client IP or a machine ID. These elements allow you to separate sessions that occur on different devices.</p> \n<h2>High-level solution overview</h2> \n<p>The end-to-end scenario described in this post uses <a href=\"https://aws.amazon.com/kinesis/data-streams/\" target=\"_blank\" rel=\"noopener\">Amazon Kinesis Data Streams</a> to capture the clickstream data and Kinesis Data Analytics to build and analyze the sessions. The aggregated analytics are used to trigger real-time events on Lambda and then send them to <a href=\"https://aws.amazon.com/kinesis/data-firehose/\" target=\"_blank\" rel=\"noopener\">Kinesis Data Firehose</a>. Kinesis Data Firehose sends data to an <a href=\"https://aws.amazon.com/s3/\" target=\"_blank\" rel=\"noopener\">Amazon S3</a> bucket, where it is ingested to a table by an AWS Glue crawler and made available for running queries with <a href=\"https://aws.amazon.com/athena/\" target=\"_blank\" rel=\"noopener\">Amazon Athena</a>. You can use this table for ad hoc analysis.</p> \n<p>The following diagram shows an end-to-end sessionization solution.</p> \n<p><img class=\"alignnone size-full wp-image-6418\" src=\"https://d2908q01vomqb2.cloudfront.net/b6692ea5df920cad691c20319a6fffd7a4a766b8/2019/02/01/ClickstreamSessionsKinesisGlueAthena3.png\" alt=\"\" width=\"800\" height=\"452\"></p> \n<ul>\n<li>\n<strong>Data ingestion:</strong> You can use Kinesis Data Streams to build custom applications that process or analyze streaming data for specialized needs. Kinesis Data Streams can continuously capture and store terabytes of data per hour from hundreds of thousands of sources, such as website clickstreams, financial transactions, social media feeds, IT logs, and location-tracking events.</li> \n <li>\n<strong>Data sessionization:</strong> Kinesis Data Analytics is the easiest way to process streaming data in real time with standard SQL without having to learn new programming languages or processing frameworks. With Kinesis Data Analytics, you can query streaming data or build entire streaming applications using SQL, so that you can gain actionable insights and respond to your business and customer needs promptly.</li> \n <li>\n<strong>Data processing and storage:</strong> The sessionization stream is read from Kinesis Data Analytics using an AWS Lambda function. The function triggers two events: one real-time dashboard in <a href=\"https://aws.amazon.com/cloudwatch/\" target=\"_blank\" rel=\"noopener\">Amazon CloudWatch</a> and a second one to persist data with Kinesis Data Firehose.</li> \n <li>\n<strong>Data analysis: </strong>AWS Glue is used to crawl Amazon S3 and build or update metadata definition for Amazon Athena tables.</li> \n</ul>\n<p>Amazon Athena is an interactive query service that makes it easy to analyze data in Amazon S3 using standard SQL. Athena provides connectivity to any application using JDBC or ODBC drivers.</p> \n<ul>\n<li>\n<strong>Data visualization:</strong> Amazon QuickSight is a visualization tool that is natively used to build dashboards over Amazon Athena data.</li> \n <li>\n<strong>Monitoring:</strong> Amazon CloudWatch is a tool that lets you monitor the streaming activities, such as the number of bytes processed or delivered per second, or the number of failures.</li> \n</ul>\n<p>After you finish the sessionization stage in Kinesis Data Analytics, you can output data into different tools. For example, you can use a <a href=\"https://docs.aws.amazon.com/kinesisanalytics/latest/dev/how-it-works-output-lambda.html\" target=\"_blank\" rel=\"noopener\">Lambda function to process the data on the fly</a> and take actions such as send SMS alerts or roll back a deployment. To learn how to implement such workflows based on AWS Lambda output, see my other blog post <a href=\"https://aws.amazon.com/blogs/big-data/implement-serverless-log-analytics-using-amazon-kinesis-analytics/\" target=\"_blank\" rel=\"noopener\">Implement Log Analytics using Amazon Kinesis Data Analytics</a>. In this post, we send data to Amazon CloudWatch, and build a real-time dashboard.</p> \n<h3>Lambda clickstream generator</h3> \n<p>To generate the workload, you can use a Python Lambda function with random values, simulating a beer-selling application.</p> \n<p>The same user ID can have sessions on different devices, such as a tablet, a browser, or a phone application. This information is captured by the device ID. As a result, the data for the Lambda function payload has these parameters: a user ID, a device ID, a client event, and a client timestamp, as shown in the following example.</p> \n<p>The following is the code for the Lambda function payload generator, which is scheduled using CloudWatch Events scheduled events:</p> \n<div class=\"hide-language\"> \n <pre><code class=\"lang-python\">...\ndef getReferrer():\n    x = random.randint(1,5)\n    x = x*50 \n    y = x+30 \n    data = {}\n    data['user_id'] = random.randint(x,y)\n    data['device_id'] = random.choice(['mobile','computer', 'tablet', 'mobile','computer'])\n    data['client_event'] = random.choice(['beer_vitrine_nav','beer_checkout','beer_product_detail',\n    'beer_products','beer_selection','beer_cart'])\n    now = datetime.datetime.now()\n    str_now = now.isoformat()\n    data['client_timestamp'] = str_now\n    return data\n\ndef lambda_handler(event, context):\n...\n        data = json.dumps(getReferrer())\n        kinesis.put_record(\n                StreamName='sessionsclicks',\n                Data=data,\n                PartitionKey='partitionkey')</code></pre> \n</div> \n<p>As a result, the following payloads are sent to Kinesis Data Analytics:</p> \n<p><img class=\"alignnone size-full wp-image-6419\" src=\"https://d2908q01vomqb2.cloudfront.net/b6692ea5df920cad691c20319a6fffd7a4a766b8/2019/02/01/ClickstreamSessionsKinesisGlueAthena4.png\" alt=\"\" width=\"800\" height=\"517\"></p> \n<h2>Using window SQL functions in Kinesis Data Analytics</h2> \n<p>Grouping sessions lets us combine all the events from a given user ID or a device ID that occurred during a specific time period. Amazon Kinesis Data Analytics SQL queries in your application code execute continuously over in-application streams. You need to specify bounded queries using a window defined in terms of time or rows. These queries are called <em>window SQL functions</em>.</p> \n<p>I had three available options for windowed query functions in Kinesis Data Analytics: <em>sliding windows</em>, <em>tumbling windows</em>, and <em>stagger windows</em>. I chose stagger window because it has some good features for the sessionization use case, as follows:</p> \n<ul>\n<li>Stagger windows open when the first event that matches a partition key condition arrives. So for each key, it evaluates its particular window as opposed to the other window functions that evaluate one unique window for all the partition keys matched.</li> \n <li>When dealing with clickstreams, you cannot rely on the order that events arrive in the stream, but when the stream was generated. Stagger windows handle the arrival of out-of-order events well. The time when the window is opened and when the window closes is considered based on the age specified, which is measured from the time when the window opened.</li> \n</ul>\n<p>To partition by the timestamp, I chose to write two distinct SQL functions.</p> \n<p>In Kinesis Data Analytics, SOURCE_SQL_STREAM_001 is by default the main stream from the source. In this case, it’s receiving the source payload from Kinesis Data Streams.</p> \n<h3>Kinesis Data Analytics SQL – Create a stream</h3> \n<p>The following function creates a stream to receive the query aggregation result:</p> \n<div class=\"hide-language\"> \n <pre><code class=\"lang-sql\">-- CREATE a Stream to receive the query aggregation result\nCREATE OR REPLACE STREAM \"DESTINATION_SQL_STREAM\"\n(\n  session_id VARCHAR(60),\n  user_id INTEGER,\n  device_id VARCHAR(10),\n  timeagg timestamp,\n  events INTEGER,\n  beginnavigation VARCHAR(32),\n  endnavigation VARCHAR(32),\n  beginsession VARCHAR(25),\n  endsession VARCHAR(25),\n  duration_sec INTEGER\n);</code></pre> \n</div> \n<h3>Kinesis Data Analytics SQL – Using a SECOND interval “STEP” function</h3> \n<p>The following function creates the PUMP and inserts it as SELECT to STREAM:</p> \n<div class=\"hide-language\"> \n <pre><code class=\"lang-sql\">-- Create the PUMP\nCREATE OR REPLACE PUMP \"WINDOW_PUMP_SEC\" AS INSERT INTO \"DESTINATION_SQL_STREAM\"\n-- Insert as Select \n    SELECT  STREAM\n-- Make the Session ID using user_ID+device_ID and Timestamp\n    UPPER(cast(\"user_id\" as VARCHAR(3))|| '_' ||SUBSTRING(\"device_id\",1,3)\n    ||cast( UNIX_TIMESTAMP(STEP(\"client_timestamp\" by interval '30' second))/1000 as VARCHAR(20))) as session_id,\n    \"user_id\" , \"device_id\",\n-- create a common rounded STEP timestamp for this session\n    STEP(\"client_timestamp\" by interval '30' second),\n-- Count the number of client events , clicks on this session\n    COUNT(\"client_event\") events,\n-- What was the first navigation action\n    first_value(\"client_event\") as beginnavigation,\n-- what was the last navigation action    \n    last_value(\"client_event\") as endnavigation,\n-- begining minute and second  \n    SUBSTRING(cast(min(\"client_timestamp\") AS VARCHAR(25)),15,19) as beginsession,\n-- ending minute and second      \n    SUBSTRING(cast(max(\"client_timestamp\") AS VARCHAR(25)),15,19) as endsession,\n-- session duration    \n    TSDIFF(max(\"client_timestamp\"),min(\"client_timestamp\"))/1000 as duration_sec\n-- from the source stream    \n    FROM \"SOURCE_SQL_STREAM_001\"\n-- using stagger window , with STEP to Seconds, for Seconds intervals    \n    WINDOWED BY STAGGER (\n                PARTITION BY \"user_id\", \"device_id\", STEP(\"client_timestamp\" by interval '30' second) \n                RANGE INTERVAL '30' SECOND );</code></pre> \n</div> \n<h3>Kinesis Data Analytics SQL – Using a MINUTE interval “FLOOR” function</h3> \n<p>The following code creates the PUMP and inserts as SELECT to STREAM:</p> \n<div class=\"hide-language\"> \n <pre><code class=\"lang-sql\">-- Create the PUMP\nCREATE OR REPLACE PUMP \"WINDOW_PUMP_MIN\" AS INSERT INTO \"DESTINATION_SQL_STREAM\"\n-- Insert as Select \nSELECT  STREAM\n-- Make the Session ID using user_ID+device_ID and Timestamp\nUPPER(cast(\"user_id\" as VARCHAR(3))|| '_' ||SUBSTRING(\"device_id\",1,3)\n||cast(UNIX_TIMESTAMP(FLOOR(\"client_timestamp\" TO MINUTE))/1000 as VARCHAR(20))) as session_id,\n\"user_id\" , \"device_id\",\n-- create a common rounded timestamp for this session\nFLOOR(\"client_timestamp\" TO MINUTE),\n-- Count the number of client events , clicks on this session\nCOUNT(\"client_event\") events,\n-- What was the first navigation action\nfirst_value(\"client_event\") as beginnavigation,\n-- what was the last navigation action\nlast_value(\"client_event\") as endnavigation,\n-- begining minute and second\nSUBSTRING(cast(min(\"client_timestamp\") AS VARCHAR(25)),15,19) as beginsession,\n-- ending minute and second\nSUBSTRING(cast(max(\"client_timestamp\") AS VARCHAR(25)),15,19) as endsession,\n-- session duration\nTSDIFF(max(\"client_timestamp\"),min(\"client_timestamp\"))/1000 as duration_sec\n-- from the source stream\nFROM \"SOURCE_SQL_STREAM_001\"\n-- using stagger window , with floor to Minute, for Minute intervals\nWINDOWED BY STAGGER (\n            PARTITION BY \"user_id\", \"device_id\", FLOOR(\"client_timestamp\" TO MINUTE) \n            RANGE INTERVAL '1' MINUTE);\n</code></pre> \n</div> \n<h3>Sessions</h3> \n<p>In Kinesis Data Analytics, you can view the resulting data transformed by the SQL, with the sessions identification and information. Session_ID is calculated by User_ID + (3 Chars) of DEVICE_ID + rounded Unix timestamp without the milliseconds.</p> \n<p><img class=\"alignnone size-full wp-image-6420\" src=\"https://d2908q01vomqb2.cloudfront.net/b6692ea5df920cad691c20319a6fffd7a4a766b8/2019/02/01/ClickstreamSessionsKinesisGlueAthena5.png\" alt=\"\" width=\"800\" height=\"241\"></p> \n<h2>Automated deployment with AWS CloudFormation</h2> \n<p>All the steps of this end-to-end solution are included in an <a href=\"https://aws.amazon.com/cloudformation/\" target=\"_blank\" rel=\"noopener\">AWS CloudFormation</a> template. Fire up the template, add the code on your web server, and voilà, you get real-time sessionization.</p> \n<p>This AWS CloudFormation template is intended to be deployed only in the us-east-1 Region.</p> \n<h3>Create the stack</h3> \n<p><strong>Step 1:</strong> To get started, sign into the AWS Management Console, and then <a href=\"https://console.aws.amazon.com/cloudformation/home?region=us-east-1#/stacks/new?stackName=blog-sessionization&amp;templateURL=https://s3.amazonaws.com/aws-bigdata-blog/artifacts/realtime-clickstream-sessions-analytics-kinesis-glue-athena/streaming-stagger-window.template\" target=\"_blank\" rel=\"noopener\">open the stagger window template</a>.</p> \n<p><strong>Step 2:</strong> On the AWS CloudFormation console, choose <strong>Next</strong>, and complete the AWS CloudFormation parameters:</p> \n<ul>\n<li>\n<strong>Stack name</strong>: The name of the stack (<em>blog-sessionization</em> or <em>sessions-blog</em>)</li> \n <li>\n<strong>StreamName</strong>: sessionsblog</li> \n <li>\n<strong>Stream Shard Count</strong>: 1 or 2 (1 MB/s) per shard.</li> \n <li>\n<strong>Bucket Name</strong>:  Change to a unique name, for example <strong>session-n-bucket-hhug123121</strong>.</li> \n <li>\n<strong>Buffer Interval</strong>: 60–900 seconds buffering hint for Kinesis Data Firehose before the data is send to Amazon S3 from Kinesis Data Firehose.</li> \n <li>\n<strong>Buffer Size</strong>: 1–128 MB per file, if the interval is not achieved first.</li> \n <li>\n<strong>Destination Prefix</strong>: Aggregated (internal folder of the bucket to save aggregated data).</li> \n <li>\n<strong>Base sessions on seconds or minutes</strong>: Choose which you want (minutes will start with 1 minute, seconds will start with 30 seconds).</li> \n</ul>\n<p><img class=\"alignnone size-full wp-image-6421\" src=\"https://d2908q01vomqb2.cloudfront.net/b6692ea5df920cad691c20319a6fffd7a4a766b8/2019/02/01/ClickstreamSessionsKinesisGlueAthena6.png\" alt=\"\" width=\"800\" height=\"570\"></p> \n<p><strong>Step 3:</strong> Check if the launch has completed, and if it has not, check for errors.</p> \n<p>The most common error is when you point to an Amazon S3 bucket that already exists.</p> \n<p><img class=\"alignnone size-full wp-image-6422\" src=\"https://d2908q01vomqb2.cloudfront.net/b6692ea5df920cad691c20319a6fffd7a4a766b8/2019/02/01/ClickstreamSessionsKinesisGlueAthena7.png\" alt=\"\" width=\"800\" height=\"114\"></p> \n<h3>Process the data</h3> \n<p><strong>Step 1:</strong> After the deployment, navigate to the <a href=\"https://console.aws.amazon.com/kinesis/home?region=us-east-1#/dashboard\" target=\"_blank\" rel=\"noopener\">solution on the Amazon Kinesis console</a>.</p> \n<p><img class=\"alignnone size-full wp-image-6423\" src=\"https://d2908q01vomqb2.cloudfront.net/b6692ea5df920cad691c20319a6fffd7a4a766b8/2019/02/01/ClickstreamSessionsKinesisGlueAthena8.png\" alt=\"\" width=\"800\" height=\"307\"></p> \n<p><strong>Step 2:</strong> Go to the <strong>Kinesis Analytics applications</strong> page, and choose <strong>AnalyticsApp-blog-sessionizationXXXXX</strong>, as follows.</p> \n<p><img class=\"alignnone size-full wp-image-6424\" src=\"https://d2908q01vomqb2.cloudfront.net/b6692ea5df920cad691c20319a6fffd7a4a766b8/2019/02/01/ClickstreamSessionsKinesisGlueAthena9.png\" alt=\"\" width=\"800\" height=\"348\"></p> \n<p><strong>Step 3:</strong> Choose <strong>Run application</strong> to start the application.</p> \n<p><img class=\"alignnone size-full wp-image-6425\" src=\"https://d2908q01vomqb2.cloudfront.net/b6692ea5df920cad691c20319a6fffd7a4a766b8/2019/02/01/ClickstreamSessionsKinesisGlueAthena10.png\" alt=\"\" width=\"800\" height=\"738\"></p> \n<p><strong>Step 4:</strong> Wait a few seconds for the application to be available, and then choose <strong>Application details</strong>.</p> \n<p><img class=\"alignnone size-full wp-image-6426\" src=\"https://d2908q01vomqb2.cloudfront.net/b6692ea5df920cad691c20319a6fffd7a4a766b8/2019/02/01/ClickstreamSessionsKinesisGlueAthena11.png\" alt=\"\" width=\"786\" height=\"380\"></p> \n<p><strong>Step 5:</strong> On the Application details page, choose <strong>Go to SQL results</strong>.</p> \n<p><img class=\"alignnone size-full wp-image-6427\" src=\"https://d2908q01vomqb2.cloudfront.net/b6692ea5df920cad691c20319a6fffd7a4a766b8/2019/02/01/ClickstreamSessionsKinesisGlueAthena12.png\" alt=\"\" width=\"800\" height=\"207\"></p> \n<p><strong>Step 6:</strong> Examine the SQL code and SOURCE_SQL_STREAM, and change the INTERVAL if you’d like.</p> \n<p><img class=\"alignnone size-full wp-image-6428\" src=\"https://d2908q01vomqb2.cloudfront.net/b6692ea5df920cad691c20319a6fffd7a4a766b8/2019/02/01/ClickstreamSessionsKinesisGlueAthena13.png\" alt=\"\" width=\"800\" height=\"566\"></p> \n<p><strong>Step 7: </strong>Choose the <strong>Real-time analytics</strong> tab to check the DESTINATION_SQL_STREAM results.</p> \n<p><img class=\"alignnone size-full wp-image-6429\" src=\"https://d2908q01vomqb2.cloudfront.net/b6692ea5df920cad691c20319a6fffd7a4a766b8/2019/02/01/ClickstreamSessionsKinesisGlueAthena14.png\" alt=\"\" width=\"800\" height=\"543\"></p> \n<p> </p> \n<p><img class=\"alignnone size-full wp-image-6430\" src=\"https://d2908q01vomqb2.cloudfront.net/b6692ea5df920cad691c20319a6fffd7a4a766b8/2019/02/01/ClickstreamSessionsKinesisGlueAthena15.png\" alt=\"\" width=\"800\" height=\"187\"></p> \n<p><strong>Step 8:</strong> Check the <strong>Destination</strong> tab to view the AWS Lambda function as the destination to your aggregation.</p> \n<p><img class=\"alignnone size-full wp-image-6431\" src=\"https://d2908q01vomqb2.cloudfront.net/b6692ea5df920cad691c20319a6fffd7a4a766b8/2019/02/01/ClickstreamSessionsKinesisGlueAthena16.png\" alt=\"\" width=\"800\" height=\"226\"></p> \n<p><strong>Step 8:</strong> Check the <a href=\"https://console.aws.amazon.com/cloudwatch/home?region=us-east-1#dashboards:\" target=\"_blank\" rel=\"noopener\">CloudWatch real-time dashboard</a>.</p> \n<p>Open the <strong>Sessionization-&lt;<em>your cloudformation stack name&gt;</em></strong> dashboard.</p> \n<p><img class=\"alignnone size-full wp-image-6432\" src=\"https://d2908q01vomqb2.cloudfront.net/b6692ea5df920cad691c20319a6fffd7a4a766b8/2019/02/01/ClickstreamSessionsKinesisGlueAthena17.png\" alt=\"\" width=\"800\" height=\"44\"></p> \n<p>Check the number of “events” during the sessions, and the “session duration” behavior from a timeframe. Then you can make decisions, such as whether you need to roll back a new site layout or new features of your application.</p> \n<p><img class=\"alignnone size-full wp-image-6433\" src=\"https://d2908q01vomqb2.cloudfront.net/b6692ea5df920cad691c20319a6fffd7a4a766b8/2019/02/01/ClickstreamSessionsKinesisGlueAthena18.png\" alt=\"\" width=\"800\" height=\"484\"></p> \n<p><strong>Step 9:</strong> <a href=\"https://console.aws.amazon.com/glue/home?region=us-east-1#catalog:tab=crawlers\" target=\"_blank\" rel=\"noopener\">Open the AWS Glue console</a> and run the crawler that the AWS CloudFormation template created for you.</p> \n<p>Choose the crawler job, and then choose <strong>Run crawler</strong>.</p> \n<p><img class=\"alignnone size-full wp-image-6435\" src=\"https://d2908q01vomqb2.cloudfront.net/b6692ea5df920cad691c20319a6fffd7a4a766b8/2019/02/01/ClickstreamSessionsKinesisGlueAthena19.png\" alt=\"\" width=\"800\" height=\"165\"></p> \n<h3>Analyze the data</h3> \n<p><strong>Step 1: </strong>After the job finishes, <a href=\"https://console.aws.amazon.com/athena/home?region=us-east-1#query\" target=\"_blank\" rel=\"noopener\">open the Amazon Athena console</a> and explore the data.</p> \n<p>On the Athena console, choose the sessionization database in the list. You should see two tables created based on the data in Amazon S3: <strong>rawdata</strong> and <strong>aggregated</strong>.</p> \n<p><img class=\"alignnone size-full wp-image-6434\" src=\"https://d2908q01vomqb2.cloudfront.net/b6692ea5df920cad691c20319a6fffd7a4a766b8/2019/02/01/ClickstreamSessionsKinesisGlueAthena20.png\" alt=\"\" width=\"670\" height=\"738\"></p> \n<p><strong>Step 2:</strong> Choose the vertical ellipsis (three dots) on the right side to explore each of the tables, as shown in the following screenshots.</p> \n<p><img class=\"alignnone size-full wp-image-6436\" src=\"https://d2908q01vomqb2.cloudfront.net/b6692ea5df920cad691c20319a6fffd7a4a766b8/2019/02/01/ClickstreamSessionsKinesisGlueAthena21.png\" alt=\"\" width=\"800\" height=\"339\"></p> \n<p><img class=\"alignnone size-full wp-image-6437\" src=\"https://d2908q01vomqb2.cloudfront.net/b6692ea5df920cad691c20319a6fffd7a4a766b8/2019/02/01/ClickstreamSessionsKinesisGlueAthena22.png\" alt=\"\" width=\"800\" height=\"336\"></p> \n<p><strong>Step 3:</strong> Create a view on the Athena console to query only today’s data from your aggregated table, as follows:</p> \n<div class=\"hide-language\"> \n <pre><code class=\"lang-sql\">CREATE OR REPLACE VIEW clicks_today AS\nSELECT \n*\nFROM \"aggregated\" \nWHERE\ncast(partition_0 as integer)=year(current_date) and\ncast(partition_1 as integer)=month(current_date) and\ncast(partition_2 as integer)=day(current_date) ;</code></pre> \n</div> \n<p>The successful query appears on the console as follows:</p> \n<p><img class=\"alignnone size-full wp-image-6438\" src=\"https://d2908q01vomqb2.cloudfront.net/b6692ea5df920cad691c20319a6fffd7a4a766b8/2019/02/01/ClickstreamSessionsKinesisGlueAthena23.png\" alt=\"\" width=\"800\" height=\"406\"></p> \n<p><strong>Step 4:</strong> Create a view to query only the current month data from your aggregated table, as in the following example:</p> \n<div class=\"hide-language\"> \n <pre><code class=\"lang-sql\">CREATE OR REPLACE VIEW clicks_month AS\nSELECT \n*\nFROM \"aggregated\" \nWHERE\ncast(partition_0 as integer)=year(current_date) and\ncast(partition_1 as integer)=month(current_date) ;</code></pre> \n</div> \n<p>The successful query appears as follows:</p> \n<p><img class=\"alignnone size-full wp-image-6439\" src=\"https://d2908q01vomqb2.cloudfront.net/b6692ea5df920cad691c20319a6fffd7a4a766b8/2019/02/01/ClickstreamSessionsKinesisGlueAthena24.png\" alt=\"\" width=\"800\" height=\"720\"></p> \n<p><strong>Step 5:</strong> Query data with the sessions grouped by the session duration ordered by sessions, as follows:</p> \n<div class=\"hide-language\"> \n <pre><code class=\"lang-sql\">SELECT duration_sec, count(1) sessions \nFROM \"clicks_today\"\nwhere duration_sec&gt;0\ngroup by duration_sec\norder by sessions desc;</code></pre> \n</div> \n<p>The query results appear as follows:</p> \n<p><img class=\"alignnone size-full wp-image-6440\" src=\"https://d2908q01vomqb2.cloudfront.net/b6692ea5df920cad691c20319a6fffd7a4a766b8/2019/02/01/ClickstreamSessionsKinesisGlueAthena25.png\" alt=\"\" width=\"800\" height=\"605\"></p> \n<h3>Visualize the data</h3> \n<p><strong>Step 1:</strong> Open the <a href=\"https://us-east-1.quicksight.aws.amazon.com/sn/start\" target=\"_blank\" rel=\"noopener\">Amazon QuickSight console</a>.</p> \n<p>If you have never used Amazon QuickSight, <a href=\"https://docs.aws.amazon.com/quicksight/latest/user/signing-up.html\" target=\"_blank\" rel=\"noopener\">perform this setup</a> first.</p> \n<p><strong>Step 2:</strong> Set up <a href=\"https://us-east-1.quicksight.aws.amazon.com/sn/console/resources?\" target=\"_blank\" rel=\"noopener\">Amazon QuickSight account settings</a> to <a href=\"https://docs.aws.amazon.com/quicksight/latest/user/managing-permissions.html\" target=\"_blank\" rel=\"noopener\">access Athena and your S3 bucket</a>.</p> \n<p>First, select the <strong>Amazon Athena</strong> check box. Select the <strong>Amazon S3</strong> check box to edit Amazon QuickSight access to your S3 buckets.</p> \n<p><img class=\"alignnone size-full wp-image-6441\" src=\"https://d2908q01vomqb2.cloudfront.net/b6692ea5df920cad691c20319a6fffd7a4a766b8/2019/02/01/ClickstreamSessionsKinesisGlueAthena26.png\" alt=\"\" width=\"703\" height=\"133\"></p> \n<p>Choose the buckets that you want to make available, and then choose <strong>Select buckets</strong>.</p> \n<p><img class=\"alignnone size-full wp-image-6442\" src=\"https://d2908q01vomqb2.cloudfront.net/b6692ea5df920cad691c20319a6fffd7a4a766b8/2019/02/01/ClickstreamSessionsKinesisGlueAthena27.png\" alt=\"\" width=\"800\" height=\"604\"></p> \n<p><strong>Step 3:</strong> Choose <strong>Manage data</strong>.</p> \n<p><img class=\"alignnone size-full wp-image-6443\" src=\"https://d2908q01vomqb2.cloudfront.net/b6692ea5df920cad691c20319a6fffd7a4a766b8/2019/02/01/ClickstreamSessionsKinesisGlueAthena28.png\" alt=\"\" width=\"800\" height=\"72\"></p> \n<p><strong>Step 4:</strong> Choose <strong>NEW DATASET</strong>.</p> \n<p>In the list of data sources, choose <strong>Athena</strong>.</p> \n<p><img class=\"alignnone size-full wp-image-6444\" src=\"https://d2908q01vomqb2.cloudfront.net/b6692ea5df920cad691c20319a6fffd7a4a766b8/2019/02/01/ClickstreamSessionsKinesisGlueAthena29.png\" alt=\"\" width=\"800\" height=\"285\"></p> \n<p><strong>Step 5:</strong> Enter <strong>daily_session</strong> as your data source name.</p> \n<p><img class=\"alignnone size-full wp-image-6445\" src=\"https://d2908q01vomqb2.cloudfront.net/b6692ea5df920cad691c20319a6fffd7a4a766b8/2019/02/01/ClickstreamSessionsKinesisGlueAthena30.png\" alt=\"\" width=\"765\" height=\"258\"></p> \n<p><strong>Step 6:</strong> Choose the view that you created for daily sessions, and choose <strong>Select</strong>.</p> \n<p><img class=\"alignnone size-full wp-image-6446\" src=\"https://d2908q01vomqb2.cloudfront.net/b6692ea5df920cad691c20319a6fffd7a4a766b8/2019/02/01/ClickstreamSessionsKinesisGlueAthena31.png\" alt=\"\" width=\"576\" height=\"464\"></p> \n<p><strong>Step 7:</strong> Then you can choose to use either SPICE (cache) or direct query access.</p> \n<p><img class=\"alignnone size-full wp-image-6447\" src=\"https://d2908q01vomqb2.cloudfront.net/b6692ea5df920cad691c20319a6fffd7a4a766b8/2019/02/01/ClickstreamSessionsKinesisGlueAthena32.png\" alt=\"\" width=\"577\" height=\"275\"></p> \n<p><strong>Step 8:</strong> Choose <strong>beginnavigation</strong> and <strong>duration_sec</strong> as metrics.</p> \n<p><img class=\"alignnone size-full wp-image-6448\" src=\"https://d2908q01vomqb2.cloudfront.net/b6692ea5df920cad691c20319a6fffd7a4a766b8/2019/02/01/ClickstreamSessionsKinesisGlueAthena33.png\" alt=\"\" width=\"800\" height=\"581\"></p> \n<p><strong>Step 9:</strong> Choose <strong>+Add</strong> to add a new visualization.</p> \n<p><img class=\"alignnone size-full wp-image-6449\" src=\"https://d2908q01vomqb2.cloudfront.net/b6692ea5df920cad691c20319a6fffd7a4a766b8/2019/02/01/ClickstreamSessionsKinesisGlueAthena34.png\" alt=\"\" width=\"118\" height=\"120\"></p> \n<p><strong>Step 10:</strong> In <strong>Visual types</strong>, choose the <strong>Tree map</strong> graph type.</p> \n<p><img class=\"alignnone size-full wp-image-6450\" src=\"https://d2908q01vomqb2.cloudfront.net/b6692ea5df920cad691c20319a6fffd7a4a766b8/2019/02/01/ClickstreamSessionsKinesisGlueAthena35.png\" alt=\"\" width=\"262\" height=\"217\"></p> \n<p><strong>Step 11:</strong> For <strong>Group by</strong>, choose <strong>device_id</strong>; for <strong>Size</strong>, choose <strong>duration_sec (Sum)</strong>; and for <strong>Color</strong>, choose <strong>events (Sum)</strong>.</p> \n<p><img class=\"alignnone size-full wp-image-6451\" src=\"https://d2908q01vomqb2.cloudfront.net/b6692ea5df920cad691c20319a6fffd7a4a766b8/2019/02/01/ClickstreamSessionsKinesisGlueAthena36.png\" alt=\"\" width=\"800\" height=\"508\"></p> \n<h2>Summary</h2> \n<p>In this post, I described how to perform sessionization of clickstream events and analyze them in a serverless architecture. The use of a Kinesis Data Analytics <em>stagger window</em> makes the SQL code short and easy to write and understand. The integration between the services enables a complete data flow with minimal coding.</p> \n<p>You also learned about ways to explore and visualize this data using Amazon Athena, AWS Glue, and Amazon QuickSight.</p> \n<p>To learn more about the Amazon Kinesis family of use cases, check the <a href=\"https://aws.amazon.com/pt/blogs/big-data/category/analytics/amazon-kinesis/\" target=\"_blank\" rel=\"noopener\">Amazon Kinesis Big Data Blog</a> page.</p> \n<p>If you have questions or suggestions, please leave a comment below.</p> \n<h3>Do more with Amazon Kinesis Data Analytics</h3> \n<p>To explore other ways to gain insights using Kinesis Data Analytics, see <a href=\"https://aws.amazon.com/pt/blogs/big-data/real-time-clickstream-anomaly-detection-with-amazon-kinesis-analytics/\" target=\"_blank\" rel=\"noopener\">Real-time Clickstream Anomaly Detection with Amazon Kinesis Analytics</a>.</p> \n<p> </p> \n<hr>\n<h3>About the Author</h3> \n<p><img class=\"size-full wp-image-6458 alignleft\" src=\"https://d2908q01vomqb2.cloudfront.net/b6692ea5df920cad691c20319a6fffd7a4a766b8/2019/02/07/hhug.png\" alt=\"\" width=\"113\" height=\"154\"><strong>Hugo is an analytics and database specialist solutions architect at Amazon Web Services out of São Paulo (Brazil). </strong>He is currently engaged with several Data Lake and Analytics projects for customers in Latin America. He loves family time, dogs and mountain biking.</p> \n<p> </p> \n<p> </p> \n<p> </p> \n<p> </p>\n",
      "enclosure": {},
      "categories": [
        "Amazon Athena",
        "Amazon Kinesis",
        "AWS Glue",
        "Amazon Kinesis Analytics"
      ]
    },
    {
      "title": "Metadata classification, lineage, and discovery using Apache Atlas on Amazon EMR",
      "pubDate": "2019-01-31 15:33:24",
      "link": "https://aws.amazon.com/blogs/big-data/metadata-classification-lineage-and-discovery-using-apache-atlas-on-amazon-emr/",
      "guid": "85d34797b9e0f6373d0584aab478350c9ac32cc5",
      "author": "Nikita Jaggi",
      "thumbnail": "https://d2908q01vomqb2.cloudfront.net/b6692ea5df920cad691c20319a6fffd7a4a766b8/2019/01/30/AtlasonEMR1.jpg",
      "description": "With the ever-evolving and growing role of data in today’s world, data governance is an essential aspect of effective data management. Many organizations use a data lake as a single repository to store data that is in various formats and belongs to a business entity of the organization. The use of metadata, cataloging, and data […]",
      "content": "\n<p>With the ever-evolving and growing role of data in today’s world, data governance is an essential aspect of effective data management. Many organizations use a data lake as a single repository to store data that is in various formats and belongs to a business entity of the organization. The use of metadata, cataloging, and data lineage is key for effective use of the lake.</p> \n<p>This post walks you through how Apache Atlas installed on <a href=\"https://aws.amazon.com/emr/\" target=\"_blank\" rel=\"noopener\">Amazon EMR</a> can provide capability for doing this. You can use this setup to dynamically classify data and view the lineage of data as it moves through various processes. As part of this, you can use a domain-specific language (DSL) in Atlas to search the metadata.</p> \n<h2>Introduction to Amazon EMR and Apache Atlas</h2> \n<p>Amazon EMR is a managed service that simplifies the implementation of big data frameworks such as Apache Hadoop and Spark. If you use Amazon EMR, you can choose from a defined set of applications or choose your own from a list.</p> \n<p><a name=\"_Toc523962241\"></a>Apache Atlas is an enterprise-scale data governance and metadata framework for Hadoop. Atlas provides open metadata management and governance capabilities for organizations to build a catalog of their data assets. Atlas supports classification of data, including storage <em>lineage, </em>which depicts how data has evolved. It also provides features to search for key elements and their business definition.</p> \n<p>Among all the features that Apache Atlas offers, the core feature of our interest in this post is the Apache Hive metadata management and data lineage. After you successfully set up Atlas, it uses a native tool to import Hive tables and analyze the data to present data lineage intuitively to the end users. To read more about Atlas and its features, see <u><a href=\"https://atlas.apache.org/\" target=\"_blank\" rel=\"noopener\">the Atlas website</a></u>.</p> \n<h2>AWS Glue Data Catalog vs. Apache Atlas</h2> \n<p>The <a href=\"https://aws.amazon.com/glue/\" target=\"_blank\" rel=\"noopener\">AWS Glue</a> Data Catalog provides a unified metadata repository across a variety of data sources and data formats. AWS Glue Data Catalog integrates with Amazon EMR, and also <a href=\"https://aws.amazon.com/rds/\" target=\"_blank\" rel=\"noopener\">Amazon RDS</a>, <a href=\"https://aws.amazon.com/redshift/\" target=\"_blank\" rel=\"noopener\">Amazon Redshift</a>, Redshift Spectrum, and <a href=\"https://aws.amazon.com/athena/\" target=\"_blank\" rel=\"noopener\">Amazon Athena</a>. The Data Catalog can work with any application compatible with the Hive metastore.</p> \n<p>The scope of installation of Apache Atlas on Amazon EMR is merely what’s needed for the Hive metastore on Amazon EMR to provide capability for lineage, discovery, and classification. Also, you can use this solution for cataloging for AWS Regions that don’t have AWS Glue.</p> \n<h2>Architecture</h2> \n<p>Apache Atlas requires that you launch an Amazon EMR cluster with prerequisite applications such as Apache Hadoop, HBase, Hue, and Hive. Apache Atlas uses Apache Solr for search functions and Apache HBase for storage. Both Solr and HBase are installed on the persistent Amazon EMR cluster as part of the Atlas installation.</p> \n<p>This solution’s architecture supports both internal and external Hive tables. For the Hive metastore to persist across multiple Amazon EMR clusters, you should use an external Amazon RDS or <a href=\"https://aws.amazon.com/rds/aurora/\" target=\"_blank\" rel=\"noopener\">Amazon Aurora</a> database to contain the metastore. A sample configuration file for the Hive service to reference an external RDS Hive metastore can be found <a href=\"https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-hive-metastore-external.html\" target=\"_blank\" rel=\"noopener\">in the Amazon EMR documentation</a>.</p> \n<p>The following diagram illustrates the architecture of our solution.</p> \n<p><strong><img class=\"alignnone size-full wp-image-6382\" src=\"https://d2908q01vomqb2.cloudfront.net/b6692ea5df920cad691c20319a6fffd7a4a766b8/2019/01/30/AtlasonEMR1.jpg\" alt=\"\" width=\"592\" height=\"566\"></strong></p> \n<h2>Amazon EMR–Apache Atlas workflow</h2> \n<p>To demonstrate the functionality of Apache Atlas, we do the following in this post:</p> \n<ol>\n<li>Launch an Amazon EMR cluster using the AWS CLI or <a href=\"https://aws.amazon.com/cloudformation/\" target=\"_blank\" rel=\"noopener\">AWS CloudFormation</a>\n</li> \n <li>Using Hue, populate external Hive tables</li> \n <li>View the data lineage of a Hive table</li> \n <li>Create a classification</li> \n <li>Discover metadata using the Atlas domain-specific language</li> \n</ol>\n<h3>1a. Launch an Amazon EMR cluster with Apache Atlas using the AWS CLI</h3> \n<p>The steps following guide you through the installation of Atlas on Amazon EMR by using the AWS CLI. This installation creates an Amazon EMR cluster with Hadoop, HBase, Hive, and Zookeeper. It also executes a step in which a script located in an <a href=\"https://aws.amazon.com/s3/\" target=\"_blank\" rel=\"noopener\">Amazon S3</a> bucket runs to install Apache Atlas under the <tt>/apache/atlas</tt> folder.</p> \n<p>The automation shell script assumes the following:</p> \n<ul>\n<li>You have a working local copy of the AWS CLI package configured, with access and secret keys.</li> \n <li>You have a default key pair, VPC, and subnet in the AWS Region where you plan to deploy your cluster.</li> \n <li>You have sufficient permissions to create S3 buckets and Amazon EMR clusters in the default AWS Region configured in the AWS CLI.</li> \n</ul>\n<div class=\"hide-language\"> \n <pre><code class=\"lang-code\">aws emr create-cluster --applications Name=Hive Name=HBase Name=Hue Name=Hadoop Name=ZooKeeper \\\n  --tags Name=\"EMR-Atlas\" \\\n  --release-label emr-5.16.0 \\\n  --ec2-attributes SubnetId=&lt;subnet-xxxxx&gt;,KeyName=&lt;Key Name&gt; \\\n--use-default-roles \\\n--ebs-root-volume-size 100 \\\n  --instance-groups 'InstanceGroupType=MASTER, InstanceCount=1, InstanceType=m4.xlarge, InstanceGroupType=CORE, InstanceCount=1, InstanceType=m4.xlarge \\\n  --log-uri ‘&lt;S3 location for logging&gt;’ \\\n--steps Name='Run Remote Script',Jar=command-runner.jar,Args=[bash,-c,'curl https://s3.amazonaws.com/aws-bigdata-blog/artifacts/aws-blog-emr-atlas/apache-atlas-emr.sh -o /tmp/script.sh; chmod +x /tmp/script.sh; /tmp/script.sh']</code></pre> \n</div> \n<p>On successful execution of the command, output containing a cluster ID is displayed:</p> \n<div class=\"hide-language\"> \n <pre><code class=\"lang-code\">{\n    \"ClusterId\": \"j-2V3BNEB9XQ54H\"\n}\n</code></pre> \n</div> \n<p>Use the following command to list the names of active clusters (your cluster shows on the list after it is ready):</p> \n<div class=\"hide-language\"> \n <pre><code class=\"lang-code\">aws emr list-clusters --active</code></pre> \n</div> \n<p>In the output of the previous command, look for the server name <tt>EMR-Atlas</tt> (unless you changed the default name in the script). If you have the jq command line utility available, you can run the following command to filter everything but the name and its cluster ID:</p> \n<div class=\"hide-language\"> \n <pre><code class=\"lang-code\">aws emr list-clusters --active | jq '.[][] | {(.Name): .Id}'\nSample output:\n{\n  \"external hive store on rds-external-store\": \"j-1MO3L3XSXZ45V\"\n}\n{\n  \"EMR-Atlas\": \"j-301TZ1GBCLK4K\"\n}\n</code></pre> \n</div> \n<p>After your cluster shows up on the active list, Amazon EMR and Atlas are ready for operation.</p> \n<h3>1b. Launch an Amazon EMR cluster with Apache Atlas using AWS CloudFormation</h3> \n<p>You can also launch your cluster with CloudFormation. Use the <a href=\"https://s3.amazonaws.com/aws-bigdata-blog/artifacts/aws-blog-emr-atlas/emr-atlas.template\" target=\"_blank\" rel=\"noopener\">emr-atlas.template</a> to set up your Amazon EMR cluster, or launch directly from the AWS Management Console by using this button:</p> \n<p><a href=\"https://console.aws.amazon.com/cloudformation/home?region=us-east-1#/stacks/new?stackName=EMR-Atlas-Cluster&amp;templateURL=https://s3.amazonaws.com/aws-bigdata-blog/artifacts/aws-blog-emr-atlas/emr-atlas.template\" target=\"_blank\" rel=\"noopener\"><img class=\"alignnone size-full wp-image-5580\" src=\"https://d2908q01vomqb2.cloudfront.net/b6692ea5df920cad691c20319a6fffd7a4a766b8/2018/09/07/LaunchStack.png\" alt=\"\" width=\"234\" height=\"60\"></a></p> \n<p>To launch, provide values for the following parameters:</p> \n<table border=\"1\" cellpadding=\"10\"><tbody>\n<tr>\n<td width=\"349\"><strong>VPC</strong></td> \n   <td width=\"349\">&lt;VPC&gt;</td> \n  </tr>\n<tr>\n<td width=\"349\"><strong>Subnet</strong></td> \n   <td width=\"349\">&lt;Subnet&gt;</td> \n  </tr>\n<tr>\n<td width=\"349\"><strong>EMRLogDir</strong></td> \n   <td width=\"349\">&lt; Amazon EMR logging directory, for example s3://xxx &gt;</td> \n  </tr>\n<tr>\n<td width=\"349\"><strong>KeyName</strong></td> \n   <td width=\"349\">&lt; EC2 key pair name &gt;</td> \n  </tr>\n</tbody></table>\n<p>Provisioning an Amazon EMR cluster by using the CloudFormation template achieves the same result as the CLI commands outlined previously.</p> \n<p>Before proceeding, wait until the CloudFormation stack events show that the status of the stack has reached “<tt>CREATE_COMPLETE</tt>”.</p> \n<h3>2. Use Hue to create Hive tables</h3> \n<p>Next, you log in to Apache Atlas and Hue and use Hue to create Hive tables.</p> \n<p>To log in to Atlas, first find the master public DNS name in the cluster installation by using the Amazon EMR Management Console. Then, use the following command to create a Secure Shell (SSH) tunnel to the Atlas web browser.</p> \n<div class=\"hide-language\"> \n <pre><code class=\"lang-code\">ssh -L 21000:localhost:21000 -i key.pem hadoop@&lt;EMR Master IP Address&gt;</code></pre> \n</div> \n<p>If the command preceding doesn’t work, make sure that your key file (*.pem) has appropriate permissions. You also might have to add an inbound rule for SSH (port 22) to the master’s security group.</p> \n<p>After successfully creating an SSH tunnel, use following URL to access the Apache Atlas UI.</p> \n<div class=\"hide-language\"> \n <pre><code class=\"lang-code\">http://localhost:21000</code></pre> \n</div> \n<p>You should see a screen like that shown following. The default login details are username <tt>admin</tt> and password <tt>admin</tt>.</p> \n<p><img class=\"alignnone size-full wp-image-6383\" src=\"https://d2908q01vomqb2.cloudfront.net/b6692ea5df920cad691c20319a6fffd7a4a766b8/2019/01/30/AtlasonEMR2.png\" alt=\"\" width=\"800\" height=\"383\"></p> \n<p>To set up a web interface for Hue, follow <a href=\"https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-web-interfaces.html\" target=\"_blank\" rel=\"noopener\">the steps in the Amazon EMR documentation</a>. As you did for Apache Atlas, create an SSH tunnel on remote port 8888 for the console access:</p> \n<div class=\"hide-language\"> \n <pre><code class=\"lang-code\">ssh -L 8888:localhost:8888 -i key.pem hadoop@&lt;EMR Master IP Address&gt;</code></pre> \n</div> \n<p>After the tunnel is up, use following URL for Hue console access.</p> \n<div class=\"hide-language\"> \n <pre><code class=\"lang-code\">http://localhost:8888/</code></pre> \n</div> \n<p>At first login, you are asked to create a Hue superuser, as shown following. Do <em>not</em> lose the superuser credentials.</p> \n<p><img class=\"alignnone size-full wp-image-6384\" src=\"https://d2908q01vomqb2.cloudfront.net/b6692ea5df920cad691c20319a6fffd7a4a766b8/2019/01/30/AtlasonEMR3.png\" alt=\"\" width=\"416\" height=\"568\"></p> \n<p>After creating the Hue superuser, you can use the Hue console to run hive queries.</p> \n<p><img class=\"alignnone size-full wp-image-6385\" src=\"https://d2908q01vomqb2.cloudfront.net/b6692ea5df920cad691c20319a6fffd7a4a766b8/2019/01/30/AtlasonEMR4.png\" alt=\"\" width=\"800\" height=\"317\"></p> \n<p>After you log in to Hue, take the following steps and run the following Hive queries:</p> \n<ol>\n<li> \n  <ul>\n<li>Run the HQL to create a new database:</li> \n  </ul>\n</li> \n</ol>\n<div class=\"hide-language\"> \n <pre><code class=\"lang-code\">create database atlas_emr;\nuse atlas_emr;</code></pre> \n</div> \n<ol>\n<li> \n  <ul>\n<li>Create a new external table called <tt>trip_details</tt> with data stored on S3. Change the S3 location to a bucket you own.</li> \n  </ul>\n</li> \n</ol>\n<div class=\"hide-language\"> \n <pre><code class=\"lang-code\">CREATE external TABLE trip_details\n(\n  pickup_date        string ,\n  pickup_time        string ,\n  location_id        int ,\n  trip_time_in_secs  int ,\n  trip_number        int ,\n  dispatching_app    string ,\n  affiliated_app     string \n)\nrow format delimited\nfields terminated by ',' stored as textfile\nLOCATION 's3://aws-bigdata-blog/artifacts/aws-blog-emr-atlas/trip_details/';</code></pre> \n</div> \n<ol>\n<li> \n  <ul>\n<li>Create a new lookup external table called <tt>trip_zone_lookup</tt> with data stored on S3.</li> \n  </ul>\n</li> \n</ol>\n<div class=\"hide-language\"> \n <pre><code class=\"lang-code\">CREATE external TABLE trip_zone_lookup \n(\nLocationID     int ,\nBorough        string ,\nZone           string ,\nservice_zone   string\n)\nrow format delimited\nfields terminated by ',' stored as textfile\nLOCATION 's3://aws-bigdata-blog/artifacts/aws-blog-emr-atlas/zone_lookup/';</code></pre> \n</div> \n<ol>\n<li> \n  <ul>\n<li>Create an intersect table of <tt>trip_details</tt> and <tt>trip_zone_lookup</tt> by joining these tables:</li> \n  </ul>\n</li> \n</ol>\n<div class=\"hide-language\"> \n <pre><code class=\"lang-code\">create table trip_details_by_zone as select *  from trip_details  join trip_zone_lookup on LocationID = location_id;</code></pre> \n</div> \n<p>Next, you perform the Hive import. For metadata to be imported in Atlas, the Atlas Hive import tool is only available by using the command line on the Amazon EMR server (there’s no web UI.)  To start, log in to the Amazon EMR master by using SSH:</p> \n<div class=\"hide-language\"> \n <pre><code class=\"lang-code\">ssh -i key.pem hadoop@&lt;EMR Master IP Address&gt;</code></pre> \n</div> \n<p>Then execute the following command. The script asks for your user name and password for Atlas. The default user name is <tt>admin</tt> and password is <tt>admin</tt>.</p> \n<div class=\"hide-language\"> \n <pre><code class=\"lang-code\">/apache/atlas/bin/import-hive.sh</code></pre> \n</div> \n<p>A successful import looks like the following:</p> \n<div class=\"hide-language\"> \n <pre><code class=\"lang-code\">Enter username for atlas :- admin\nEnter password for atlas :- \n2018-09-06T13:23:33,519 INFO [main] org.apache.atlas.AtlasBaseClient - Client has only one service URL, will use that for all actions: http://localhost:21000\n2018-09-06T13:23:33,543 INFO [main] org.apache.hadoop.hive.conf.HiveConf - Found configuration file file:/etc/hive/conf.dist/hive-site.xml\n2018-09-06T13:23:34,394 WARN [main] org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n2018-09-06T13:23:35,272 INFO [main] hive.metastore - Trying to connect to metastore with URI thrift://ip-172-31-90-79.ec2.internal:9083\n2018-09-06T13:23:35,310 INFO [main] hive.metastore - Opened a connection to metastore, current connections: 1\n2018-09-06T13:23:35,365 INFO [main] hive.metastore - Connected to metastore.\n2018-09-06T13:23:35,591 INFO [main] org.apache.atlas.hive.bridge.HiveMetaStoreBridge - Importing Hive metadata\n2018-09-06T13:23:35,602 INFO [main] org.apache.atlas.hive.bridge.HiveMetaStoreBridge - Found 2 databases\n2018-09-06T13:23:35,713 INFO [main] org.apache.atlas.AtlasBaseClient - method=GET path=api/atlas/v2/entity/uniqueAttribute/type/ contentType=application/json; charset=UTF-8 accept=application/json status=200\n2018-09-06T13:23:35,987 INFO [main] org.apache.atlas.hive.bridge.HiveMetaStoreBridge - Database atlas_emr is already registered - id=cc311c0e-df88-40dc-ac12-6a1ce139ca88. Updating it.\n2018-09-06T13:23:36,130 INFO [main] org.apache.atlas.AtlasBaseClient - method=POST path=api/atlas/v2/entity/ contentType=application/json; charset=UTF-8 accept=application/json status=200\n2018-09-06T13:23:36,144 INFO [main] org.apache.atlas.hive.bridge.HiveMetaStoreBridge - Updated hive_db entity: name=atlas_emr@primary, guid=cc311c0e-df88-40dc-ac12-6a1ce139ca88\n2018-09-06T13:23:36,164 INFO [main] org.apache.atlas.hive.bridge.HiveMetaStoreBridge - Found 3 tables to import in database atlas_emr\n2018-09-06T13:23:36,287 INFO [main] org.apache.atlas.AtlasBaseClient - method=GET path=api/atlas/v2/entity/uniqueAttribute/type/ contentType=application/json; charset=UTF-8 accept=application/json status=200\n2018-09-06T13:23:36,294 INFO [main] org.apache.atlas.hive.bridge.HiveMetaStoreBridge - Table atlas_emr.trip_details is already registered with id c2935940-5725-4bb3-9adb-d153e2e8b911. Updating entity.\n2018-09-06T13:23:36,688 INFO [main] org.apache.atlas.AtlasBaseClient - method=POST path=api/atlas/v2/entity/ contentType=application/json; charset=UTF-8 accept=application/json status=200\n2018-09-06T13:23:36,689 INFO [main] org.apache.atlas.hive.bridge.HiveMetaStoreBridge - Updated hive_table entity: name=atlas_emr.trip_details@primary, guid=c2935940-5725-4bb3-9adb-d153e2e8b911\n2018-09-06T13:23:36,702 INFO [main] org.apache.atlas.AtlasBaseClient - method=GET path=api/atlas/v2/entity/uniqueAttribute/type/ contentType=application/json; charset=UTF-8 accept=application/json status=200\n2018-09-06T13:23:36,703 INFO [main] org.apache.atlas.hive.bridge.HiveMetaStoreBridge - Process atlas_emr.trip_details@primary:1536239968000 is already registered\n2018-09-06T13:23:36,791 INFO [main] org.apache.atlas.AtlasBaseClient - method=GET path=api/atlas/v2/entity/uniqueAttribute/type/ contentType=application/json; charset=UTF-8 accept=application/json status=200\n2018-09-06T13:23:36,802 INFO [main] org.apache.atlas.hive.bridge.HiveMetaStoreBridge - Table atlas_emr.trip_details_by_zone is already registered with id c0ff33ae-ca82-4048-9671-c0b6597e1475. Updating entity.\n2018-09-06T13:23:36,988 INFO [main] org.apache.atlas.AtlasBaseClient - method=POST path=api/atlas/v2/entity/ contentType=application/json; charset=UTF-8 accept=application/json status=200\n2018-09-06T13:23:36,989 INFO [main] org.apache.atlas.hive.bridge.HiveMetaStoreBridge - Updated hive_table entity: name=atlas_emr.trip_details_by_zone@primary, guid=c0ff33ae-ca82-4048-9671-c0b6597e1475\n2018-09-06T13:23:37,035 INFO [main] org.apache.atlas.AtlasBaseClient - method=GET path=api/atlas/v2/entity/uniqueAttribute/type/ contentType=application/json; charset=UTF-8 accept=application/json status=200\n2018-09-06T13:23:37,038 INFO [main] org.apache.atlas.hive.bridge.HiveMetaStoreBridge - Table atlas_emr.trip_zone_lookup is already registered with id 834d102a-6f92-4fc9-a498-4adb4a3e7897. Updating entity.\n2018-09-06T13:23:37,213 INFO [main] org.apache.atlas.AtlasBaseClient - method=POST path=api/atlas/v2/entity/ contentType=application/json; charset=UTF-8 accept=application/json status=200\n2018-09-06T13:23:37,214 INFO [main] org.apache.atlas.hive.bridge.HiveMetaStoreBridge - Updated hive_table entity: name=atlas_emr.trip_zone_lookup@primary, guid=834d102a-6f92-4fc9-a498-4adb4a3e7897\n2018-09-06T13:23:37,228 INFO [main] org.apache.atlas.AtlasBaseClient - method=GET path=api/atlas/v2/entity/uniqueAttribute/type/ contentType=application/json; charset=UTF-8 accept=application/json status=200\n2018-09-06T13:23:37,228 INFO [main] org.apache.atlas.hive.bridge.HiveMetaStoreBridge - Process atlas_emr.trip_zone_lookup@primary:1536239987000 is already registered\n2018-09-06T13:23:37,229 INFO [main] org.apache.atlas.hive.bridge.HiveMetaStoreBridge - Successfully imported 3 tables from database atlas_emr\n2018-09-06T13:23:37,243 INFO [main] org.apache.atlas.AtlasBaseClient - method=GET path=api/atlas/v2/entity/uniqueAttribute/type/ contentType=application/json; charset=UTF-8 accept=application/json status=404\n2018-09-06T13:23:37,353 INFO [main] org.apache.atlas.AtlasBaseClient - method=POST path=api/atlas/v2/entity/ contentType=application/json; charset=UTF-8 accept=application/json status=200\n2018-09-06T13:23:37,361 INFO [main] org.apache.atlas.AtlasBaseClient - method=GET path=api/atlas/v2/entity/guid/ contentType=application/json; charset=UTF-8 accept=application/json status=200\n2018-09-06T13:23:37,362 INFO [main] org.apache.atlas.hive.bridge.HiveMetaStoreBridge - Created hive_db entity: name=default@primary, guid=798fab06-ad75-4324-b7cd-e4d02b6525e8\n2018-09-06T13:23:37,365 INFO [main] org.apache.atlas.hive.bridge.HiveMetaStoreBridge - No tables to import in database default\nHive Meta Data imported successfully!!!</code></pre> \n</div> \n<p>After a successful Hive import, you can return to the Atlas Web UI to search the Hive database or the tables that were imported. On the left pane of the Atlas UI, ensure <strong>Search</strong> is selected, and enter the following information in the two fields listed following:</p> \n<ol>\n<li> \n  <ol>\n<li> \n    <ul>\n<li>\n<strong>Search By Type</strong>: <tt>hive_table</tt>\n</li> \n     <li>\n<strong>Search By Text</strong>: <tt>trip_details</tt>\n</li> \n    </ul>\n</li> \n  </ol>\n</li> \n</ol>\n<p>The output of the preceding query should look like this:</p> \n<p><strong><img class=\"alignnone size-full wp-image-6386\" src=\"https://d2908q01vomqb2.cloudfront.net/b6692ea5df920cad691c20319a6fffd7a4a766b8/2019/01/30/AtlasonEMR5.png\" alt=\"\" width=\"800\" height=\"331\"></strong></p> \n<h3>3. View the data lineage of your Hive tables using Atlas</h3> \n<p>To view the lineage of the created tables, you can use the Atlas web search. For example, to see the lineage of the intersect table <tt>trip_details_by_zone</tt> created earlier, enter the following information:</p> \n<ol>\n<li> \n  <ol>\n<li> \n    <ul>\n<li>\n<strong>Search By Type:</strong> <tt>hive_table</tt>\n</li> \n     <li>\n<strong>Search By Text: </strong><tt>trip_details_by_zone</tt>\n</li> \n    </ul>\n</li> \n  </ol>\n</li> \n</ol>\n<p>The output of the preceding query should look like this:</p> \n<p><img class=\"alignnone size-full wp-image-6387\" src=\"https://d2908q01vomqb2.cloudfront.net/b6692ea5df920cad691c20319a6fffd7a4a766b8/2019/01/30/AtlasonEMR6.png\" alt=\"\" width=\"800\" height=\"336\"></p> \n<p>Now choose the table name <tt>trip_details_by_zone</tt> to view the details of the table as shown following.</p> \n<p><img class=\"alignnone size-full wp-image-6388\" src=\"https://d2908q01vomqb2.cloudfront.net/b6692ea5df920cad691c20319a6fffd7a4a766b8/2019/01/30/AtlasonEMR7.png\" alt=\"\" width=\"800\" height=\"429\"></p> \n<p>Now when you choose <strong>Lineage</strong>, you should see the lineage of the table. As shown following, the lineage provides information about its base tables and is an intersect table of two tables.</p> \n<p><img class=\"alignnone size-full wp-image-6389\" src=\"https://d2908q01vomqb2.cloudfront.net/b6692ea5df920cad691c20319a6fffd7a4a766b8/2019/01/30/AtlasonEMR8.png\" alt=\"\" width=\"800\" height=\"529\"></p> \n<h3>4. Create a classification for metadata management</h3> \n<p>Atlas can help you to classify your metadata to comply with data governance requirements specific to your organization. We create an example classification next.</p> \n<p>To create a classification, take the following steps</p> \n<ol>\n<li> \n  <ol>\n<li> \n    <ol>\n<li>Choose Classification from the left pane, and choose the +</li> \n     <li>Type <tt>PII</tt> in the <strong>Name</strong> field, and <tt>Personally Identifiable Information</tt> in the <strong>Description</strong>\n</li> \n     <li>Choose <strong>Create</strong>.</li> \n    </ol>\n</li> \n  </ol>\n</li> \n</ol>\n<p><img class=\"alignnone size-full wp-image-6390\" src=\"https://d2908q01vomqb2.cloudfront.net/b6692ea5df920cad691c20319a6fffd7a4a766b8/2019/01/30/AtlasonEMR9.png\" alt=\"\" width=\"793\" height=\"568\"></p> \n<p>Next, classify the table as PII:</p> \n<ol>\n<li> \n  <ol>\n<li> \n    <ol>\n<li>Return to the <strong>Search</strong> tab on the left pane.</li> \n     <li>In the <strong>Search By Text</strong> field, type: <tt>trip_zone_lookup</tt>\n</li> \n    </ol>\n</li> \n  </ol>\n</li> \n</ol>\n<p><img class=\"alignnone size-full wp-image-6391\" src=\"https://d2908q01vomqb2.cloudfront.net/b6692ea5df920cad691c20319a6fffd7a4a766b8/2019/01/30/AtlasonEMR10.png\" alt=\"\" width=\"800\" height=\"330\"></p> \n<ol>\n<li> \n  <ol>\n<li> \n    <ol start=\"3\">\n<li>Choose the <strong>Classification</strong> tab and choose the add icon (<strong>+</strong>).</li> \n     <li>Choose the classification that you created (<tt>PII</tt>) from the list.</li> \n    </ol>\n</li> \n  </ol>\n</li> \n</ol>\n<p><img class=\"alignnone size-full wp-image-6392\" src=\"https://d2908q01vomqb2.cloudfront.net/b6692ea5df920cad691c20319a6fffd7a4a766b8/2019/01/30/AtlasonEMR11.png\" alt=\"\" width=\"800\" height=\"180\"></p> \n<ol>\n<li> \n  <ol>\n<li> \n    <ol start=\"5\">\n<li>Choose <strong>Add</strong>.</li> \n    </ol>\n</li> \n  </ol>\n</li> \n</ol>\n<p>You can classify columns and databases in a similar manner.</p> \n<p>Next, view all the entities belonging to this classification.</p> \n<ol>\n<li> \n  <ol>\n<li> \n    <ol>\n<li>Choose the <strong>Classification</strong> tab.</li> \n     <li>Choose the <tt>PII</tt> classification that you created.</li> \n     <li>View all the entities belonging to this classification, displayed on the main pane.</li> \n    </ol>\n</li> \n  </ol>\n</li> \n</ol>\n<p><img class=\"alignnone size-full wp-image-6393\" src=\"https://d2908q01vomqb2.cloudfront.net/b6692ea5df920cad691c20319a6fffd7a4a766b8/2019/01/30/AtlasonEMR12.png\" alt=\"\" width=\"800\" height=\"284\"></p> \n<p>5. Discover metadata using the Atlas domain-specific language (DSL)</p> \n<p>Next, you can search Atlas for entities using the Atlas domain-specific language (DSL), which is a SQL-like query language. This language has simple constructs that help users navigate Atlas data repositories. The syntax loosely emulates the popular SQL from the relational database world.</p> \n<p>To search a table using DSL:</p> \n<ol>\n<li> \n  <ol>\n<li> \n    <ol>\n<li>Choose <strong>Search</strong>.</li> \n     <li>Choose <strong>Advanced Search</strong>.</li> \n     <li>In <strong>Search By Type</strong>, choose <tt>hive_table</tt>.</li> \n     <li>In <strong>Search By Query</strong>, search for the table <tt>trip_details</tt> using the following DSL snippet:</li> \n    </ol>\n</li> \n  </ol>\n</li> \n</ol>\n<div class=\"hide-language\"> \n <pre><code class=\"lang-code\">from hive_table where name = trip_details</code></pre> \n</div> \n<p>As shown following, Atlas shows the table’s schema, lineage, and classification information.</p> \n<p><img class=\"alignnone size-full wp-image-6394\" src=\"https://d2908q01vomqb2.cloudfront.net/b6692ea5df920cad691c20319a6fffd7a4a766b8/2019/01/30/AtlasonEMR13.png\" alt=\"\" width=\"800\" height=\"251\"></p> \n<p>Next, search a column using DSL:</p> \n<ol>\n<li> \n  <ol>\n<li> \n    <ol>\n<li>Choose <strong>Search</strong>.</li> \n     <li>Choose <strong>Advanced Search</strong>.</li> \n     <li>In <strong>Search By Type</strong>, choose <tt>hive_column</tt>.</li> \n     <li>In <strong>Search By Que</strong>ry, search for <tt>column location_id</tt> using the following DSL snippet:</li> \n    </ol>\n</li> \n  </ol>\n</li> \n</ol>\n<div class=\"hide-language\"> \n <pre><code class=\"lang-code\">from hive_column where name = 'location_id'</code></pre> \n</div> \n<p>As shown following, Atlas shows the existence of column <tt>location_id</tt> in both of the tables created previously:</p> \n<p><img class=\"alignnone size-full wp-image-6395\" src=\"https://d2908q01vomqb2.cloudfront.net/b6692ea5df920cad691c20319a6fffd7a4a766b8/2019/01/30/AtlasonEMR14.png\" alt=\"\" width=\"800\" height=\"253\"></p> \n<p>You can also count tables using DSL:</p> \n<ol>\n<li> \n  <ol>\n<li> \n    <ol>\n<li>Choose <strong>Search</strong>.</li> \n     <li>Choose <strong>Advanced Search</strong>.</li> \n     <li>In <strong>Search By Type</strong>, choose <tt>hive_table</tt>.</li> \n     <li>In <strong>Search By Query</strong>, search for table store using the following DSL command:</li> \n    </ol>\n</li> \n  </ol>\n</li> \n</ol>\n<div class=\"hide-language\"> \n <pre><code class=\"lang-code\">hive_table select count()</code></pre> \n</div> \n<p>As shown following, Atlas shows the total number of tables.</p> \n<p><img class=\"alignnone size-full wp-image-6396\" src=\"https://d2908q01vomqb2.cloudfront.net/b6692ea5df920cad691c20319a6fffd7a4a766b8/2019/01/30/AtlasonEMR15.png\" alt=\"\" width=\"800\" height=\"250\"></p> \n<p>The final step is to clean up. To avoid unnecessary charges, you should remove your Amazon EMR cluster after you’re done experimenting with it.</p> \n<p>The simplest way to do so, if you used CloudFormation, is to remove the CloudFormation stack that you created earlier. By default, the cluster is created with termination protection enabled. To remove the cluster, you first need to turn termination protection off, which you can do by using the Amazon EMR console.</p> \n<h2>Conclusion</h2> \n<p>In this post, we outline the steps required to install and configure an Amazon EMR cluster with Apache Atlas by using the AWS CLI or CloudFormation. We also explore how you can import data into Atlas and use the Atlas console to perform queries and view the lineage of our data artifacts.</p> \n<p>For more information about Amazon EMR or any other big data topics on AWS, see <a href=\"https://aws.amazon.com/blogs/big-data/tag/amazon-emr\" target=\"_blank\" rel=\"noopener\">the EMR blog posts on the AWS Big Data blog</a>.</p> \n<p> </p> \n<hr>\n<h3>About the Authors</h3> \n<p><strong><img class=\"size-full wp-image-6401 alignleft\" src=\"https://d2908q01vomqb2.cloudfront.net/b6692ea5df920cad691c20319a6fffd7a4a766b8/2019/01/30/njaggi.png\" alt=\"\" width=\"113\" height=\"144\">Nikita Jaggi is a senior big data consultant with AWS.</strong></p> \n<p> </p> \n<p> </p> \n<p> </p> \n<p> </p> \n<p><img class=\"size-full wp-image-6402 alignleft\" src=\"https://d2908q01vomqb2.cloudfront.net/b6692ea5df920cad691c20319a6fffd7a4a766b8/2019/01/30/npark.png\" alt=\"\" width=\"113\" height=\"151\"><strong>Andrew Park is a cloud infrastructure architect at AWS. </strong>In addition to being operationally focused in customer engagements, he often works directly with customers to build and to deliver custom AWS solutions.  Having been a Linux solutions engineer for a long time, Andrew loves deep dives into Linux-related challenges. He is an open source advocate, loves baseball, is a recent winner of the “Happy Camper” award in local AWS practice, and loves being helpful in all contexts.<strong><br></strong></p>\n",
      "enclosure": {},
      "categories": [
        "Amazon EMR"
      ]
    },
    {
      "title": "Our data lake story: How Woot.com built a serverless data lake on AWS",
      "pubDate": "2019-01-22 20:00:56",
      "link": "https://aws.amazon.com/blogs/big-data/our-data-lake-story-how-woot-com-built-a-serverless-data-lake-on-aws/",
      "guid": "fdc5417745d57a39b9707eff98caf401ac6cf376",
      "author": "Karthik Kumar Odapally",
      "thumbnail": "https://d2908q01vomqb2.cloudfront.net/b6692ea5df920cad691c20319a6fffd7a4a766b8/2019/01/22/DataLakeWoot_1.png",
      "description": "In this post, we talk about designing a cloud-native data warehouse as a replacement for our legacy data warehouse built on a relational database. At the beginning of the design process, the simplest solution appeared to be a straightforward lift-and-shift migration from one relational database to another. However, we decided to step back and focus […]",
      "content": "\n<p>In this post, we talk about designing a cloud-native data warehouse as a replacement for our legacy data warehouse built on a relational database.</p> \n<p>At the beginning of the design process, the simplest solution appeared to be a straightforward lift-and-shift migration from one relational database to another. However, we decided to step back and focus first on what we really needed out of a data warehouse. We started looking at how we could decouple our legacy Oracle database into smaller microservices, using the right tool for the right job. Our process wasn’t just about using the AWS tools. More, it was about having a mind shift to use cloud-native technologies to get us to our final state.</p> \n<p>This migration required developing new extract, transform, load (ETL) pipelines to get new data flowing in while also migrating existing data. Because of this migration, we were able to deprecate multiple servers and move to a fully serverless data warehouse orchestrated by <a href=\"https://aws.amazon.com/glue/\" target=\"_blank\" rel=\"noopener\">AWS Glue</a>.</p> \n<p>In this blog post, we are going to show you:</p> \n<ul>\n<li>Why we chose a serverless data lake for our data warehouse.</li> \n <li>An architectural diagram of Woot’s systems.</li> \n <li>An overview of the migration project.</li> \n <li>Our migration results.</li> \n</ul>\n<h2>Architectural and design concerns</h2> \n<p>Here are some of the design points that we considered:</p> \n<ul>\n<li>Customer experience. We always start with what our customer needs, and then work backwards from there. Our data warehouse is used across the business by people with varying level of technical expertise. We focused on the ability for different types of users to gain insights into their operations and to provide better feedback mechanisms to improve the overall customer experience.</li> \n <li>Minimal infrastructure maintenance. The “Woot data warehouse team” is really just one person—Chaya! Because of this, it’s important for us to focus on AWS services that enable us to use cloud-native technologies. These remove the undifferentiated heavy lifting of managing infrastructure as demand changes and technologies evolve.</li> \n <li>Responsiveness to data source changes. Our data warehouse gets data from a range of internal services. In our existing data warehouse, any updates to those services required manual updates to ETL jobs and tables. The response times for these data sources are critical to our key stakeholders. This requires us to take a data-driven approach to selecting a high-performance architecture.</li> \n <li>Separation from production systems. Access to our production systems is tightly coupled. To allow multiple users, we needed to decouple it from our production systems and minimize the complexities of navigating resources in multiple VPCs.</li> \n</ul>\n<p>Based on these requirements, we decided to change the data warehouse both operationally and architecturally. From an operational standpoint, we designed a new shared responsibility model for data ingestion. Architecturally, we chose a serverless model over a traditional relational database. These two decisions ended up driving every design and implementation decision that we made in our migration.</p> \n<p>As we moved to a shared responsibility model, several important points came up. First, our new way of data ingestion was a major cultural shift for Woot’s technical organization. In the past, data ingestion had been exclusively the responsibility of the data warehouse team and required customized pipelines to pull data from services. We decided to shift to “push, not pull”: Services should send data to the data warehouse.</p> \n<p>This is where shared responsibility came in. For the first time, our development teams had ownership over their services’ data in the data warehouse. However, we didn’t want our developers to have to become mini data engineers. Instead, we had to give them an easy way to push data that fit with the existing skill set of a developer. The data also needed to be accessible by the range of technologies used by our website.</p> \n<p>These considerations led us to select the following AWS services for our serverless data warehouse:</p> \n<ul>\n<li>\n<a href=\"https://aws.amazon.com/kinesis/data-firehose/\" target=\"_blank\" rel=\"noopener\">Amazon Kinesis Data Firehose</a> for data ingestion</li> \n <li>\n<a href=\"https://aws.amazon.com/s3/\" target=\"_blank\" rel=\"noopener\">Amazon S3</a> for data storage</li> \n <li>\n<a href=\"https://aws.amazon.com/lambda/\" target=\"_blank\" rel=\"noopener\">AWS Lambda</a> and <a href=\"https://aws.amazon.com/glue/\" target=\"_blank\" rel=\"noopener\">AWS Glue</a> for data processing</li> \n <li>\n<a href=\"https://aws.amazon.com/dms/\" target=\"_blank\" rel=\"noopener\">AWS Data Migration Service</a> (AWS DMS) and AWS Glue for data migration</li> \n <li>\n<a href=\"https://aws.amazon.com/glue/\" target=\"_blank\" rel=\"noopener\">AWS Glue</a> for orchestration and metadata management</li> \n <li>\n<a href=\"https://aws.amazon.com/athena/\" target=\"_blank\" rel=\"noopener\">Amazon Athena</a> and <a href=\"https://aws.amazon.com/quicksight/\" target=\"_blank\" rel=\"noopener\">Amazon QuickSight</a> for querying and data visualization</li> \n</ul>\n<p>The following diagram shows at a high level how we use these services.</p> \n<p><img class=\"alignnone size-full wp-image-6370\" src=\"https://d2908q01vomqb2.cloudfront.net/b6692ea5df920cad691c20319a6fffd7a4a766b8/2019/01/22/DataLakeWoot_1.png\" alt=\"\" width=\"800\" height=\"371\"></p> \n<h2>Tradeoffs</h2> \n<p>These components together met all of our requirements and enabled our shared responsibility model. However, we made few tradeoffs compared to a lift-and-shift migration to another relational database:</p> \n<ul>\n<li>The biggest tradeoff was upfront effort vs. ongoing maintenance. We effectively had to start from scratch with all of our data pipelines and introduce a new technology into all of our website services, which required a concerted effort across multiple teams. Minimal ongoing maintenance was a core requirement. We were willing to make this tradeoff to take advantage of the managed infrastructure of the serverless components that we use.</li> \n <li>Another tradeoff was balancing usability for nontechnical users vs. taking advantage of big data technologies. Making customer experience a core requirement helped us navigate the decision-making when considering these tradeoffs. Ultimately, only switching to another relational database would mean that our customers would have the same experience, not a better one.</li> \n</ul>\n<h2>Building data pipelines with Kinesis Data Firehose and Lambda</h2> \n<p><img class=\"alignnone size-full wp-image-6371\" src=\"https://d2908q01vomqb2.cloudfront.net/b6692ea5df920cad691c20319a6fffd7a4a766b8/2019/01/22/DataLakeWoot_2.png\" alt=\"\" width=\"800\" height=\"422\"></p> \n<p>Because our site already runs on AWS, using an AWS SDK to send data to Kinesis Data Firehose was an easy sell to developers. Things like the following were considerations:</p> \n<ul>\n<li>Direct PUT ingestion for Kinesis Data Firehose is natural for developers to implement, works in all languages used across our services, and delivers data to Amazon S3.</li> \n <li>Using S3 for data storage means that we automatically get high availability, scalability, and durability. And because S3 is a global resource, it enables us to manage the data warehouse in a separate AWS account and avoid the complexity of navigating multiple VPCs.</li> \n</ul>\n<p>We also consume data stored in <a href=\"https://aws.amazon.com/dynamodb/\" target=\"_blank\" rel=\"noopener\">Amazon DynamoDB</a> tables. Kinesis Data Firehose again provided the core of the solution, this time combined with DynamoDB Streams and Lambda. For each DynamoDB table, we enabled DynamoDB Streams and then used the stream to trigger a Lambda function.</p> \n<p>The Lambda function cleans the DynamoDB stream output and writes the cleaned JSON to Kinesis Data Firehose using boto3. After doing this, it converges with the other process and outputs the data to S3. For more information, see <u><a href=\"https://aws.amazon.com/blogs/database/how-to-stream-data-from-amazon-dynamodb-to-amazon-aurora-using-aws-lambda-and-amazon-kinesis-firehose/\" target=\"_blank\" rel=\"noopener\">How to Stream Data from Amazon DynamoDB to Amazon Aurora using AWS Lambda and Amazon Kinesis Firehose</a></u> on the AWS Database Blog.</p> \n<p>Lambda gave us more fine-grained control and enabled us to move files between accounts:</p> \n<ul>\n<li>We enabled S3 event notifications on the S3 bucket and created an <a href=\"https://aws.amazon.com/sns/\" target=\"_blank\" rel=\"noopener\">Amazon SNS</a> topic to receive notifications whenever Kinesis Data Firehose put an object in the bucket.</li> \n <li>The SNS topic triggered a Lambda function, which took the Kinesis output and moved it to the data warehouse account in our chosen partition structure.</li> \n</ul>\n<p>S3 event notifications can trigger Lambda functions, but we chose SNS as an intermediary because the S3 bucket and Lambda function were in separate accounts.</p> \n<h2>Migrating existing data with AWS DMS and AWS Glue</h2> \n<p><img class=\"alignnone size-full wp-image-6372\" src=\"https://d2908q01vomqb2.cloudfront.net/b6692ea5df920cad691c20319a6fffd7a4a766b8/2019/01/22/DataLakeWoot_3.png\" alt=\"\" width=\"800\" height=\"530\"></p> \n<p>We needed to migrate data from our existing RDS database to S3, which we accomplished with AWS DMS. DMS natively supports S3 as a target, as described in <a href=\"https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Target.S3.html\" target=\"_blank\" rel=\"noopener\">the DMS documentation</a>.</p> \n<p>Setting this up was relatively straightforward. We exported data directly from our production VPC to the separate data warehouse account by tweaking the connection attributes in DMS. The string that we used was this:</p> \n<p><code>\"cannedAclForObjects=BUCKET_OWNER_FULL_CONTROL;compressionType=GZIP;addColumnName=true;”</code></p> \n<p>This code gives ownership to the bucket owner (the destination data warehouse account), compresses the files to save on storage costs, and includes all column names. After the data was in S3, we used an AWS Glue crawler to infer the schemas of all exported tables and then compared against the source data.</p> \n<p>With AWS Glue, some of the challenges we overcame were these:</p> \n<ul>\n<li>Unstructured text data, such as forum and blog posts. DMS exports these to CSV. This approach conflicted with the commas present in the text data. We opted to use AWS Glue to export data from RDS to S3 in Parquet format, which is unaffected by commas because it encodes columns directly.</li> \n <li>Cross-account exports. We resolved this by including the code</li> \n</ul>\n<p><code>\"glueContext._jsc.hadoopConfiguration().set(\"fs.s3.canned.acl\", \"BucketOwnerFullControl”)”</code></p> \n<p>at the top of each AWS Glue job to grant bucket owner access to all S3 files produced by AWS Glue.</p> \n<p>Overall, AWS DMS was quicker to set up and great for exporting large amounts of data with rule-based transformations. AWS Glue required more upfront effort to set up jobs, but provided better results for cases where we needed more control over the output.</p> \n<p>If you’re looking to convert existing raw data (CSV or JSON) into Parquet, you can set up an AWS Glue job to do that. The process is described in the AWS Big Data Blog post <a href=\"https://aws.amazon.com/blogs/big-data/build-a-data-lake-foundation-with-aws-glue-and-amazon-s3/\" target=\"_blank\" rel=\"noopener\">Build a data lake foundation with AWS Glue and Amazon S3</a>.</p> \n<h2>Bringing it all together with AWS Glue, Amazon Athena, and Amazon QuickSight</h2> \n<p><img class=\"alignnone size-full wp-image-6373\" src=\"https://d2908q01vomqb2.cloudfront.net/b6692ea5df920cad691c20319a6fffd7a4a766b8/2019/01/22/DataLakeWoot_4.png\" alt=\"\" width=\"666\" height=\"542\"></p> \n<p>After data landed in S3, it was time for the real fun to start: actually working with the data! Can you tell I’m a data engineer? For me, a big part of the fun was exploring AWS Glue:</p> \n<ul>\n<li>AWS Glue handles our ETL job scheduling.</li> \n <li>AWS Glue crawlers manage the metadata in the AWS Glue Data Catalog.</li> \n</ul>\n<p>Crawlers are the “secret sauce” that enables us to be responsive to schema changes. Throughout the pipeline, we chose to make each step as schema-agnostic as possible, which allows any schema changes to flow through until they reach AWS Glue.</p> \n<p>However, raw data is not ideal for most of our business users, because it often has duplicates or incorrect data types. Most importantly, the data out of Firehose is in JSON format, but we quickly observed significant query performance gains from using Parquet format. Here, we used one of the performance tips in the Big Data Blog post <a href=\"https://aws.amazon.com/blogs/big-data/top-10-performance-tuning-tips-for-amazon-athena/\" target=\"_blank\" rel=\"noopener\">Top 10 performance tuning tips for Amazon Athena</a>.</p> \n<p>With our shared responsibility model, the data warehouse and BI teams are responsible for the final processing of data into curated datasets ready for reporting. Using Lambda and AWS Glue enables these teams to work in Python and SQL (the core languages for Amazon data engineering and BI roles). It also enables them to deploy code with minimal infrastructure setup or maintenance.</p> \n<p>Our ETL process is as follows:</p> \n<ul>\n<li>Scheduled triggers.</li> \n <li>Series of conditional triggers that control the flow of subsequent jobs that depend on previous jobs.</li> \n <li>A similar pattern across many jobs of reading in the raw data, deduplicating the data, and then writing to Parquet. We centralized this logic by creating a Python library of functions and uploading it to S3. We then included that library in the AWS Glue job as an additional Python library. For more information on how to do this, see <a href=\"https://docs.aws.amazon.com/glue/latest/dg/aws-glue-programming-python-libraries.html\" target=\"_blank\" rel=\"noopener\">Using Python Libraries with AWS Glue</a> in the AWS Glue documentation.</li> \n</ul>\n<p>We also migrated complex jobs used to create reporting tables with business metrics:</p> \n<ul>\n<li>The AWS Glue use of PySpark simplified the migration of these queries, because you can embed SparkSQL queries directly in the job.</li> \n <li>Converting to SparkSQL took some trial and error, but ultimately required less work than translating SQL queries into Spark methods. However, for people on our BI team who had previously worked with Pandas or Spark, working with Spark dataframes was a natural transition. As someone who used SQL for several years before learning Python, I appreciate that PySpark lets me quickly switch back and forth between SQL and an object-oriented framework.</li> \n</ul>\n<p>Another hidden benefit of using AWS Glue jobs is that the AWS Glue version of Python (like Lambda) already has boto3 installed. Thus, ETL jobs can directly use AWS API operations without additional configuration.</p> \n<p>For example, some of our longer-running jobs created read inconsistency if a user happened to query that table while AWS Glue was writing data to S3. We modified the AWS Glue jobs to write to a temporary directory with Spark and then used boto3 to move the files into place. Doing this reduced read inconsistency by up to 90 percent. It was great to have this functionality readily available, which may not have been the case if we managed our own Spark cluster.</p> \n<h2>Comparing previous state and current state</h2> \n<p>After we had all the datasets in place, it was time for our customers to come on board and start querying. This is where we really leveled up the customer experience.</p> \n<p>Previously, users had to download a SQL client, request a user name and password, set it up, and learn SQL to get data out. Now, users just sign in to the AWS Management Console through automatically provisioned IAM roles and run queries in their browser with Athena. Or if they want to skip SQL altogether, they can use our Amazon QuickSight account with accounts managed through our pre-existing Active Directory server.</p> \n<p>Integration with Active Directory was a big win for us. We wanted to enable users to get up and running without having to wait for an account to be created or managing separate credentials. We already use Active Directory across the company for access to multiple resources. Upgrading to Amazon QuickSight Enterprise Edition enabled us to manage access with our existing AD groups and credentials.</p> \n<h2>Migration results</h2> \n<p>Our legacy data warehouse was developed over the course of five years. We recreated it as a serverless data lake using AWS Glue in about three months.</p> \n<p>In the end, it took more upfront effort than simply migrating to another relational database. We also dealt with more uncertainty because we used many products that were relatively new to us (especially AWS Glue).</p> \n<p>However, in the months since the migration was completed, we’ve gotten great feedback from data warehouse users about the new tools. Our users have been amazed by these things:</p> \n<ul>\n<li>How fast Athena is.</li> \n <li>How intuitive and beautiful Amazon QuickSight is. They love that no setup is required—it’s easy enough that even our CEO has started using it!</li> \n <li>That Athena plus the AWS Glue Data Catalog have given us the performance gains of a true big data platform, but for end users it retains the look and feel of a relational database.</li> \n</ul>\n<h2>Summary</h2> \n<p>From an operational perspective, the investment has already started to pay off. Literally: Our operating costs have fallen by almost 90 percent.</p> \n<p>Personally, I was thrilled that recently I was able to take a three-week vacation and didn’t get paged once, thanks to the serverless infrastructure. And for our BI engineers in addition to myself, the S3-centric architecture is enabling us to experiment with new technologies by integrating seamlessly with other services, such as Amazon EMR, Amazon SageMaker, Amazon Redshift Spectrum, and Lambda. It’s been exciting to see how these services have grown in the time since we’ve adopted them (for example, the recent AWS Glue launch of <a href=\"https://docs.aws.amazon.com/glue/latest/dg/monitoring-awsglue-with-cloudwatch-metrics.html\" target=\"_blank\" rel=\"noopener\">Amazon CloudWatch metrics</a> and <a href=\"https://docs.aws.amazon.com/athena/latest/ug/views.html\" target=\"_blank\" rel=\"noopener\">Athena’s launch of views</a>).</p> \n<p>We are thrilled that we’ve invested in technologies that continue to grow as we do. We are incredibly proud of our team for accomplishing this ambitious migration. We hope our experience can inspire other engineers to dive in to building a data lake of their own.</p> \n<p>For additional information, see these similar AWS Big Data blog posts:</p> \n<ul>\n<li><a href=\"https://aws.amazon.com/answers/big-data/data-lake-solution/\" target=\"_blank\" rel=\"noopener\">How do I deploy a data lake on AWS?</a></li> \n <li><a href=\"https://aws.amazon.com/blogs/big-data/build-a-data-lake-foundation-with-aws-glue-and-amazon-s3/\" target=\"_blank\" rel=\"noopener\">Build a data lake foundation with AWS Glue and Amazon S3</a></li> \n</ul>\n<hr>\n<h3>About the authors</h3> \n<p><img class=\"size-full wp-image-6377 alignleft\" src=\"https://d2908q01vomqb2.cloudfront.net/b6692ea5df920cad691c20319a6fffd7a4a766b8/2019/01/22/chayac1.png\" alt=\"\" width=\"113\" height=\"155\"><strong>Chaya Carey is a data engineer at <a href=\"https://www.woot.com/\" target=\"_blank\" rel=\"noopener\">Woot.com</a></strong>. At Woot, she’s responsible for managing the data warehouse and other scalable data solutions. Outside of work, she’s passionate about Seattle’s bar and restaurant scene, books, and video games.</p> \n<p> </p> \n<p> </p> \n<p> </p> \n<p><img class=\"size-full wp-image-4890 alignleft\" src=\"https://d2908q01vomqb2.cloudfront.net/b6692ea5df920cad691c20319a6fffd7a4a766b8/2018/04/26/KarthikO.png\" alt=\"\" width=\"113\" height=\"137\"><strong>Karthik Odapally is a senior solutions architect at AWS</strong>. His passion is to build cost-effective and highly scalable solutions on the cloud. In his spare time, he bakes cookies and cupcakes for family and friends here in the PNW. He loves vintage racing cars.</p> \n<p> </p> \n<p> </p> \n<p> </p> \n<p> </p>\n",
      "enclosure": {},
      "categories": [
        "Amazon Athena",
        "Amazon Kinesis",
        "Amazon QuickSight",
        "Amazon Simple Storage Services (S3)",
        "AWS Database Migration Service",
        "AWS Glue",
        "AWS Lambda",
        "Amazon Kinesis Data Firehose",
        "Amazon Quicksight",
        "Amazon S3",
        "AWS Data Migration Service"
      ]
    },
    {
      "title": "Analyze and visualize nested JSON data with Amazon Athena and Amazon QuickSight",
      "pubDate": "2019-01-18 17:08:48",
      "link": "https://aws.amazon.com/blogs/big-data/analyze-and-visualize-nested-json-data-with-amazon-athena-and-amazon-quicksight/",
      "guid": "a404994d80cb11379d24b20df6da6950194af3f9",
      "author": "Mariano Kamp",
      "thumbnail": "https://d2908q01vomqb2.cloudfront.net/b6692ea5df920cad691c20319a6fffd7a4a766b8/2019/01/16/AthenaJSON1.png",
      "description": "Although structured data remains the backbone for many data platforms, increasingly unstructured or semistructured data is used to enrich existing information or to create new insights. Amazon Athena enables you to analyze a wide variety of data. This includes tabular data in comma-separated value (CSV) or Apache Parquet files, data extracted from log files using regular expressions, […]",
      "content": "\n<p>Although structured data remains the backbone for many data platforms, increasingly unstructured or semistructured data is used to enrich existing information or to create new insights. <u><a href=\"https://aws.amazon.com/athena/\" target=\"_blank\" rel=\"noopener\">Amazon Athena</a></u> enables you to analyze a wide variety of data. This includes tabular data in comma-separated value (CSV) or Apache Parquet files, data extracted from log files using regular expressions, and JSON-formatted data. Athena is serverless, so there is no infrastructure to manage, and you pay only for the queries that you run.</p> \n<p>In this blog post, I show you how to use JSON-formatted data and translate a nested data structure into a tabular view. For data engineers, using this type of data is becoming increasingly important. For example, you can use API-powered data feeds from operational systems to create data products. Such data can also help to add more finely grained facets to your understanding of customers and interactions. Understanding the fuller picture helps you better understand your customers and tailor experiences or predict outcomes.</p> \n<p>To illustrate, I use an end-to-end example. It processes financial data retrieved from an API operation that is formatted as JSON. We analyze the data in Amazon Athena and visualize the results in <u><a href=\"https://aws.amazon.com/quicksight/\" target=\"_blank\" rel=\"noopener\">Amazon QuickSight</a></u>. Along the way, we compare and contrast alternative options.</p> \n<p>The result looks similar to what you see below.<br><img class=\"alignnone wp-image-6317 size-full\" src=\"https://d2908q01vomqb2.cloudfront.net/b6692ea5df920cad691c20319a6fffd7a4a766b8/2019/01/16/AthenaJSON1.png\" alt=\"\" width=\"800\" height=\"448\"></p> \n<h2>Analyzing JSON-formatted data</h2> \n<p>For our end-to-end example, we use financial data as provided by IEX. The <u><a href=\"https://iextrading.com/developer/docs/#financials\" target=\"_blank\" rel=\"noopener\">financials API call</a></u> pulls income statement, balance sheet, and cash flow data from four reported years of a stock.</p> \n<p>Following, you can see example output. On the top level is an attribute called <code>symbol</code>, which identifies the stock described here: Apple. On the same level is an attribute called <code>financials</code>. This is a data container. The actual information is one level below, including such attributes as <code>reportDate</code>, <code>cashflow</code>, and <code>researchAndDevelopment</code>.</p> \n<p>The data container is an array. In the example following, financial data for only one year is shown. However, the <code>{...}</code> indicates that there might be more. In our case, data for four years is returned when making the actual API call.</p> \n<div class=\"hide-language\"> \n <pre><code class=\"lang-json\">{\n  \"symbol\": \"AAPL\",\n  \"financials\": [\n    {\n      \"reportDate\": \"2017-03-31\",\n      \"grossProfit\": 20591000000,\n      \"costOfRevenue\": 32305000000,\n      \"operatingRevenue\": 52896000000,\n      \"totalRevenue\": 52896000000,\n      \"operatingIncome\": 14097000000,\n      \"netIncome\": 11029000000,\n      \"researchAndDevelopment\": 2776000000,\n      \"operatingExpense\": 6494000000,\n      \"currentAssets\": 101990000000,\n      \"totalAssets\": 334532000000,\n      \"totalLiabilities\": 200450000000,\n      \"currentCash\": 15157000000,\n      \"currentDebt\": 13991000000,\n      \"totalCash\": 67101000000,\n      \"totalDebt\": 98522000000,\n      \"shareholderEquity\": 134082000000,\n      \"cashChange\": -1214000000,\n      \"cashFlow\": 12523000000,\n      \"operatingGainsLosses\": null\n    } // , { ... }\n  ]\n}\n</code></pre> \n</div> \n<p>Data is provided for free by <u><a href=\"https://iextrading.com/developer\" target=\"_blank\" rel=\"noopener\">IEX</a></u> (see the <u><a href=\"https://iextrading.com/api-exhibit-a/\" target=\"_blank\" rel=\"noopener\">IEX Terms of Use</a></u>).</p> \n<p>It has become commonplace to use external data from API operations as feeds into <u><a href=\"https://aws.amazon.com/s3/\" target=\"_blank\" rel=\"noopener\">Amazon S3</a></u>. Although this is usually done in an automated fashion, in our case we manually acquire the API call’s results.</p> \n<p>To download the data, you can use a script, described following.</p> \n<p>Alternatively, you can click the following three links: <u><a href=\"https://api.iextrading.com/1.0/stock/aapl/financials?period=annual\" target=\"_blank\" rel=\"noopener\">1</a></u>, <u><a href=\"https://api.iextrading.com/1.0/stock/nvda/financials?period=annual\" target=\"_blank\" rel=\"noopener\">2</a></u>, <u><a href=\"https://api.iextrading.com/1.0/stock/fb/financials?period=annual\" target=\"_blank\" rel=\"noopener\">3</a></u>. You can then save the resulting JSON files to your local disk, then upload the JSON to an Amazon S3 bucket. In my case, the location of the data is <code>s3://athena-json/financials</code>, but you should create your own bucket. The result looks similar to this:</p> \n<p><img class=\"alignnone wp-image-6318 size-full\" src=\"https://d2908q01vomqb2.cloudfront.net/b6692ea5df920cad691c20319a6fffd7a4a766b8/2019/01/16/AthenaJSON2.png\" alt=\"\" width=\"710\" height=\"474\"></p> \n<p>You can also use a Unix-like shell on your local computer or on an <u><a href=\"https://aws.amazon.com/ec2/\" target=\"_blank\" rel=\"noopener\">Amazon EC2</a></u> instance to populate a S3 location with the API data:</p> \n<div class=\"hide-language\"> \n <pre><code class=\"lang-bash\">$ curl -s \"https://api.iextrading.com/1.0/stock/aapl/financials?period=annual\" &gt; aapl.json \n$ curl -s \"https://api.iextrading.com/1.0/stock/nvda/financials?period=annual\" &gt; nvda.json \n$ curl -s \"https://api.iextrading.com/1.0/stock/fb/financials?period=annual\" &gt; fb.json \n\n$ ls -ltrh *.json\n-rw-r--r--  1 mkamp  ANT\\Domain Users   2.2K Nov 21 16:57 aapl.json\n-rw-r--r--  1 mkamp  ANT\\Domain Users   2.1K Nov 21 16:57 nvda.json\n-rw-r--r--  1 mkamp  ANT\\Domain Users   2.1K Nov 21 16:57 fb.json \n\n$ aws s3 sync . s3://athena-json/financials/ --exclude \"*\" --include \"*.json\"\nupload: ./aapl.json to s3://athena-json/financials/aapl.json   \nupload: ./nvda.json to s3://athena-json/financials/nvda.json   \nupload: ./fb.json to s3://athena-json/financials/fb.json       \n\n$ aws s3 ls s3://athena-json/financials/\n2018-11-21 16:58:30       2245 aapl.json\n2018-11-21 16:58:30       2162 fb.json\n2018-11-21 16:58:30       2150 nvda.json</code></pre> \n</div> \n<h3>Mapping JSON structures to table structures</h3> \n<p>Now we have the data in S3. Let’s make it accessible to Athena. This is a simple two-step process:</p> \n<ol>\n<li>Create metadata. Doing so is analogous to traditional databases, where we use DDL to describe a table structure. This step maps the structure of the JSON formatted data to columns.</li> \n <li>Specify where to find the JSON files.</li> \n</ol>\n<p>We can use all information of the JSON file at this time, or we can concentrate on mapping the information that we need today. The new data structure in Athena overlays the files in S3 only virtually. Therefore, even though we just map a subset of the contained information at this time, all information is retained in the files and can be used later on as needed. This is a powerful concept and enables an iterative approach to data modeling.</p> \n<p>You can use the following SQL statement to create the table. The table is then named <code>financials_raw</code>—see (1) following. We use that name to access the data from this point on. We map the symbol and the list of financials as an array and some figures. We define that the underlying files are to be interpreted as JSON in (2), and that the data lives following <code>s3://athena-json/financials/</code> in (3).</p> \n<div class=\"hide-language\"> \n <pre><code class=\"lang-sql\">CREATE EXTERNAL TABLE financials_raw ( -- (1)\n    symbol string,\n    financials array&lt;\n        struct&lt;reportdate: string,\n             grossprofit: bigint,\n             totalrevenue: bigint,\n             totalcash: bigint,\n             totaldebt: bigint,\n             researchanddevelopment: bigint&gt;&gt;\n)\nROW FORMAT SERDE 'org.openx.data.jsonserde.JsonSerDe' -- (2)\nLOCATION 's3://athena-json/financials/' -- (3)</code></pre> \n</div> \n<p>You can run this statement using the Athena console as depicted following:</p> \n<p><img class=\"alignnone size-full wp-image-6319\" src=\"https://d2908q01vomqb2.cloudfront.net/b6692ea5df920cad691c20319a6fffd7a4a766b8/2019/01/16/AthenaJSON3.png\" alt=\"\" width=\"800\" height=\"458\"></p> \n<p>After you run the SQL statement on the left, the just-created table <code>financials_raw</code> is listed under the heading <strong>Tables</strong>. Now let’s have a look what’s in this table. Choose the three vertical dots to the right of the table name and choose <strong>Preview table</strong>. Athena creates a <code>SELECT</code> statement to show 10 rows of the table:</p> \n<p><img class=\"alignnone size-full wp-image-6320\" src=\"https://d2908q01vomqb2.cloudfront.net/b6692ea5df920cad691c20319a6fffd7a4a766b8/2019/01/16/AthenaJSON4.png\" alt=\"\" width=\"800\" height=\"454\"></p> \n<p>Looking at the output, you can see that Athena was able to understand the underlying data in the JSON files. Specifically, we can see two columns:</p> \n<ul>\n<li>\n<code>symbol</code>, which contains flat data, the symbol of the stock</li> \n <li>\n<code>financials</code>, which now contains an array of financials reports</li> \n</ul>\n<p>If you look closely and observe the <code>reportdate</code> attribute, you find that the row contains more than one financial report.</p> \n<p>Even though the data is nested—in our case <code>financials</code> is an array—you can access the elements directly from your column projections:</p> \n<div class=\"hide-language\"> \n <pre><code class=\"lang-sql\">SELECT\n  symbol, \n  financials[1].reportdate one_report_date, -- indexes start with 1\n  financials[1].totalrevenue one_total_revenue,\n  financials[2].reportdate another_report_date,\n  financials[2].totalrevenue another_total_revenue\nFROM\n  financials_raw\nORDER BY\n  1 -- the 1 indicates to order by the first column</code></pre> \n</div> \n<p><img class=\"alignnone size-full wp-image-6321\" src=\"https://d2908q01vomqb2.cloudfront.net/b6692ea5df920cad691c20319a6fffd7a4a766b8/2019/01/16/AthenaJSON5.png\" alt=\"\" width=\"800\" height=\"172\"></p> \n<p>As you can see preceding, all data is accessible. From this point on, it is structured, nested data, but not JSON anymore.</p> \n<p>It’s still not tabular, though. We come back to this in a minute. First let’s have a look at a different way that would also have brought us to this point.</p> \n<h3>Alternative approach: Deferring the JSON extraction to query time</h3> \n<p>There are many different ways to use JSON formatted data in Athena. In the previous section, we use a simple, explicit, and rigid approach. In contrast, we now see a rather generic, dynamic approach.</p> \n<p>In this case, we defer the final decisions about the data structures from table design to query design. To do that, we leave the data untouched in its JSON form as long as possible. As a consequence, the <code>CREATE TABLE</code> statement is much simpler than in the previous section:</p> \n<div class=\"hide-language\"> \n <pre><code class=\"lang-sql\">CREATE EXTERNAL TABLE financials_raw_json (\n  -- Using a mixed approach, extracting the symbol for \n  -- convenience directly from the JSON data\n  symbol string,\n  -- But otherwise storing the RAW JSON Data as string\n  financials string \n)\nROW FORMAT SERDE 'org.openx.data.jsonserde.JsonSerDe'\nLOCATION 's3://athena-json/financials/' \nExecuting\nSELECT * FROM financials_raw_json\n</code></pre> \n</div> \n<p>This shows that the data is accessible:</p> \n<p><img class=\"alignnone size-full wp-image-6322\" src=\"https://d2908q01vomqb2.cloudfront.net/b6692ea5df920cad691c20319a6fffd7a4a766b8/2019/01/16/AthenaJSON6.png\" alt=\"\" width=\"800\" height=\"165\"></p> \n<p>Even though the data is now accessible, it is only treated as a single <code>string</code> or <code>varchar</code>. This type is generic and doesn’t reflect the rich structure and the attributes of the underlying data.</p> \n<p>But before diving into the richness of the data, I want to acknowledge that it’s hard to see from the query results which data type a column is. When using your queries, the focus is on the actual data, so seeing the data types all the time can be distracting. However in this case, when creating your queries and data structures, it is useful to use <code>typeof</code>. For example, use the following SQL statement:</p> \n<div class=\"hide-language\"> \n <pre><code class=\"lang-sql\">SELECT typeof(financials) FROM financials_raw_json</code></pre> \n</div> \n<p>Using this SQL statement, you can verify for yourself that the column is treated as a <code>varchar</code>.</p> \n<p>To now introduce the data structure during query design, Athena provides specific functionality <u><a href=\"https://docs.aws.amazon.com/athena/latest/ug/querying-JSON.html\" target=\"_blank\" rel=\"noopener\">covered in the documentation</a></u> to work with JSON formatted data.</p> \n<p>The following table shows how to extract the data, starting at the root of the record in the first example. The table then shows additional examples on how to navigate further down the document tree. The first column shows the expression that can be used in a SQL statement like <code>SELECT &lt;expr&gt; FROM financials_raw_json</code>, where <code>&lt;expr&gt;</code> is to be replaced by the expression in the first column. The remaining columns explain the results.</p> \n<table border=\"1\" cellpadding=\"10\">\n<thead><tr>\n<td width=\"280\"><strong>Expression </strong></td> \n   <td width=\"240\"><strong>Result</strong></td> \n   <td width=\"88\"><strong>Type</strong></td> \n   <td width=\"220\"><strong>Description </strong></td> \n  </tr></thead>\n<tbody>\n<tr>\n<td width=\"280\"><code>json_extract(financials, '$')</code></td> \n   <td width=\"240\"><code>[{.., \"reportdate\":\"2017-12-31\",..},{..}, {..}, {.., \"reportdate\":\"2014-12-31\", ..}]</code></td> \n   <td width=\"88\"><code>json</code></td> \n   <td width=\"220\">Selecting the root of the document (<code>financials</code>).</td> \n  </tr>\n<tr>\n<td width=\"280\"><code>json_extract(financials, '$[0]')</code></td> \n   <td width=\"240\"><code>{.., \"reportdate\":\"2017-12-31\", \"totalcash\":\"41711000000\", ..}</code></td> \n   <td width=\"88\"><code>json</code></td> \n   <td width=\"220\">Selecting the first element of the <code>financials</code> array. The indexing starts at 0, as opposed to 1, which is customary in SQL.</td> \n  </tr>\n<tr>\n<td width=\"280\"><code>json_extract(financials, '$[0].reportdate')</code></td> \n   <td width=\"240\"><code>\"2017-12-31\"</code></td> \n   <td width=\"88\"><code>json</code></td> \n   <td width=\"220\">Selecting the <code>totalcash</code> attribute of the first element of the <code>financials</code> array.</td> \n  </tr>\n<tr>\n<td width=\"280\"><code>json_extract_scalar(financials, '$[0].reportdate')</code></td> \n   <td width=\"240\"><code>2017-12-31</code></td> \n   <td width=\"88\"><code>varchar</code></td> \n   <td width=\"220\">As preceding, but now the type became a <code>varchar</code> because we are now using <code>json_extract_scalar</code>.</td> \n  </tr>\n<tr>\n<td width=\"280\"><code>json_size(financials, '$')</code></td> \n   <td width=\"240\"><code>4</code></td> \n   <td width=\"88\"><code>bigint</code></td> \n   <td width=\"220\">The size of the <code>financials</code> array; 4 represents the four years contained in each JSON.</td> \n  </tr>\n</tbody>\n</table>\n<p>To implement our example, we now have more than enough skills and we can leave it at that.<br> However, there are more functions to go back and forth between JSON and Athena. You can find more information in the <a href=\"https://prestodb.io/docs/0.172/functions/json.html\" target=\"_blank\" rel=\"noopener\">Apache Presto documentation</a>. Athena is our managed service based on Apache Presto. Thus, when looking for information it is also helpful to consult Presto documentation.</p> \n<p>Let’s put the JSON functions introduced preceding to use:</p> \n<div class=\"hide-language\"> \n <pre><code class=\"lang-sql\">SELECT \n  symbol,\n  -- indexes start with 0, as is customary with JavaScript/JSON\n  json_extract_scalar(financials, '$[0].reportdate') one_report_date,  \n  json_extract_scalar(financials, '$[0].totalrevenue') one_total_revenue,\n  json_extract_scalar(financials, '$[1].reportdate') another_report_date,\n  json_extract_scalar(financials, '$[1].totalrevenue') another_total_revenue\nFROM\n  financials_raw_json\nORDER BY \n  1</code></pre> \n</div> \n<p><img class=\"alignnone size-full wp-image-6323\" src=\"https://d2908q01vomqb2.cloudfront.net/b6692ea5df920cad691c20319a6fffd7a4a766b8/2019/01/16/AthenaJSON7.png\" alt=\"\" width=\"800\" height=\"162\"></p> \n<p>As with the first approach, we still have to deal with the nested data inside the rows. By doing so, we can get rid of the explicit indexing of the financial reports as used preceding.</p> \n<h3>Comparing approaches</h3> \n<p>If you go back and compare our latest SQL query with our earlier SQL query, you can see that they produce the same output. On the surface, they even look alike because they project the same attributes. But a closer look reveals that the first statement uses a structure that has already been created during <code>CREATE TABLE</code>. In contrast, the second approach interprets the JSON document for each column projection as part of the query.</p> \n<p>Interpreting the data structures during the query design enables you to change the structures across different SQL queries or even within the same SQL query. Different column projections in the same query can interpret the same data, even the same column, differently. This can be extremely powerful, if such a dynamic and differentiated interpretation of the data is valuable. On the other hand, it takes more discipline to make sure that during maintenance different interpretations are not introduced by accident.</p> \n<p>In both approaches, the underlying data is not touched. Athena only overlays the physical data, which makes changing the structure of your interpretation fast. Which approach better suits you depends on the intended use.</p> \n<p>To determine this, you can ask the following questions. As a rule of thumb, are your intended users data engineers or data scientists? Do they want to experiment and change their mind frequently? Maybe they even want to have different use case–specific interpretations of the same data, Then they would fare better with the latter approach of leaving the JSON data untouched until query design. They would also then likely be willing to invest in learning the JSON extensions to gain access to this dynamic approach.</p> \n<p>If on the other hand your users have established data sources with stable structures, the former approach fits better. It enables your users to query the data with SQL only, with no need for information about the underlying JSON data structures.</p> \n<p>Use the following side-by-side comparison to choose the appropriate approach for your case at hand.</p> \n<table border=\"1\" cellpadding=\"10\"><tbody>\n<tr>\n<td width=\"283\"><strong>Data structure interpretation happens at <u>table creation time</u></strong></td> \n   <td width=\"283\"><strong>Data structure interpretation happens at <u>query creation time</u></strong></td> \n  </tr>\n<tr>\n<td width=\"283\">The interpretation of data structures is scoped to the whole table. All subsequent queries use the same structures.</td> \n   <td width=\"283\">The data interpretation is scoped to an individual query. Each query can potentially interpret the data differently</td> \n  </tr>\n<tr>\n<td width=\"283\">The interpretation of data structures evolves centrally.</td> \n   <td width=\"283\">The interpretation of data structures can be changed on a per-query basis so that different queries can evolve with different speeds and into different directions.</td> \n  </tr>\n<tr>\n<td width=\"283\">It is easy to provide a single version of the truth, because there is just a single interpretation of the underlying data structures.</td> \n   <td width=\"283\">A single version of the truth is hard to maintain and needs coordination across the different queries using the same data. Rapidly evolving data interpretations can easily go hand-in-hand with an evolving understanding of use cases.</td> \n  </tr>\n<tr>\n<td width=\"283\">Applicable to well-understood data structures that are slowly and consciously evolving. A single interpretation of the underlying data structures is valued more than change velocity.</td> \n   <td width=\"283\">Applicable to experimental, rapidly evolving interpretations of data structures and use cases. Change velocity is more important than a single, stable interpretation of data structures.</td> \n  </tr>\n<tr>\n<td width=\"283\">Production data pipelines benefit from this approach.</td> \n   <td width=\"283\">Exploratory data analysis benefit from this approach.</td> \n  </tr>\n</tbody></table>\n<p>Both approaches can serve well at different times in the development lifecycle, and each approach can be migrated to the other.</p> \n<p>In any case, this is not a black and white decision. In our example, we keep the tables <code>financials_raw</code> and <code>financials_raw_json</code>, both accessing the same underlying data. The data structures are just metadata, so keeping both around doesn’t store the actual data redundantly.</p> \n<p>For example, <code>financials_raw</code> might be used by data engineers as the source of productive pipelines where the attributes and their meaning are well-understood and stable across use cases. At the same time, data scientists might use <code>financials_raw_json</code> for exploratory data analysis where they refine their interpretation of the data rapidly and on a per-query basis.</p> \n<h3>Working with nested data</h3> \n<p>At this point, we can access data that is JSON formatted through Athena. However, the underlying structure is still hierarchical, and the data is still nested. For many use cases, especially for analytical uses, expressing data in a tabular fashion—as rows—is more natural. This is also the standard way when using SQL and business intelligence tools. To unnest the hierarchical data into flattened rows, we need to reconcile these two approaches.</p> \n<p>To simplify, we can set the financial reports example aside for the moment. Instead, let’s experiment with a narrower example. Reconciling different ways of thinking can sometimes be hard to follow. The narrow example and hands-on experimentation should make this easier. Copy the code we discuss into the Athena console to play along.</p> \n<p>The following code is self-contained and uses synthetic data. This lends itself particular well to experimentation:</p> \n<div class=\"hide-language\"> \n <pre><code class=\"lang-sql\">SELECT \n  parent, children\nFROM (\n  VALUES\n    ('Parent 1', ARRAY['Child 1.1', 'Child 1.2']),\n    ('Parent 2', ARRAY['Child 2.1', 'Child 2.2', 'Child 2.3'])\n) AS t(parent, children)</code></pre> \n</div> \n<p><img class=\"alignnone size-full wp-image-6324\" src=\"https://d2908q01vomqb2.cloudfront.net/b6692ea5df920cad691c20319a6fffd7a4a766b8/2019/01/16/AthenaJSON7_1.png\" alt=\"\" width=\"586\" height=\"168\"></p> \n<p>Looking at the data, this is similar to our situation with the financial reports. There we had multiple financial reports for one stock symbol, multiple children for each parent. To flatten the data, we first unnest the individual children for each parent. Then we cross-join each child with its parent, which creates an individual row for each child that contains the child and its parent.</p> \n<p><img class=\"alignnone wp-image-6325 size-full\" src=\"https://d2908q01vomqb2.cloudfront.net/b6692ea5df920cad691c20319a6fffd7a4a766b8/2019/01/16/AthenaJSON8.png\" alt=\"\" width=\"800\" height=\"530\"></p> \n<p>In the following SQL statement, <code>UNNEST</code> takes the <code>children</code> column from the original table as a parameter. It creates a new dataset with the new column <code>child</code>, which is later cross-joined. The enclosing <code>SELECT</code> statement can then reference the new child column directly.</p> \n<div class=\"hide-language\"> \n <pre><code class=\"lang-sql\">SELECT \n  parent, child\nFROM (\n  VALUES\n    ('Parent 1', ARRAY['Child 1.1', 'Child 1.2']),\n    ('Parent 2', ARRAY['Child 2.1', 'Child 2.2', 'Child 2.3'])\n) AS t(parent, children)\nCROSS JOIN UNNEST(children) AS t(child)\n</code></pre> \n</div> \n<p><img class=\"alignnone size-full wp-image-6326\" src=\"https://d2908q01vomqb2.cloudfront.net/b6692ea5df920cad691c20319a6fffd7a4a766b8/2019/01/16/AthenaJSON9.png\" alt=\"\" width=\"630\" height=\"348\"></p> \n<p>If you played along with the simplified example, it should be easy now to see how this method can be applied to our financial reports:</p> \n<div class=\"hide-language\"> \n <pre><code class=\"lang-sql\">SELECT \n    symbol,\n    report\nFROM \n    financials_raw\nCROSS JOIN UNNEST(financials) AS t(report)</code></pre> \n</div> \n<p><img class=\"alignnone size-full wp-image-6327\" src=\"https://d2908q01vomqb2.cloudfront.net/b6692ea5df920cad691c20319a6fffd7a4a766b8/2019/01/16/AthenaJSON10.png\" alt=\"\" width=\"800\" height=\"362\"></p> \n<p>Bam! Now that was easy, wasn’t it?</p> \n<p>Using this as a basis, let’s select the data that we want to provide to our business users and turn the query into a view. The underlying data has still not been touched, is still formatted as JSON, and is still expressed using nested hierarchies. The new view makes all of this transparent and provides a tabular view.</p> \n<p>Let’s create the view:</p> \n<div class=\"hide-language\"> \n <pre><code class=\"lang-sql\">CREATE OR REPLACE VIEW financial_reports_view AS\nSELECT \n  symbol,\n  CAST(report.reportdate AS DATE) reportdate,\n  report.totalrevenue,\n  report.researchanddevelopment\nFROM \n  financials_raw\nCROSS JOIN UNNEST(financials) AS t(report)\nORDER BY 1 ASC, 2 DESC</code></pre> \n</div> \n<p>…and then check our work:</p> \n<div class=\"hide-language\"> \n <pre><code class=\"lang-sql\">SELECT\n  *\nFROM\n  financial_reports_view</code></pre> \n</div> \n<p><img class=\"alignnone size-full wp-image-6328\" src=\"https://d2908q01vomqb2.cloudfront.net/b6692ea5df920cad691c20319a6fffd7a4a766b8/2019/01/16/AthenaJSON11.png\" alt=\"\" width=\"800\" height=\"452\"></p> \n<p>This is a good basis and acts as an interface for our business users.</p> \n<p>The previous steps were based on the initial approach of mapping the JSON structures directly to columns. Let’s also explore the alternative path that we discussed before. How does this look like when we keep the data JSON formatted for longer, as we did in our alternative approach?</p> \n<p>For variety, this approach also shows <code>json_parse</code>, which is used here to parse the whole JSON document and converts the list of financial reports and their contained key-value pairs into an <code>ARRAY(MAP(VARCHAR, VARCHAR))</code>. This array in turn is then used in the unnesting and its children eventually in the column projections. With <code>element_at</code> elements in the JSON, you can access the value by name. You can also see the use of <code>WITH</code> to define subqueries, helping to structure the SQL statement.</p> \n<p>If you run the following query, it returns the same result as the approach preceding. You can also turn this query into a view.</p> \n<div class=\"hide-language\"> \n <pre><code class=\"lang-sql\">WITH financial_reports_parsed AS (\n  SELECT \n    symbol,   \n    CAST(json_parse(financials) AS ARRAY(MAP(VARCHAR, VARCHAR))) financial_reports\n  FROM         \n    financials_raw_json)\nSELECT \n  symbol,\n  CAST(element_at(report, 'reportdate') AS DATE) reportdate,  \n  element_at(report, 'totalrevenue') totalrevenue,\n  element_at(report, 'researchanddevelopment') researchanddevelopment\nFROM\n  financial_reports_parsed\nCROSS JOIN UNNEST(financial_reports) AS t(report)\nORDER BY 1 ASC, 2 DESC\n</code></pre> \n</div> \n<h2>Visualizing the data</h2> \n<p>Let’s get back to our example. We created the <code>financial_reports_view</code> that acts as our interface to other business intelligence tools. In this blog post, we use it to provide data for visualization using <u><a href=\"https://aws.amazon.com/quicksight/\" target=\"_blank\" rel=\"noopener\">Amazon QuickSight</a></u>. Amazon QuickSight can directly access data through Athena. Its pay-per-session pricing enables you to put analytical insights into the hands of everyone in your organization.</p> \n<p>Let’s set this up together. We first need to select our view to create a new data source in Athena and then we use this data source to populate the visualization.</p> \n<p>We are creating the visual that is displayed at the top of this post. If you want just the data and you’re not interested in condensing data to a visual story, you can skip ahead to the post conclusion section.</p> \n<h3>Creating an Athena data source in Amazon QuickSight</h3> \n<p>Before we can use the data in Amazon QuickSight, we need to first grant access to the underlying S3 bucket. If you haven’t done so already for other analyses, see our <a href=\"https://docs.aws.amazon.com/quicksight/latest/user/managing-permissions.html\" target=\"_blank\" rel=\"noopener\">documentation</a> on how to do so.</p> \n<p>On the Amazon QuickSight home page, choose <strong>Manage data</strong> from the upper-right corner, then choose <strong>New data set</strong> and pick Athena as data source. In the following dialog box, give the data source a descriptive name and choose <strong>Create data source</strong>.</p> \n<p><img class=\"alignnone wp-image-6329 size-full\" src=\"https://d2908q01vomqb2.cloudfront.net/b6692ea5df920cad691c20319a6fffd7a4a766b8/2019/01/16/AthenaJSON12.png\" alt=\"\" width=\"800\" height=\"257\"></p> \n<p>Choose the default database and our view <code>financial_reports_view</code>, then choose <strong>Select</strong> to confirm. If you used multiple schemas in Athena, you could pick them here as your database.</p> \n<p><img class=\"alignnone wp-image-6330 size-full\" src=\"https://d2908q01vomqb2.cloudfront.net/b6692ea5df920cad691c20319a6fffd7a4a766b8/2019/01/16/AthenaJSON13.png\" alt=\"\" width=\"800\" height=\"655\"></p> \n<p>In the next dialog box, you can choose if you want to import the data into <u><a href=\"https://docs.aws.amazon.com/quicksight/latest/user/welcome.html#spice\" target=\"_blank\" rel=\"noopener\">SPICE</a></u> for quicker analytics or to directly query the data.</p> \n<p>SPICE is the super-fast, parallel, in-memory calculation engine in Amazon QuickSight. For our example, you can go either way. Using SPICE results in the data being loaded from Athena only once, until it is either manually refreshed or automatically refreshed (using a schedule). Using direct query means that all queries are run on Athena.</p> \n<p><img class=\"alignnone size-full wp-image-6331\" src=\"https://d2908q01vomqb2.cloudfront.net/b6692ea5df920cad691c20319a6fffd7a4a766b8/2019/01/16/AthenaJSON14.png\" alt=\"\" width=\"800\" height=\"367\"></p> \n<p>Our view now is a data source for Amazon QuickSight and we can turn to visualizing the data.</p> \n<h3>Creating a visual in Amazon QuickSight</h3> \n<p><img class=\"alignnone size-full wp-image-6332\" src=\"https://d2908q01vomqb2.cloudfront.net/b6692ea5df920cad691c20319a6fffd7a4a766b8/2019/01/16/AthenaJSON15.png\" alt=\"\" width=\"800\" height=\"394\"></p> \n<p>You can see the data fields on the left. Notice that <code>reportdate</code> is shown with a calendar symbol and <code>researchanddevelopment</code> as a number. Amazon QuickSight picks up the data types that we defined in Athena.</p> \n<p>The canvas on the right is still empty. Before we populate it with data, let’s select <strong>Line Chart</strong> from the available visual types.</p> \n<p><img class=\"alignnone wp-image-6333 size-full\" src=\"https://d2908q01vomqb2.cloudfront.net/b6692ea5df920cad691c20319a6fffd7a4a766b8/2019/01/16/AthenaJSON16.png\" alt=\"\" width=\"800\" height=\"294\"></p> \n<p>To populate the graph, drag and drop the fields from the field list on the left onto their respective destinations. In our case, we put the <code>reportdate</code> onto the X axis well. We put our metric <code>researchanddevelopment</code> towards the value well, so that it’s displayed on the y-axis. We put the symbol onto the <strong>Color</strong> well, helping us to tell the different stocks apart.</p> \n<p><img class=\"alignnone wp-image-6346 size-full\" src=\"https://d2908q01vomqb2.cloudfront.net/b6692ea5df920cad691c20319a6fffd7a4a766b8/2019/01/17/AthenaJSON18_1.png\" alt=\"\" width=\"411\" height=\"263\"></p> \n<p>An initial version of our visualization is now shown on the canvas. Drag the handle at the lower-right corner to adjust the size to your liking. Also, pick <strong>Format visual</strong> from the drop-down menu in the upper right corner. Doing this opens a dialog with more options to enhance the visualization.</p> \n<p>Expand the <strong>Data labels</strong> section and choose <strong>Show data labels</strong>. Your changes are immediately reflected in the visualization.</p> \n<p><img class=\"alignnone size-full wp-image-6335\" src=\"https://d2908q01vomqb2.cloudfront.net/b6692ea5df920cad691c20319a6fffd7a4a766b8/2019/01/16/AthenaJSON19.png\" alt=\"\" width=\"800\" height=\"444\"></p> \n<p>You can also interact with the data directly. Given that Amazon QuickSight picked up on the <code>reportdate</code> being a <code>DATE</code>, it provides a date slider at the bottom of the visual. You can use this slider to adjust the time frame shown.</p> \n<p>You can add further customizations. These can include changing the title of the visual or the axis, adjusting the size of the visual, and adding additional visualizations. Other possible customizations are adding data filters and capturing the combination of visuals into a dashboard. You might even turn the dashboard into a scheduled report that gets sent out once a day by email.</p> \n<h2>Conclusion</h2> \n<p>We have seen how to use JSON formatted data that is stored in S3. We contrasted two approaches to map the JSON formatted data to data structures in Athena:</p> \n<ul>\n<li>Mapping the JSON structures at table creation time to columns.</li> \n <li>Leaving the JSON structures untouched and instead mapping the contents as a whole to a string, so that the JSON contents remains intact. The JSON contents can later be interpreted and the structures at query creation time mapped to columns.</li> \n</ul>\n<p>The approaches are not mutually exclusive, but can be used in parallel for the same underlying data.</p> \n<p>Furthermore, JSON data can be hierarchical, which must be unnested and cross-joined to provide the data in a flattened, tabular fashion.</p> \n<p>For our example, we provided the data in a tabular fashion and created a view that encapsulates the transformations, hiding the complexity from its users. We used the view as an interface to Amazon QuickSight. Amazon QuickSight directly accesses the Athena view and visualizes the data.</p> \n<h3>More on using JSON</h3> \n<p>JSON features blend nicely into the existing SQL oriented functions in Athena, but are not ANSI SQL compatible. Also, the JSON file is expected to carry each record in a separate line (see the <u><a href=\"http://jsonlines.org/\" target=\"_blank\" rel=\"noopener\">JSON lines</a></u> website).</p> \n<p>In the <u><a href=\"https://docs.aws.amazon.com/athena/latest/ug/json.html\" target=\"_blank\" rel=\"noopener\">documentation for the JSON SerDe Libraries</a></u>, you can find how to use the property <code>ignore.malformed.json</code> to indicate if malformed JSON records should be turned into nulls or an error. Further information about the two possible JSON SerDe implementations is linked in the documentation. If necessary, you can dig deeper and find out how to take explicit control of how column names are parsed, for example to avoid clashing with reserved keywords.</p> \n<h3>How to efficiently store data</h3> \n<p>During our excursions, we never touched the actual data. We only defined different ways to interpret the data. This approach works well for us here, because we are only dealing with a small amount of data. If you want to use these concepts at scale, consider how to apply partitioning of data and possibly how to consolidate data into larger files.</p> \n<p>Depending on the data, also consider whether storing it in a columnar fashion, using for example Apache Parquet might be beneficial. You can find additional practical suggestions in our AWS Big Data Blog post <a href=\"https://aws.amazon.com/blogs/big-data/top-10-performance-tuning-tips-for-amazon-athena/\" target=\"_blank\" rel=\"noopener\">Top 10 Performance Tuning Tips for Amazon Athena</a>.</p> \n<p>All these options don’t replace what you learned in this article, but benefit from your being able to compare and contrast JSON formatted data and nested data. They can be used in a complementary fashion.</p> \n<p>Further, this <u><a href=\"https://aws.amazon.com/blogs/big-data/analyze-your-amazon-cloudfront-access-logs-at-scale/\" target=\"_blank\" rel=\"noopener\">AWS Big Data Blog post</a></u> walks you through a real-world scenario showing how to store and query data efficiently.</p> \n<hr>\n<h3>About the Author</h3> \n<p><img class=\"size-full wp-image-6353 alignleft\" src=\"https://d2908q01vomqb2.cloudfront.net/b6692ea5df920cad691c20319a6fffd7a4a766b8/2019/01/18/mkamp-1.png\" alt=\"\" width=\"113\" height=\"131\"><strong>Mariano Kamp is a principal solutions architect with Amazon Web Services. </strong>He works with financial services customers in Germany and has more than 25 years of industry experience covering a wide range of technologies. His area of depth is Analytics.</p> \n<p>In his spare time, Mariano enjoys hiking with his wife.</p> \n<p> </p> \n<p> </p> \n<p> </p> \n<p> </p>\n",
      "enclosure": {},
      "categories": [
        "Amazon Athena",
        "Amazon QuickSight",
        "Amazon Quicksight"
      ]
    },
    {
      "title": "Bannerconnect uses Amazon Redshift to help clients improve digital marketing results",
      "pubDate": "2019-01-03 18:26:31",
      "link": "https://aws.amazon.com/blogs/big-data/bannerconnect-uses-amazon-redshift-to-help-clients-improve-digital-marketing-results/",
      "guid": "2d59273d0d44d45b62708952e0829ee4b836b87a",
      "author": "Danny Stommen",
      "thumbnail": "https://d2908q01vomqb2.cloudfront.net/b6692ea5df920cad691c20319a6fffd7a4a766b8/2018/12/28/BannerconnectRedshift1.png",
      "description": "Bannerconnect uses programmatic marketing solutions that empower advertisers to win attention and customers by getting their ads seen by the right person at the right time and place. Data-driven insights help large advertisers, trade desks, and agencies boost brand awareness and maximize the results of their digital marketing. Timely analysis of log data is critical […]",
      "content": "\n<p>Bannerconnect uses programmatic marketing solutions that empower advertisers to win attention and customers by getting their ads seen by the right person at the right time and place. Data-driven insights help large advertisers, trade desks, and agencies boost brand awareness and maximize the results of their digital marketing. Timely analysis of log data is critical to respond to dynamic changes in customer behavior, optimize marketing campaigns quickly, and to gain competitive advantage.</p> \n<p>By moving to AWS and Amazon Redshift, our clients can now get near real-time analytics at their fingertips. In this blog post, we describe the challenges that we faced with our legacy, on-premises data warehouse, and discuss the benefits we received by moving to Amazon Redshift. Now, we can ingest data faster, do more sophisticated analytics, and help our clients make faster, data-driven decisions to improve their digital marketing.</p> \n<h2>Legacy on-premises situation and challenges</h2> \n<p>Our on-premises, legacy infrastructure consisted of the IBM PureData System as our log level data warehouse. We used a MySQL database for storing all metadata and all analytics data. In this physical, nonvirtualized environment, we needed to plan capacity carefully, far in advance, to handle data growth. We needed a sizeable team to manage upgrades, maintenance, backups, and the day-to-day management of workloads and query performance.</p> \n<p>We faced many challenges. We had only 1 gigabyte of bandwidth available to load log-level data into the data warehouse. At peak loads, our extract, transform, load (ETL) server ran completely full, and bandwidth became a bottleneck that delayed when the data was available for analytics. Software and firmware upgrades to the data warehouse needed to be scheduled, and maintenance downtime sometimes took up to eight hours to complete. Our infrastructure was also fragile. We ran everything on one PureData System, and we didn’t have a separate development and test environment. Clients that had direct access to our production environment could submit incorrect SQL queries and pull down the entire data warehouse.</p> \n<p>From the log-level data, we created aggregates and stored them in MySQL. Indexes slowed down the loading process significantly. Several aggregations that we wanted to do were simply not possible. Running ad hoc (one-time) queries against 200 gigabytes of uncompressed, row-based data took ages to complete. Many dashboard queries took 10–15 minutes or longer, and were ultimately cancelled. Users were frustrated, so we knew that we had to evolve to a more responsive solution, end-to-end. We chose AWS and Amazon Redshift for our data warehouse.</p> \n<h2>Moving to Amazon Redshift</h2> \n<p>Because our legacy software was not designed to run in the cloud, we decided to rebuild our applications using all available AWS components. This saved us the hassle of any migration process, and we could design our applications to use the full potential of AWS.</p> \n<p>Our new infrastructure uses Amazon Redshift as our log-level data warehouse. We use a 40-node ds2.xlarge cluster for our production processes. Here, we run log-level queries to aggregate data for the analytics cluster, and run thousands of queries each day to optimize our marketing campaigns.</p> \n<p>We set up a separate 30-node ds2.xlarge Amazon Redshift cluster for client access. We replicate the log level data to this cluster, and allow our clients to run queries here without jeopardizing our production processes. Our clients perform data science queries against the data in this cluster.</p> \n<p>We also created a 24-node dc2.large cluster for high-performance queries that would not be affected by large, complex queries running on our other clusters. We use this cluster for ad hoc analytics on aggregated data, made available through our API.</p> \n<p>We use Amazon S3 as our main data store, giving us infinite storage. Amazon EC2 hosts our ETL processes, API, and several other applications.</p> \n<p><img class=\"alignnone size-full wp-image-6249\" src=\"https://d2908q01vomqb2.cloudfront.net/b6692ea5df920cad691c20319a6fffd7a4a766b8/2018/12/28/BannerconnectRedshift1.png\" alt=\"\" width=\"800\" height=\"303\"></p> \n<p><strong>Bannerconnect architecture. </strong>Amazon S3 is not added to the flowchart to make it simpler. You can add S3 to almost every arrow in the chart.</p> \n<h2>Our biggest gains</h2> \n<p>Creating our next-generation solution on AWS and Amazon Redshift provides many benefits for us. We simply follow the <a href=\"https://docs.aws.amazon.com/redshift/latest/dg/best-practices.html\" target=\"_blank\" rel=\"noopener\">best practices</a> provided in Amazon Redshift documentation.</p> \n<ul>\n<li>\n<strong>Managed service: </strong>We wanted to focus on what we are good at, developing software, and not managing and maintaining infrastructure. With Amazon Redshift, we no longer have to do software updates, firmware upgrades, or deal with broken hardware. We no longer need to plan capacity months in advance, or deal with integrating new servers into our environment. Amazon Redshift completely automates all of this for us, including scaling with our data growth, so we can focus on data and analytics to better serve our clients.</li> \n <li>\n<strong>A full, separate development and test environment: </strong>We now have an isolated Amazon Redshift cluster (single node), where we can perform, develop, and test without worrying about breaking the production environment. In our on-premises setup, we did have a development database, but it was always on the production infrastructure. Furthermore, we have an exact copy of our entire infrastructure in our test environment (obviously all in small scale and small instance types). This lets us run automated tests on each deployment to verify that all data flows work as expected.</li> \n <li>\n<strong>Infinite scalability:</strong> We can scale instantly to any capacity we need. Amazon S3 gives us infinite storage, and we can scale Amazon Redshift compute capacity in just a few clicks.</li> \n <li>\n<strong>Separate clusters: </strong>Clients now can’t jeopardize our production processes. Clients still write incorrect SQL queries. However, by using query monitoring rules in Amazon Redshift, we can identify these queries and have Amazon Redshift automatically stop them. Bad queries might affect the client cluster momentarily, but they don’t affect our production processes at all.</li> \n <li>\n<strong>Faster ad hoc analytics: </strong>Due to the massive parallel processing, data compression, and columnar-based storage capabilities in Amazon Redshift, we can create aggregates that were not possible in MySQL. In terms of performance, it’s hard to give good numbers to compare. Running a query against a smaller aggregate using an index on MySQL might be faster at times. However, the majority of our queries are significantly faster on Amazon Redshift. For example, our biggest aggregated table contains about 2 billion records and 500 GB of data (compressed). MySQL couldn’t handle this, but Amazon Redshift results are retrieved within seconds. Large, complex queries took a long time on MySQL. Amazon Redshift completes these in tens of seconds or less.</li> \n</ul>\n<h2>Building the multicluster environment</h2> \n<p>This section explores an easy option to build a multicluster setup using AWS CloudFormation templates. With the templates, you can launch multiple Amazon Redshift clusters inside a VPC in both private and public subnets in different Availability Zones. The private subnet enables internal applications, such as EC2 instances, to execute the ETL process to interact with the Amazon Redshift cluster to refresh data. You can use the public Amazon Redshift cluster for the external client tools and scripts. Here is the architecture of this setup:</p> \n<p><img class=\"alignnone size-full wp-image-6250\" src=\"https://d2908q01vomqb2.cloudfront.net/b6692ea5df920cad691c20319a6fffd7a4a766b8/2018/12/28/BannerconnectRedshift2.png\" alt=\"\" width=\"800\" height=\"730\"></p> \n<p>Let’s walk through the configuration. For demonstration purposes, we use just two Amazon Redshift clusters in a private and public subnet, but you can modify these steps to add more parallel clusters. The configuration is a two-step process to first create the network stack and later launch the Amazon Redshift cluster in those stacks. This process creates the following:</p> \n<ul>\n<li>VPC and associated subnets, security groups, and routes</li> \n <li>IAM roles to load data from S3 to Amazon Redshift</li> \n <li>Amazon Redshift cluster or clusters</li> \n</ul>\n<h3>Directions</h3> \n<p><strong>Step 1 – Create a network stack</strong></p> \n<ol>\n<li>Sign in to the AWS Management Console and navigate to CloudFormation, then do the following:</li> \n</ol>\n<ul>\n<li>Choose the AWS Region to launch the stack in, for example US East (Ohio).</li> \n <li>Choose <strong>Create Stack</strong>.</li> \n <li>Choose <strong>Specify an Amazon S3 template URL</strong>.</li> \n <li>Copy and paste this URL into the text box: https://s3.amazonaws.com/salamander-us-east-1/Bannerconnect/networkstack.json</li> \n</ul>\n<ol start=\"2\">\n<li>Choose <strong>Next</strong> and provide the following information: \n  <ul>\n<li>\n<strong>Stack Name</strong>: Name the stack anything convenient.</li> \n   <li>\n<strong>CIDR Prefix</strong>: Enter a class B CIDR prefix (for example, 168, 10.1, or 172.16).</li> \n   <li>\n<strong>Environment Tag</strong>: Name the environment anything convenient to tag the resources.</li> \n   <li>\n<strong>Key Name</strong>: The EC2 key pair to allow access to EC2 instances. If you don’t already have one, see <a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-key-pairs.html\" target=\"_blank\" rel=\"noopener\">Amazon EC2 Key Pairs</a> in the EC2 documentation.</li> \n   <li>Use the default values for all other parameters and choose <strong>Create</strong>.</li> \n  </ul>\n</li> \n <li>The stack takes 10 minutes to launch, after which the network stack is ready.</li> \n <li>Review the outputs of the stack when the launch is complete to note the resource names that were created.</li> \n</ol>\n<p><strong>Step 2 – Create an Amazon Redshift cluster or clusters</strong></p> \n<ol>\n<li>Navigate back to the CloudFormation console and do the following:</li> \n</ol>\n<ul>\n<li>Choose <strong>Create Stack</strong>.</li> \n <li>Choose <strong>Specify an Amazon S3 template URL</strong>.</li> \n <li>Copy and paste this URL into the text box: https://s3.amazonaws.com/salamander-us-east-1/Bannerconnect/reshiftstack.json</li> \n</ul>\n<ol start=\"2\">\n<li>Choose <strong>Next</strong> and provide the following information: \n  <ul>\n<li>\n<strong>Stack Name</strong>: Name the stack anything convenient.</li> \n   <li>\n<strong>Cluster Type</strong>: Choose a multi-node or single-node cluster.</li> \n   <li>\n<strong>Inbound Traffic</strong>: Allow inbound traffic to the cluster from this CIDR range.</li> \n   <li>\n<strong>Master Username</strong>: The user name that is associated with the master user account for the cluster that is being created. The default is adminuser.</li> \n   <li>\n<strong>Master User Password</strong>: The password that is associated with the master user account for the cluster that is being created.</li> \n   <li>\n<strong>Network Stack Name</strong>: The name of the active CloudFormation stack that was created in step 1, which contains the networking resources such as the subnet and security group.</li> \n   <li>\n<strong>Node Type</strong>: The node type to be provisioned for the Amazon Redshift cluster.</li> \n   <li>\n<strong>Number Of Nodes</strong>: The number of compute nodes in the Amazon Redshift cluster: \n    <ul>\n<li>When the cluster type is specified as single-node, this value should be 1.</li> \n     <li>When the cluster type is specified as multi-node, this value should be greater than 1.</li> \n    </ul>\n</li> \n   <li>\n<strong>Port Number</strong>: The port number on which the cluster accepts incoming connections. The default is 5439.</li> \n   <li>\n<strong>Public Access</strong>: Public access to the Amazon Redshift cluster, either <strong>true</strong> or <strong>false</strong>. When this value is <strong>true</strong>, the cluster is launched in a public subnet. When this value is <strong>false</strong>, the cluster is launched in a private subnet.</li> \n   <li>Use the default values for all other parameters and choose <strong>Create</strong>.</li> \n  </ul>\n</li> \n <li>The stack take 10 minutes to launch, after which the Amazon Redshift cluster is launched in the network stack.</li> \n <li>Review the outputs of the stack when the launch is complete to note the resource names that were created for the Amazon Redshift cluster.</li> \n <li>Repeat steps 5–8 to add more Amazon Redshift clusters to this network stack.</li> \n</ol>\n<p>With this easy deployment using the AWS CloudFormation template, you can launch all the resources needed for a multicluster setup with a few clicks.</p> \n<h2>Conclusion</h2> \n<p>Migrating to Amazon Redshift and setting up the data warehouse on AWS enabled us to build highly scalable decoupled applications and to use different clusters for different use cases. Operationally, we were able to build robust dev, test, and prod systems independently that are easy to manage to implement complex data workflows.</p> \n<p>Recently, we started using Amazon Redshift Spectrum to query data directly from Amazon S3, without needing to load the data into Amazon Redshift. This saves us loading time and speeds up time to analytics, creating many new possibilities for us. Loading dynamic data with different formats and columns becomes a lot easier with Amazon Redshift Spectrum.</p> \n<p> </p> \n<hr>\n<h3>About the Authors</h3> \n<p><strong><img class=\"size-full wp-image-6252 alignleft\" src=\"https://d2908q01vomqb2.cloudfront.net/b6692ea5df920cad691c20319a6fffd7a4a766b8/2018/12/28/DannyStommen.png\" alt=\"\" width=\"113\" height=\"168\">Danny Stommen has been working for Bannerconnect for 10 years</strong>, with his current role being Solutions Architect and most of his time working on the CORE solution. Next to work, he enjoys spending quality time with his family and actively playing soccer.</p> \n<p> </p> \n<p> </p> \n<p> </p> \n<p><img class=\"size-full wp-image-6253 alignleft\" src=\"https://d2908q01vomqb2.cloudfront.net/b6692ea5df920cad691c20319a6fffd7a4a766b8/2018/12/28/Thiyagu.png\" alt=\"\" width=\"113\" height=\"168\"><strong>Thiyagarajan Arumugam is a Big Data Solutions Architect at Amazon Web Services</strong> and designs customer architectures to process data at scale. Before working at AWS, he built data warehouse solutions at Amazon.com. In his free time, he enjoys all outdoor sports and practices the Indian classical drum mridangam.</p>\n",
      "enclosure": {},
      "categories": [
        "Amazon Redshift"
      ]
    },
    {
      "title": "Running Amazon Payments analytics on Amazon Redshift with 750TB of data",
      "pubDate": "2018-12-29 00:56:51",
      "link": "https://aws.amazon.com/blogs/big-data/running-amazon-payments-analytics-on-amazon-redshift-with-750tb-of-data/",
      "guid": "e469177fda00f2dcfa7883f6ea7a15de34825170",
      "author": "Bishwabandhu Newton",
      "thumbnail": "https://d2908q01vomqb2.cloudfront.net/b6692ea5df920cad691c20319a6fffd7a4a766b8/2018/12/29/Redshift7501.png",
      "description": "The Amazon Payments Data Engineering team is responsible for data ingestion, transformation, and the computation and storage of data. It makes these services available for more than 300 business customers across the globe. These customers include product managers, marketing managers, program managers, data scientists, business analysts and software development engineers. They use the data for […]",
      "content": "\n<p>The Amazon Payments Data Engineering team is responsible for data ingestion, transformation, and the computation and storage of data. It makes these services available for more than 300 business customers across the globe. These customers include product managers, marketing managers, program managers, data scientists, business analysts and software development engineers. They use the data for their scheduled and one-time queries to make the right business decisions. This data is also used for building weekly, monthly and quarterly business review metrics, which are reviewed by the leadership team.</p> \n<p>We support various consumer payment business teams, including the following:</p> \n<ul>\n<li>Amazon Payment Products (Credit cards, Shop with Points, Amazon Currency Convertor, International Payment Products)</li> \n <li>Gift Cards</li> \n <li>Payment Acceptance Experience</li> \n <li>Amazon Business Payments.</li> \n</ul>\n<p>We also feed into the machine learning recommendation engine. This engine suggests the best payment product to a customer on Amazon’s payment checkout page.</p> \n<h2>Challenges with old datawarehouse</h2> \n<p>This section describes our previous challenges with our data warehouse and analytical needs. With payment products launches and their extension to new marketplaces, we had exponential growth in data volume. Later, scaling our extract, transform, and load process (ETL) was met with severe challenges, and resulted in delays and operational burdens. Here are the specific challenges we faced with our data warehouse:</p> \n<ul>\n<li>Upsert did not scale, so we got updates more than ~10MN per run. The consumer product catalog dataset has more than 6.5BN records listed in the US marketplace, and occasionally the daily updates exceeded 10MN mark. We saw a similar trend for the Order attributes dataset.</li> \n <li>Data aggregation either took longer or never finished if we had to analyze even for 6 months of payments data. Often, business owners wanted to aggregate the data based on certain attributes. For example, the number of successful transactions and monetary value by certain types of cards.</li> \n <li>Shared cluster, and thus shared storage and compute caused resource crunch and impacted all its users. Each team was given ~100TB each on the Data warehouse. Each team could bring their table and join with central data warehouse tables. Any bad query on the cluster impacted all other queries on the same cluster. It was difficult to identify the owner of those bad queries.</li> \n <li>There were more than 30,000 production tables. it became almost impossible to host all of them on the same cluster.</li> \n <li>Index corruption on a larger table was cumbersome to rebuild and backfill the table.</li> \n <li>We required a Database Administrator to apply patches and updates.</li> \n</ul>\n<h2>Using Amazon Redshift as the new payments data warehouse</h2> \n<p>We started exploring different options which suits our analytical needs, which is fast, reliable and scales well for future data growth. With all of the previously described issues, Central Data warehouse moved towards separating the compute and storage layer and they decided to be responsible for storage. They built a data lake on <a href=\"https://aws.amazon.com/s3/?nc2=h_m1\" target=\"_blank\" rel=\"noopener\">Amazon S3</a>, which is encrypted to store even the highly confidential critical data. Each consumer team got the guideline to bring their own compute capacity for their analytical needs. Our payments team started looking for the following advantages:</p> \n<ul>\n<li>Expedient analytics.</li> \n <li>Integrates with S3 and the other AWS services.</li> \n <li>Affordable storage and compute rates.</li> \n <li>Works for ETL processing.</li> \n</ul>\n<p>We chose <a href=\"https://aws.amazon.com/redshift/?nc2=h_m1\" target=\"_blank\" rel=\"noopener\">Amazon Redshift</a> because of it has the following features:</p> \n<ul>\n<li>Bulk uploads are faster. ~700MN data inserts into ~30 minutes.</li> \n <li>Data upsert is exceptionally fast.</li> \n <li>Aggregation query on multi-million dataset with fewer columns of data returns in a few seconds as compared to a few minutes.</li> \n <li>No need for DBA time to be allocated to maintain the database. Data engineers can easily perform backups, re-snapshot to a new cluster, set up alarms in case of cluster issues, and add new nodes.</li> \n <li>The ability to keep data on S3. This data is accessible from multiple independent Amazon Redshift clusters through Spectrum and also allows users to join Spectrum tables with other tables created locally on Amazon Redshift. It offloads some processing to the Spectrum layer while storing the data on S3.</li> \n <li>The ability to use Amazon Redshift best practices to design our tables in regards to distribution keys, sort keys, and compression. As a result, the query performance exceeded our SLA expectations.</li> \n <li>An effective compression factor. This saves more than 40 to 50 percent of space by choosing the right compression. This enables faster query and efficient storage option.</li> \n</ul>\n<h2>Sources of data and storage</h2> \n<p>We consume data from different sources, like PostgreSQL, <a href=\"https://aws.amazon.com/dynamodb/?nc2=h_m1\" target=\"_blank\" rel=\"noopener\">Amazon DynamoDB</a> live streams, Central data warehouse data lake and bank partners’ data through secure protocols. Data from PostgreSQL databases are in relational format, whereas DynamoDB has key value pairs. We translate the key/value data to relational format and store in Amazon Redshift and S3. Most frequently accessed data is kept in Amazon Redshift. Less frequently accessed and larger datasets are stored in S3 and accessed through Amazon Redshift Spectrum.</p> \n<p>Central data lake hosts more than 30,000 tables from different teams, such as Orders, Shipments, and Refunds. Whereas, we as a payments team need approximately 200 tables from this data lake as source tables. Later, we built a data mart specific to payment products, which feeds both to scheduled and one-time data and reporting needs. All small and mid-size tables, smaller than 50 TB, are directly loaded in Amazon Redshift from data lake, which physically stores the data. Tables larger than 50 TB are not stored locally on Amazon Redshift. Instead, we pull from the data lake using EMR-Hive, convert the format from tsv to ORC/Parquet and store on S3. We create an Amazon Redshift Spectrum table on top of S3 data. Format conversion lowers the runtime for each analytical aggregation queries, whereas storing on S3 makes sure we do not fill up entire Amazon Redshift cluster with data rather use it for efficient computing.</p> \n<h2>Data Architecture</h2> \n<p><img class=\"alignnone size-full wp-image-6273\" src=\"https://d2908q01vomqb2.cloudfront.net/b6692ea5df920cad691c20319a6fffd7a4a766b8/2018/12/29/Redshift7501.png\" alt=\"\" width=\"707\" height=\"877\"></p> \n<h3>Different components</h3> \n<ol>\n<li>\n<strong><u>Central data warehouse data lake (Andes)</u></strong> — Almost all the systems in Amazon, wanting to share their data with other teams, publish their data to this datalake. It is an encrypted storage built on top of Amazon S3 which has metadata attached along with datafiles. Every dataset has a onetime dump and then incremental delta files. Teams willing to consume the data by</li> \n</ol>\n<ul>\n<li>Physically copying the data into their own Amazon Redshift cluster. It is efficient for smaller and mid-size tables which are accessed most frequently.</li> \n <li>Using Amazon Redshift Spectrum to run analytical queries on datasets stored in data lake. It helps in accessing cold large data, generally larger than 50TB, thereby avoids scaling up your Amazon Redshift cluster just because all the space might be consumed by these larger data files.</li> \n <li>Using the <a href=\"https://aws.amazon.com/glue/?hp=tile&amp;so-exp=below\" target=\"_blank\" rel=\"noopener\">AWS Glue</a> catalog to update the metadata in your team’s S3 bucket and use Amazon EMR to pull the data, apply transformation, change format and store the final data in S3 bucket, which can further be consumed using Amazon Redshift Spectrum. It is efficient when the dataset is large and need transformations before being consumed.</li> \n</ul>\n<ol start=\"2\">\n<li>\n<strong>Amazon Redshift clusters</strong> — Amazon Redshift has centric architecture and is best suited for being single place for all source of truth, but we are managing three clusters mostly because of having consistent SLA of our reports, decoupling the user query experience with central data lake ingestion process (which is resource intensive). Here are the cluster specific reasons for why we need these as separate clusters.</li> \n</ol>\n<ul>\n<li>Staging cluster: \n  <ul>\n<li>Our data sources are dynamic which are in the transition state and moving away from relational to non-relational sources; for example, Oracle to Postgres or to DynamoDB.</li> \n   <li>The mechanism to pull data from central data lake and store into Amazon Redshift, is also evolving and is resource intensive in current state.</li> \n   <li>Our datamart is payment specific, though the table names in our datamart looks similar to central data lakes tables, but our datamart data is different than central data lake datasets. We apply necessary transformation and filters before bringing the data to the user’s Amazon Redshift cluster.</li> \n  </ul>\n</li> \n</ul>\n<ul>\n<li>User cluster: Our internal business users wanted to create the tables in public schema for their analysis. They also needed direct connect access for any adhoc analysis. Most of the users know SQL and are aware of best practices but there are users who are new to SQL and sometimes their query is not optimized and impact other running queries, we have <a href=\"https://aws.amazon.com/blogs/big-data/run-mixed-workloads-with-amazon-redshift-workload-management/\" target=\"_blank\" rel=\"noopener\">Amazon Redshift workload manager (WLM)</a> settings to protect our cluster from these bad queries.</li> \n <li>Prod ETL cluster: We have tight SLA to make dataset available for data users. In order to minimize the impact of bad queries running on the system we have set up replica of user cluster. All prod transforms run here and the output data is copied to both user and prod clusters. It insures the SLA we commit to data business users.</li> \n</ul>\n<ol start=\"3\">\n<li>\n<strong>Near real time data ingestion</strong> — Many applications like promotional data, card registration, gift card issuance etc need realtime data collection to detect fraud. Application data is stored in Amazon DynamoDB, with DynamoDB Streams enabled. We consume the data from these streams through an <a href=\"https://aws.amazon.com/lambda/?nc2=h_m1\" target=\"_blank\" rel=\"noopener\">AWS Lambda</a> function and <a href=\"https://aws.amazon.com/kinesis/data-firehose/\" target=\"_blank\" rel=\"noopener\">Amazon Kinesis Data Firehose</a>. Kinesis Firehose delivers the data to S3 and submits the copy command to load the data into Redshift. We have micro batch of 15 mins which makes sure not all the connections are consumed by these near-real time data applications.</li> \n <li>\n<strong>Alternate compute on <a href=\"https://aws.amazon.com/emr/\" target=\"_blank\" rel=\"noopener\">Amazon EMR</a></strong> — We receive website clicks data through clickstream feeds which can run into billions of records in a day for each marketplace. These large datasets are critical but less frequently accessed on Amazon Redshift. We decided to choose S3 as a storage option and applied the transformations using Amazon EMR. With this approach, we made sure we do not fill up the database with massive cold data and at the same time we enable data access on S3 using Amazon Redshift Spectrum, which provided similar query performance. As Amazon Redshift is a columnar database and is exceptionally fast for any sort of aggregation if we choose fewer dimensional columns. We wanted similar performance for the data we stored on S3. We were able to do it using Amazon EMR and by changing the data format from TSV to ORC or Parquet. Every day, we created new partition data on S3 and refreshed the Amazon Redshift Spectrum table definition to include new partition data. These Spectrum table were accessed by business users for their one-time analysis using any Amazon Redshift SQL client or for scheduling any ETL pipeline.</li> \n <li>\n<strong>Publish the data to data warehouse datalake for non-payment users</strong> — We built payment specific datasets. For example, decline transaction behavior, wallet penetration and others. Sometimes non-payments business users are also interested in consuming these datasets. We publish these datasets to central data warehouse data lake for them. Additionally, payments application teams are the source for payment product application data. Data engineering team consumes these datasets, apply needed transformation and publish it both for payments and non-payments user through Amazon Redshift and the data lake.</li> \n</ol>\n<h3>Schema management</h3> \n<p>We have a prod schema which stores all production tables and only platform team has access to make any changes to it. We also provide payment product specific sandboxes which is accessible by product specific member. There is generic public schema for any payments data users. They can create, load, truncate/drop the tables in this schema.</p> \n<h3>Database and ETL lookup</h3> \n<p>Here are few fun facts about our Amazon Redshift database objects.</p> \n<ul>\n<li>Number of Databases: 4 \n  <ul>\n<li>Staging Database DB1: ds2.8xlarge x 20 nodes<br> Memory. 244 GiB per node<br> Storage. 16TB HDD storage per node</li> \n   <li>User Database DB2: ds2.8xlarge x 24 nodes<br> Memory. 244 GiB per node<br> Storage. 16TB HDD storage per node</li> \n   <li>Platform Database DB3: ds2.8xlarge x 24 nodes<br> Memory. 244 GiB per node<br> Storage. 16TB HDD storage per node</li> \n   <li>Reporting Database DB4: ds2.8xlarge x 4 nodes<br> Memory. 244 GiB per node<br> Storage. 16TB HDD storage per node</li> \n  </ul>\n</li> \n</ul>\n<ul>\n<li>Size of Databases: \n  <ul>\n<li>Total memory: 17 TB</li> \n   <li>Total storage: 1.15 Petabytes</li> \n  </ul>\n</li> \n</ul>\n<ul>\n<li>Number of tables: \n  <ul>\n<li>Analytics prod db: 6500</li> \n   <li>Analytics staging db: 390</li> \n  </ul>\n</li> \n</ul>\n<h3><strong>User cluster</strong></h3> \n<p>Here are some stats around the database that is exposed to the users. It has both core tables and users are allowed to create their own tables based on their need. We have the mirror image of the same database, which hosts all of the tables, except user=created tables. Another database is used to run ETL platform related prod pipeline. Most of these tables have entire history, except the snapshot tables like clickstream datasets, which have been archived to S3.</p> \n<p><img class=\"alignnone size-full wp-image-6272\" src=\"https://d2908q01vomqb2.cloudfront.net/b6692ea5df920cad691c20319a6fffd7a4a766b8/2018/12/29/Redshift7502_1.png\" alt=\"\" width=\"800\" height=\"486\"></p> \n<p><img class=\"alignnone size-full wp-image-6275\" src=\"https://d2908q01vomqb2.cloudfront.net/b6692ea5df920cad691c20319a6fffd7a4a766b8/2018/12/29/Redshift7503_1.png\" alt=\"\" width=\"800\" height=\"487\"></p> \n<p><strong>Staging cluster</strong></p> \n<p>Here are some stats around the staging database. It is the landing zone for all the data coming from other teams or from the central data warehouse data lake. Table retention has been applied to all the tables as most of the ELT downstream jobs look for the last updated date, pulls just incremental data and store in user and replica databases.</p> \n<p><img class=\"alignnone size-full wp-image-6258\" src=\"https://d2908q01vomqb2.cloudfront.net/b6692ea5df920cad691c20319a6fffd7a4a766b8/2018/12/28/Redshift7504.png\" alt=\"\" width=\"1586\" height=\"972\"></p> \n<p><img class=\"alignnone size-full wp-image-6259\" src=\"https://d2908q01vomqb2.cloudfront.net/b6692ea5df920cad691c20319a6fffd7a4a766b8/2018/12/28/Redshift7505.png\" alt=\"\" width=\"1586\" height=\"980\"></p> \n<h3>Scheduled ETL and Query load on database</h3> \n<ul>\n<li>Number of daily extraction ETL jobs: 2943</li> \n <li>Number of loading ETL jobs: 1655</li> \n</ul>\n<p><img class=\"alignnone size-full wp-image-6285\" src=\"https://d2908q01vomqb2.cloudfront.net/b6692ea5df920cad691c20319a6fffd7a4a766b8/2019/01/04/750_Replace1-1.png\" alt=\"\" width=\"800\" height=\"447\"></p> \n<p><img class=\"alignnone size-full wp-image-6286\" src=\"https://d2908q01vomqb2.cloudfront.net/b6692ea5df920cad691c20319a6fffd7a4a766b8/2019/01/04/750_Replace2.png\" alt=\"\" width=\"800\" height=\"425\"></p> \n<ul>\n<li>Total daily load processed volume: 119 BN</li> \n <li>Total daily loading runtime: 11,415 mins</li> \n</ul>\n<p><img class=\"alignnone size-full wp-image-6287\" alt=\"\" width=\"800\" height=\"296\"></p> \n<ul>\n<li>Total daily data extraction volume: 166 BN</li> \n <li>Total daily date extraction runtimes: 25,585 mins</li> \n</ul>\n<p><img class=\"alignnone size-full wp-image-6288\" src=\"https://d2908q01vomqb2.cloudfront.net/b6692ea5df920cad691c20319a6fffd7a4a766b8/2019/01/04/750_Replace4.png\" alt=\"\" width=\"800\" height=\"387\"></p> \n<p><strong>Both scheduled and one-time query loads on the database</strong></p> \n<ul>\n<li>Daily query load on database by different database users.</li> \n</ul>\n<p><img class=\"alignnone size-full wp-image-6289\" src=\"https://d2908q01vomqb2.cloudfront.net/b6692ea5df920cad691c20319a6fffd7a4a766b8/2019/01/04/750_Replace5.png\" alt=\"\" width=\"800\" height=\"475\"></p> \n<p><strong>Best practices</strong></p> \n<ol>\n<li>Design tables with right sort keys and distribution key: Query performance is dependent on how much data it scans and if the joins are co-located join. Choosing right sort key make sure we do not scan the data which we do not need and selecting right distribution key makes sure the joining data is present on the same node and there is less movement of data over network resulting in better query performance. For more information, see <a href=\"https://docs.aws.amazon.com/redshift/latest/dg/c_designing-tables-best-practices.html\" target=\"_blank\" rel=\"noopener\">Amazon Redshift Best Practices for Designing Tables</a>.</li> \n</ol>\n<ol start=\"2\">\n<li>Refer to <a href=\"https://docs.aws.amazon.com/redshift/latest/dg/c_designing-queries-best-practices.html\" target=\"_blank\" rel=\"noopener\">Amazon Redshift Best Practices for Designing Queries</a> while writing the query.</li> \n</ol>\n<ol start=\"3\">\n<li>Change the loading strategy by splitting larger files into smaller files, use bulk loads instead serial inserts. For more information, see <a href=\"https://docs.aws.amazon.com/redshift/latest/dg/c_loading-data-best-practices.html\" target=\"_blank\" rel=\"noopener\">Amazon Redshift Best Practices for Loading Data</a>.</li> \n</ol>\n<ol start=\"4\">\n<li>Configure the appropriate WLM setting to avoid system abuse by allocating right run-times, memory, priority queues, etc. For more information, see <a href=\"https://docs.aws.amazon.com/redshift/latest/dg/tutorial-configuring-workload-management.html\" target=\"_blank\" rel=\"noopener\">Tutorial: Configuring Workload Management (WLM) Queues to Improve Query Processing</a>.</li> \n</ol>\n<ol start=\"5\">\n<li>Use Amazon Redshift advisor to identify potential tables needing compression, tables with missing stats, uncompressed data loads, and further fine tune your ETL pipelines. For more information, see <a href=\"https://docs.aws.amazon.com/redshift/latest/dg/advisor.html\" target=\"_blank\" rel=\"noopener\">Working Recommendations from Amazon Redshift Advisor</a>.</li> \n</ol>\n<ol start=\"6\">\n<li>Identify the tables with most of the wasted space and vacuum them frequently. It releases the wasted space and, at the same time, increases the query performance. For more information, see <a href=\"https://docs.aws.amazon.com/redshift/latest/dg/t_Reclaiming_storage_space202.html\" target=\"_blank\" rel=\"noopener\">Vacuuming Tables</a>.</li> \n</ol>\n<ol start=\"7\">\n<li>Analyze the SQLs submitted to DB and identify the pattern on table usage and expensive joins. It helps data engineers build more denormalized tables by pre-joining these tables, and helping users access a single table, which is fast and efficient.</li> \n</ol>\n<h2>Conclusion and Next Steps</h2> \n<p>Amazon Redshift clusters with total capacity of 1.15 PB, ~6500 tables, 4500 scheduled ETL runs, 13,000 ETL queries a day, is solving almost all the ETL need of business users in payments team. However, the recent volume growth is filling up our dbs more than we expected, Next step could be choosing cheaper storage option by building a datalake on S3 and access them using Amazon Redshift spectrum without even bothering about scaling challenges and with seamless user experience.</p> \n<p> </p> \n<hr>\n<h3>About the authors</h3> \n<p><strong><img class=\"size-full wp-image-6270 alignleft\" src=\"https://d2908q01vomqb2.cloudfront.net/b6692ea5df920cad691c20319a6fffd7a4a766b8/2018/12/29/BishwaNewton.png\" alt=\"\" width=\"113\" height=\"149\">Bishwabandhu Newton is a senior data engineer with Amazon Consumer Payments team</strong>. He has over 12 years of data warehousing experience, with 9+ years at Amazon.com.</p> \n<p><strong> </strong></p> \n<p> </p> \n<p> </p> \n<p><img class=\"size-full wp-image-6268 alignleft\" src=\"https://d2908q01vomqb2.cloudfront.net/b6692ea5df920cad691c20319a6fffd7a4a766b8/2018/12/28/mscaer.png\" alt=\"\" width=\"113\" height=\"154\"><strong>Matt Scaer is a Principal Data Warehousing Specialist Solution Architect</strong>, with over 20 years of data warehousing experience, with 11+ years at both AWS and Amazon.com.</p>\n",
      "enclosure": {},
      "categories": [
        "Amazon Redshift"
      ]
    },
    {
      "title": "Analyze your Amazon CloudFront access logs at scale",
      "pubDate": "2018-12-21 18:01:36",
      "link": "https://aws.amazon.com/blogs/big-data/analyze-your-amazon-cloudfront-access-logs-at-scale/",
      "guid": "3601230fa7ff68d99d99b3b0308d24cdec3faed7",
      "author": "Steffen Grunwald",
      "thumbnail": "https://d2908q01vomqb2.cloudfront.net/b6692ea5df920cad691c20319a6fffd7a4a766b8/2018/09/07/LaunchStack.png",
      "description": "This blog post focuses on two measures to restructure Amazon CloudFront access logs for optimization: partitioning and conversion to columnar formats. For more details on performance tuning read the blog post about the top 10 performance tuning tips for Amazon Athena.",
      "content": "\n<p>Many AWS customers are using <a href=\"https://aws.amazon.com/cloudfront/\" target=\"_blank\" rel=\"noopener\">Amazon CloudFront</a>, a global content delivery network (CDN) service. It delivers websites, videos, and API operations to browsers and clients with low latency and high transfer speeds. Amazon CloudFront protects your backends from massive load or malicious requests by caching or a web application firewall. As a result, sometimes only a small fraction of all requests gets to your backends. You can configure Amazon CloudFront to store access logs with detailed information of every request to <a href=\"https://aws.amazon.com/s3/\" target=\"_blank\" rel=\"noopener\">Amazon Simple Storage Service</a> (S3). This lets you gain insight into your cache efficiency and learn how your customers are using your products.</p> \n<p>A common choice to run standard SQL queries on your data in S3 is <a href=\"https://aws.amazon.com/athena/\" target=\"_blank\" rel=\"noopener\">Amazon Athena</a>. Queries analyze your data immediately without the prior setup of infrastructure or loading your data. You pay only for the queries that you run. Amazon Athena is ideal for quick, interactive querying. It supports complex analysis of your data, including large joins, unions, nested queries, and window functions.</p> \n<p>This blog post shows you how you can restructure your Amazon CloudFront access logs storage to optimize the cost and performance for queries. It demonstrates <strong>common patterns</strong> that are also applicable to other sources of <strong>time series</strong> data.</p> \n<h2>Optimizing Amazon CloudFront access logs for Amazon Athena queries</h2> \n<p>There are two main aspects to optimize: cost and performance.</p> \n<p><strong>Cost</strong> should be low for both storage of your data and the queries. Access logs are stored in S3, which is billed by GB/ month. Thus, it makes sense to <em>compress</em> your data – especially when you want to keep your logs for a long time. Also cost incurs on queries. When you optimize the storage cost, usually the query cost follows. Access logs are delivered compressed by gzip and Amazon Athena can deal with compression. Amazon Athena is billed by the amount of compressed data scanned, so the benefits of compression are passed on to you as cost savings.</p> \n<p>Queries further benefit from <em>partitioning</em>. Partitioning divides your table into parts and keeps the related data together based on column values. For time-based queries, you benefit from partitioning by year, month, day, and hour. In Amazon CloudFront access logs, this indicates the request time. Depending on your data and queries, you add further dimensions to partitions. For example, for access logs it could be the domain name that was requested. When querying your data, you specify filters based on the partition to make Amazon Athena scan less data.</p> \n<p>Generally, <strong>performance</strong> improves by scanning less data. Conversion of your access logs to <em>columnar formats</em> reduces the data to scan significantly. Columnar formats retain all information but store values by column. This allows creation of dictionaries, and effective use of Run Length Encoding and other compression techniques. Amazon Athena can further optimize the amount of data to read, because it does not scan columns at all if a column is not used in a filter or the result of a query. Columnar formats also split a file into chunks and calculate metadata on file- and chunk level like the range (min/ max), count, or sum of values. If the metadata indicates that the file or chunk is not relevant for the query Amazon Athena skips it. In addition, if you know your queries and the information you are looking for, you can further <em>aggregate</em> your data (for example, by day) for improved performance of frequent queries.</p> \n<p>This blog post focuses on two measures to restructure Amazon CloudFront access logs for optimization: <strong>partitioning</strong> and conversion to<strong> columnar formats</strong>. For more details on performance tuning read the blog post about the <a href=\"https://aws.amazon.com/blogs/big-data/top-10-performance-tuning-tips-for-amazon-athena/\" target=\"_blank\" rel=\"noopener\">top 10 performance tuning tips for Amazon Athena</a>.</p> \n<p>This blog post describes the concepts of a solution and includes code excerpts for better illustration of the implementation. Visit the AWS Samples repository for a fully working implementation of the concepts. Launching the packaged sample application from the <u><a href=\"https://serverlessrepo.aws.amazon.com/applications\" target=\"_blank\" rel=\"noopener\">AWS Serverless Application Repository</a></u>, you deploy it within minutes in one step:</p> \n<p><a href=\"https://serverlessrepo.aws.amazon.com/#/applications/arn:aws:serverlessrepo:us-east-1:387304072572:applications~amazon-cloudfront-access-logs-queries\" target=\"_blank\" rel=\"noopener\"><img class=\"alignnone size-full wp-image-5580\" src=\"https://d2908q01vomqb2.cloudfront.net/b6692ea5df920cad691c20319a6fffd7a4a766b8/2018/09/07/LaunchStack.png\" alt=\"\" width=\"234\" height=\"60\"></a></p> \n<p><span></span></p> \n<h2>Partitioning CloudFront Access Logs in S3</h2> \n<p>Amazon CloudFront delivers each access log file in CSV format to an S3 bucket of your choice. Its name adheres to the following format (for more information, see <a href=\"https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/AccessLogs.html\" target=\"_blank\" rel=\"noopener\">Configuring and Using Access Logs</a>):</p> \n<div class=\"hide-language\"> \n <pre><code class=\"lang-code\">/optional-prefix/distribution-ID.YYYY-MM-DD-HH.unique-ID.gz</code></pre> \n</div> \n<p>The file name includes the date and time of the period in which the requests occurred in Coordinated Universal time (UTC). Although you can specify an optional prefix for an Amazon CloudFront distribution, all access log files for a distribution are stored with the same prefix.</p> \n<p>When you have a large amount of access log data, this makes it hard to only scan and process parts of it efficiently. Thus, you must partition your data. Most tools in the big data space (for example, the Apache Hadoop ecosystem, Amazon Athena, <a href=\"https://aws.amazon.com/glue/?hp=tile&amp;so-exp=below\" target=\"_blank\" rel=\"noopener\">AWS Glue</a>) can deal with partitioning using the <a href=\"https://cwiki.apache.org/confluence/display/Hive/LanguageManual+DDL#LanguageManualDDL-AlterPartition\" target=\"_blank\" rel=\"noopener\">Apache Hive</a> style. A partition is a directory that is self-descriptive. The directory name not only reflects the value of a column but also the column name. For access logs this is a desirable structure:</p> \n<div class=\"hide-language\"> \n <pre><code class=\"lang-code\">/optional-prefix/year=YYYY/month=MM/day=DD/hour=HH/distribution-ID.YYYY-MM-DD-HH.unique-ID.gz</code></pre> \n</div> \n<p>To generate this structure, the sample application initiates the processing of each file by an S3 event notification. As soon as Amazon CloudFront puts a new access log file to an S3 bucket, an event triggers the AWS Lambda function <code>moveAccessLogs</code>. This moves the file to a prefix corresponding to the filename. Technically, the move is a copy followed by deletion of the original file.</p> \n<p> </p> \n<p><img class=\"alignnone size-full wp-image-6181\" src=\"https://d2908q01vomqb2.cloudfront.net/b6692ea5df920cad691c20319a6fffd7a4a766b8/2018/12/19/AnalyzeCloudFront1.jpg\" alt=\"\" width=\"800\" height=\"497\"></p> \n<p> </p> \n<h3>Migration of your Amazon CloudFront Access Logs</h3> \n<p>The deployment of the sample application contains a single S3 bucket called <code>&lt;StackName&gt;-cf-access-logs.</code> You can modify your existing Amazon CloudFront distribution configuration to deliver access logs to this bucket with the <code>new/</code> log prefix. Files are moved to the canonical file structure for Amazon Athena partitioning as soon as they are put into the bucket.</p> \n<p>To migrate all previous access log files, copy them manually to the <code>new/</code> folder in the bucket. For example, you could copy the files by using the <a href=\"https://aws.amazon.com/cli/?nc2=h_m1\" target=\"_blank\" rel=\"noopener\">AWS Command Line Interface (AWS CLI)</a>. These files are treated the same way as the incoming files by Amazon CloudFront.</p> \n<h3>Load the Partitions and query your Access Logs</h3> \n<p>Before you can query the access logs in your bucket with Amazon Athena the AWS Glue Data Catalog needs metadata. On deployment, the sample application creates a table with the definition of the schema and the location. The new table is created by adding the partitioning information to the <code>CREATE TABLE</code> statement from the <a href=\"https://docs.aws.amazon.com/athena/latest/ug/cloudfront-logs.html#create-cloudfront-table\" target=\"_blank\" rel=\"noopener\">Amazon CloudFront documentation</a> (mind the <code>PARTITIONED BY</code> clause):</p> \n<div class=\"hide-language\"> \n <pre><code class=\"lang-sql\">CREATE EXTERNAL TABLE IF NOT EXISTS\n    cf_access_logs.partitioned_gz (\n         date DATE,\n         time STRING,\n         location STRING,\n         bytes BIGINT,\n         requestip STRING,\n         method STRING,\n         host STRING,\n         uri STRING,\n         status INT,\n         referrer STRING,\n         useragent STRING,\n         querystring STRING,\n         cookie STRING,\n         resulttype STRING,\n         requestid STRING,\n         hostheader STRING,\n         requestprotocol STRING,\n         requestbytes BIGINT,\n         timetaken FLOAT,\n         xforwardedfor STRING,\n         sslprotocol STRING,\n         sslcipher STRING,\n         responseresulttype STRING,\n         httpversion STRING,\n         filestatus STRING,\n         encryptedfields INT \n)\nPARTITIONED BY(\n         year string,\n         month string,\n         day string,\n         hour string )\nROW FORMAT DELIMITED FIELDS TERMINATED BY '\\t'\nLOCATION 's3://&lt;StackName&gt;-cf-access-logs/partitioned-gz/'\nTBLPROPERTIES ( 'skip.header.line.count'='2');</code></pre> \n</div> \n<p>You can load the partitions added so far by running the metastore check (<code>msck</code>) statement via the Amazon <a href=\"https://console.aws.amazon.com/athena/home#query\" target=\"_blank\" rel=\"noopener\">Athena query editor.</a> It discovers the partition structure in S3 and adds partitions to the metastore.</p> \n<div class=\"hide-language\"> \n <pre><code class=\"lang-sql\">msck repair table cf_access_logs.partitioned_gz</code></pre> \n</div> \n<p>You are now ready for your first query on your data in the Amazon Athena query editor:</p> \n<div class=\"hide-language\"> \n <pre><code class=\"lang-sql\">SELECT SUM(bytes) AS total_bytes\nFROM cf_access_logs.partitioned_gz\nWHERE year = '2017'\nAND month = '10'\nAND day = '01'\nAND hour BETWEEN '00' AND '11';</code></pre> \n</div> \n<p>This query does not specify the request date (called <code>date</code> in a previous example) column of the table but the columns used for partitioning. These columns are dependent on <code>date</code> but the table definition does not specify this relationship. When you specify only the request date column, Amazon Athena scans every file as there is no hint which files contain the relevant rows and which files do not. By specifying the partition columns, Amazon Athena scans only a small subset of the total amount of Amazon CloudFront access log files. This optimizes both the performance and the cost of your queries. You can add further columns to the <code>WHERE</code> clause, such as the <code>time</code> to further narrow down the results.</p> \n<p>To save cost, consider narrowing the scope of partitions down to a minimum by also putting the partitioning columns into the <code>WHERE</code> clause. You validate the approach by observing the amount of data that was scanned in the <a href=\"https://docs.aws.amazon.com/athena/latest/APIReference/API_QueryExecutionStatistics.html\" target=\"_blank\" rel=\"noopener\">query execution statistics</a> for your queries. These statistics are also displayed in the Amazon Athena query editor after your statement has been run:</p> \n<p><img class=\"alignnone size-full wp-image-6183\" src=\"https://d2908q01vomqb2.cloudfront.net/b6692ea5df920cad691c20319a6fffd7a4a766b8/2018/12/19/AnalyzeCloudFront2.jpg\" alt=\"\" width=\"800\" height=\"157\"></p> \n<h3>Adding Partitions continuously</h3> \n<p>As Amazon CloudFront continuously delivers new access log data for requests, new prefixes for partitions are created in S3. However, Amazon Athena only queries the files contained in the known partitions, i.e. partitions that have been added before to the metastore. That’s why periodically triggering the <code>msck</code> command would not be the best solution. First, it is a time-consuming operation since Amazon Athena scans all S3 paths to validate and load your partitions. More importantly, this way you only add partitions that already have data delivered. Thus, there is some time period when the data exists in S3 but is not visible to Amazon Athena queries yet.</p> \n<p>The sample application solves this by adding the partition for each hour in advance because partitions are just dependent on the request time. This way Amazon Athena scans files as soon as they exist in S3. A scheduled AWS Lambda function runs a statement like this:</p> \n<div class=\"hide-language\"> \n <pre><code class=\"lang-java\">ALTER TABLE cf_access_logs.partitioned_gz\nADD IF NOT EXISTS \nPARTITION (\n    year = '2017',\n    month = '10',\n    day = '01',\n    hour = '02' );</code></pre> \n</div> \n<p>It can omit the specification of the canonical <code>location</code> attribute in this statement as it is automatically derived from the column values.</p> \n<h2>Conversion of the Access Logs to a Columnar Format</h2> \n<p>As mentioned previously, with columnar formats Amazon Athena skips scanning of data not relevant for a query resulting in less cost. Amazon Athena currently supports the columnar formats Apache ORC and Apache Parquet.</p> \n<p>Key to the conversion is the Amazon Athena <code>CREATE TABLE AS SELECT</code> (<a href=\"https://docs.aws.amazon.com/athena/latest/ug/ctas.html\" target=\"_blank\" rel=\"noopener\">CTAS</a>) feature. A CTAS query creates a new table from the results of another <code>SELECT</code> query. Amazon Athena stores data files created by the CTAS statement in a specified location in Amazon S3. You can use CTAS to aggregate or transform the data, and to convert it into columnar formats. The sample application uses CTAS to hourly rewrite all logs from the CSV format to the Apache Parquet format. After this the resulting data will be added to a single partitioned table (the <em>target table</em>).</p> \n<h3>Creating the Target Table in Apache Parquet Format</h3> \n<p>The target table is a slightly modified version of the <code>partitioned_gz</code> table. Besides a different location the following table shows the different Serializer/Deserializer (SerDe) configuration for Apache Parquet:</p> \n<div class=\"hide-language\"> \n <pre><code class=\"lang-sql\">CREATE EXTERNAL TABLE `cf_access_logs.partitioned_parquet`(\n  `date` date, \n  `time` string, \n  `location` string, \n  `bytes` bigint, \n  `requestip` string, \n  `method` string, \n  `host` string, \n  `uri` string, \n  `status` int, \n  `referrer` string, \n  `useragent` string, \n  `querystring` string, \n  `cookie` string, \n  `resulttype` string, \n  `requestid` string, \n  `hostheader` string, \n  `requestprotocol` string, \n  `requestbytes` bigint, \n  `timetaken` float, \n  `xforwardedfor` string, \n  `sslprotocol` string, \n  `sslcipher` string, \n  `responseresulttype` string, \n  `httpversion` string, \n  `filestatus` string, \n  `encryptedfields` int)\nPARTITIONED BY ( \n  `year` string, \n  `month` string, \n  `day` string, \n  `hour` string)\nROW FORMAT SERDE \n  'org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe' \nSTORED AS INPUTFORMAT \n  'org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat' \nOUTPUTFORMAT \n  'org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat'\nLOCATION\n  's3://&lt;StackName&gt;-cf-access-logs/partitioned-parquet'\nTBLPROPERTIES (\n  'has_encrypted_data'='false', \n  'parquet.compression'='SNAPPY')</code></pre> \n</div> \n<h3>Transformation to Apache Parquet by the CTAS Query</h3> \n<p>The sample application provides a scheduled AWS Lambda function <code>transformPartition</code> that runs a CTAS query on a single partition per run, taking one hour of data into account. The target location for the Apache Parquet files is the Apache Hive style path in the location of the <code>partitioned_parquet</code> table.</p> \n<p> </p> \n<p><img class=\"alignnone size-full wp-image-6184\" src=\"https://d2908q01vomqb2.cloudfront.net/b6692ea5df920cad691c20319a6fffd7a4a766b8/2018/12/19/AnalyzeCloudFront3.jpg\" alt=\"\" width=\"800\" height=\"446\"></p> \n<p> </p> \n<p>The files written to S3 are important but the table in the AWS Glue Data Catalog for this data is just a by-product. Hence the function drops the CTAS table immediately and create the corresponding partition in the <code>partitioned_parquet</code> table instead.</p> \n<div class=\"hide-language\"> \n <pre><code class=\"lang-sql\">CREATE TABLE cf_access_logs.ctas_2017_10_01_02\nWITH ( format='PARQUET',\n    external_location='s3://&lt;StackName&gt;-cf-access-logs/partitioned_parquet/year=2017/month=10/day=01/hour=02',\n    parquet_compression = 'SNAPPY')\nAS SELECT *\nFROM cf_access_logs.partitioned_gz\nWHERE year = '2017'\n    AND month = '10'\n    AND day = '01'\n    AND hour = '02';\n\nDROP TABLE cf_access_logs.ctas_2017_10_01_02;\n\nALTER TABLE cf_access_logs.partitioned_parquet\nADD IF NOT EXISTS \nPARTITION (\n    year = '2017',\n    month = '10',\n    day = '01',\n    hour = '02' );</code></pre> \n</div> \n<p>The statement should be run as soon as new data is written. Amazon CloudFront usually delivers the log file for a time period to your Amazon S3 bucket within an hour of the events that appear in the log. The sample application schedules the <code>transformPartition</code> function hourly to transform the data for the hour before the previous hour.</p> \n<p>Some or all log file entries for a time period can sometimes be delayed by up to 24 hours. If you must mitigate this case, you delete and recreate a partition after that period. Also if you migrated partitions from previous Amazon CloudFront access logs, run the <code>transformPartition</code> function for each partition. The sample applications only transforms continuously added files.</p> \n<p>When all files of a gzip partition are converted to Apache Parquet, you can save cost by getting rid of data that you do not need. Use the <a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/object-lifecycle-mgmt.html\" target=\"_blank\" rel=\"noopener\">Lifecycle Policies</a> in S3 to archive the gzip files in a cheaper storage class or delete them after a specific amount of days.</p> \n<h3>Query data over Multiple Tables</h3> \n<p>You now have two derived tables from the original Amazon CloudFront access log data:</p> \n<ul>\n<li>\n<strong>partitioned_gz </strong>contains gzip compressed CSV files that are added as soon as new files are delivered.</li> \n <li>Access logs in <strong>partitioned_parquet </strong>are written after one hour latest. A rough assumption is that the CTAS query takes a maximum of 15 minutes to transform a gzip partition. You must measure and confirm this assumption. Depending on the data size, this can be much faster.</li> \n</ul>\n<p>The following diagram shows how the complete view on all data is composed of the two tables. The last complete partition of Apache Parquet files ends before the current time minus the transformation duration and the duration until Amazon CloudFront delivers the access log files.</p> \n<p><img class=\"alignnone size-full wp-image-6185\" src=\"https://d2908q01vomqb2.cloudfront.net/b6692ea5df920cad691c20319a6fffd7a4a766b8/2018/12/19/AnalyzeCloudFront4.jpg\" alt=\"\" width=\"800\" height=\"255\"></p> \n<p>For convenience the sample application creates the Amazon Athena view <code>combined</code> as a union of both tables. It includes an additional column called <code>file</code>. This is the file that stores the row.</p> \n<div class=\"hide-language\"> \n <pre><code class=\"lang-sql\">CREATE OR REPLACE VIEW cf_access_logs.combined AS\nSELECT *, \"$path\" AS file\nFROM cf_access_logs.partitioned_gz\nWHERE concat(year, month, day, hour) &gt;=\n       date_format(date_trunc('hour', (current_timestamp -\n       INTERVAL '15' MINUTE - INTERVAL '1' HOUR)), '%Y%m%d%H')\nUNION ALL SELECT *, \"$path\" AS file\nFROM cf_access_logs.partitioned_parquet\nWHERE concat(year, month, day, hour) &lt;\n       date_format(date_trunc('hour', (current_timestamp -\n       INTERVAL '15' MINUTE - INTERVAL '1' HOUR)), '%Y%m%d%H')</code></pre> \n</div> \n<p>Now you can query the data from the view to take advantage of the columnar based file partitions automatically. As mentioned before, you should add the partition columns (<code>year, month, day, hour</code>) to your statement to limit the files Amazon Athena scans.</p> \n<div class=\"hide-language\"> \n <pre><code class=\"lang-sql\">SELECT SUM(bytes) AS total_bytes\nFROM cf_access_logs.combined\nWHERE year = '2017'\n   AND month = '10'\n   AND day = '01'\n</code></pre> \n</div> \n<h2>Summary</h2> \n<p>In this blog post, you learned how to optimize the cost and performance of your Amazon Athena queries with two steps. First, you divide the overall data into small partitions. This allows queries to run much faster by reducing the number of files to scan. The second step converts each partition into a columnar format to reduce storage cost and increase the efficiency of scans by Amazon Athena.</p> \n<p>The results of both steps are combined in a single view for convenient interactive queries by you or your application. All data is partitioned by the time of the request. Thus, this format is best suited for interactive drill-downs into your logs for which the columns are limited and the time range is known. This way, it complements the <a href=\"https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/reports.html\" target=\"_blank\" rel=\"noopener\">Amazon CloudFront reports</a>, for example, by providing easy access to:</p> \n<ul>\n<li>Data from more than 60 days in the past</li> \n <li>The distribution of detailed HTTP status codes (for example, 200, 403, 404) on a certain day or hour</li> \n <li>Statistics based on the URI paths</li> \n <li>Statistics of objects that are not listed in Amazon CloudFront’s 50 most popular objects report</li> \n <li>A drill down into the attributes of each request</li> \n</ul>\n<p>We hope you find this blog post and the sample application useful also for other types of time series data beside Amazon CloudFront access logs. Feel free to submit enhancements to the example application in the source repository or provide feedback in the comments.</p> \n<p> </p> \n<hr>\n<h3>About the Author</h3> \n<p><img class=\"size-full wp-image-6190 alignleft\" src=\"https://d2908q01vomqb2.cloudfront.net/b6692ea5df920cad691c20319a6fffd7a4a766b8/2018/12/20/SteffenG.png\" alt=\"\" width=\"113\" height=\"151\"><strong>Steffen Grunwald is a senior solutions architect with Amazon Web Services. </strong>Supporting German enterprise customers on their journey to the cloud, he loves to dive deep into application architectures and development processes to drive performance, operational efficiency, and increase the speed of innovation.</p> \n<p> </p> \n<p> </p> \n<p> </p> \n<p> </p>\n",
      "enclosure": {},
      "categories": [
        "Amazon CloudFront",
        "Amazon Cloudfront"
      ]
    },
    {
      "title": "Reduce costs by migrating Apache Spark and Hadoop to Amazon EMR",
      "pubDate": "2018-12-20 21:05:18",
      "link": "https://aws.amazon.com/blogs/big-data/reduce-costs-by-migrating-apache-spark-and-hadoop-to-amazon-emr/",
      "guid": "3e175a125f9a62097fb9529d728c93b5c55fb262",
      "author": "Nikki Rouda",
      "thumbnail": "https://d2908q01vomqb2.cloudfront.net/b6692ea5df920cad691c20319a6fffd7a4a766b8/2018/12/20/MigrateSparkHadooptoEMR.png",
      "description": "Apache Spark and Hadoop are popular frameworks to process data for analytics, often at a fraction of the cost of legacy approaches, yet at scale they may still become expensive propositions. This blog post discusses ways to reduce your total costs of ownership, while also improving staff productivity at the same time. This can be […]",
      "content": "\n<p>Apache Spark and Hadoop are popular frameworks to process data for analytics, often at a fraction of the cost of legacy approaches, yet at scale they may still become expensive propositions. This blog post discusses ways to reduce your total costs of ownership, while also improving staff productivity at the same time. This can be accomplished by migrating your on-premises workloads to <a href=\"https://aws.amazon.com/emr/\" target=\"_blank\" rel=\"noopener\">Amazon EMR</a>, making good architectural choices, and taking advantage of features designed to reduce the resource consumption. The advice included has been learned from hundreds of engagements with customers and many points have been validated by the findings of a recently sponsored business value study conducted by IDC’s Carl Olofson and Harsh Singh in the IDC White Paper, sponsored by Amazon Web Services (AWS), <a href=\"https://d1.awsstatic.com/analyst-reports/IDC%20Economic%20Benefits%20of%20Migrating%20to%20EMR%20White%20Paper.pdf\" target=\"_blank\" rel=\"noopener\">“The Economic Benefits of Migrating Apache Spark and Hadoop to Amazon EMR”</a> (November 2018).</p> \n<p>Let’s begin with a few headline statistics to demonstrate the positive economic impact from migrating to Amazon EMR. IDC’s survey of nine Amazon EMR customers found an average 57 percent reduced cost of ownership. This was accompanied by a 342 percent five-year ROI, and eight months to break even on the investment. Those customers varied significantly in the size of their deployments for Spark and Hadoop, and accordingly in their results. However, these are compelling numbers for IT and finance leaders to consider as they set their long-term strategy for big data processing.</p> \n<p>Now, how exactly does Amazon EMR save you money over on-premises deployments of Spark and Hadoop? The IDC White Paper identified three common answers, which are detailed in this post.</p> \n<h2>Reducing physical infrastructure costs</h2> \n<p>An on-premises deployment inevitably has not fully used its hardware for several reasons. One is the provisioning cycle in which servers are selected, ordered, and installed in anticipation of future needs. Despite the efforts to estimate resource needs, this cycle usually brings a tendency to over-provision. In other words, investing capital in assets that aren’t used immediately. This cycle can also result in reaching the resource capacity limits — limiting the ability to complete needed big data processing in a timely fashion.</p> \n<p>This is further magnified by the Spark and Hadoop architecture because of the way distinct resources are provisioned in fixed ratios in servers. Each server has a given number of processors and memory (compute) and a set amount of storage capacity. Some workloads may be storage intensive, with vast quantities of data being retained on servers for possible future use cases. Meanwhile, the purchased compute resources sit idle until the day that data is processed and analyzed. Alternately, some workloads may require large quantities of memory or many processors for complex operations, but otherwise run against relatively small volumes of data. In these cases, the local storage may not be fully used.</p> \n<p>Achieving durability in HDFS on premises requires multiple copies of data, which increases the hardware requirement. This is already incorporated into <a href=\"https://aws.amazon.com/s3/?nc2=h_m1\" target=\"_blank\" rel=\"noopener\">Amazon S3</a>, so decoupling compute and storage also reduces the hardware footprint by removing the need to replicate for durability. Although Spark and Hadoop use commodity hardware, which is far more cost efficient than traditional data warehouse appliances, the on premises approach is inherently wasteful, rigid, and not agile enough to meet varying needs over time.</p> \n<p>AWS solutions architect Bruno Faria recently gave a talk at re:Invent 2018 about more sophisticated approaches to <a href=\"https://www.slideshare.net/AmazonWebServices/lower-costs-on-amazon-emr-auto-scaling-spot-pricing-expert-strategies-ant385-aws-reinvent-2018\" target=\"_blank\" rel=\"noopener\">“Lower Costs on Amazon EMR: Auto Scaling, Spot Pricing, and Expert Strategies”</a>. Let’s recap those points.</p> \n<p>Amazon EMR has several natural advantages over the challenges faced on premises. Customers pay only for the resources they actually consume, and only when they use them. On-demand resources lessen the over- and under-provisioning problem. More advanced customers can decouple their compute and storage resources too. They can store as much data as they need to in Amazon S3 and scale their only costs as data volumes grow, not in advance.</p> \n<p>Moreover, they can implement their choice of compute instances that are appropriately sized to the job at hand. They are also on-demand, and charged on a granular “per-second of usage” basis. This brings both more cost savings and more agility. Sometimes, customers “lift and shift” their on-premises workloads to AWS and run in <a href=\"https://aws.amazon.com/ec2/?nc2=h_m1\">Amazon EC2</a> without Amazon EMR. However, this doesn’t take advantage of the decoupling effect, and is recommended only as an intermediate step of migration.</p> \n<p>Once a customer is running their Spark and Hadoop in AWS, they can further reduce compute costs. They can choose from <em>reserved</em> instances, which means making a payment for an expected baseline at significant discounts. This is complemented by <em>On-demand</em> instances, meaning available at any time and paid per-second, and also <em><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-spot-instances.html\" target=\"_blank\" rel=\"noopener\">Spot instances</a></em>. Spot instances provide interruptible resources for less sensitive workloads at heavily reduced prices. <em>Instance fleets</em> also include the ability for you to specify:</p> \n<ul>\n<li>A defined duration (Spot block) to keep the Spot Instances running.</li> \n <li>The maximum Spot price that you’re willing to pay.</li> \n <li>A timeout period for provisioning Spot Instances.</li> \n</ul>\n<p>To let customers blend these purchasing options for the best results, you should understand the:</p> \n<ul>\n<li>Baseline demand, with predictable SLAs and needs.</li> \n <li>Periodic or unexpected peaks, to exceed SLA at reduced costs.</li> \n <li>Nature of the jobs, whether they are transient or long running.</li> \n</ul>\n<p><img class=\"alignnone size-full wp-image-6197\" src=\"https://d2908q01vomqb2.cloudfront.net/b6692ea5df920cad691c20319a6fffd7a4a766b8/2018/12/20/MigrateSparkHadooptoEMR.png\" alt=\"\" width=\"658\" height=\"315\"></p> \n<p>For more information about the instance purchasing options, see <a href=\"https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-instance-purchasing-options.html\" target=\"_blank\" rel=\"noopener\">Instance Purchasing Options</a>.</p> \n<p>The <a href=\"https://aws.amazon.com/blogs/aws/new-auto-scaling-for-emr-clusters/\">Auto Scaling feature</a> in EMR, available since 2016, can make this even more efficient to implement. It lets the service automatically manage user demand versus resources consumed as this ratio varies over time. For more information, see these <a href=\"https://aws.amazon.com/blogs/big-data/best-practices-for-resizing-and-automatic-scaling-in-amazon-emr/\" target=\"_blank\" rel=\"noopener\">best practices for automatic scaling</a>.</p> \n<p>Even storage costs can be reduced further. EMRFS, which decouples storage with Amazon S3 provides the ability to scale the clusters’ compute resources independent of storage capacity. In other words, EMRFS alone should already help to reduce costs. Another way to save on storage costs is to partition data to reduce the amount that needs to be scanned for processing and analytics. This is subject to the “Goldilocks” principle, where a customer wants partitions sized to avoid paying for reading unneeded data, but large enough to avoid excess overhead in finding the data needed.</p> \n<p>An example of <a href=\"https://aws.amazon.com/blogs/big-data/data-lake-ingestion-automatically-partition-hive-external-tables-with-aws/\" target=\"_blank\" rel=\"noopener\">automatic partitioning of Apache Hive external tables is here.</a> Optimizing file sizes can reduce Amazon S3 requests, and compressing data can minimize the bandwidth required to read data into compute instances. Not least, columnar formats for data storage are usually more efficient for big data processing and analytics than row-based layouts.</p> \n<p>Applying these recommended methods led to the 57 percent reduction in total cost of ownership, cited earlier in this post.</p> \n<p>Capturing these ideas nicely is a quote from the IDC White Paper where an Amazon EMR customer said the following:</p> \n<p>“<em>Amazon EMR gave us the best bang for the buck. One of the key factors is that our data is obviously growing. Running our big data operations on [Amazon] EMR increases confidence. It’s really good since we get cheap storage for huge amounts of data. The second thing is that the computation that we need fluctuates highly. Some of the data in our database is only occasionally used by our business or data analysts. We choose EMR because it is the most cost-effective solution as well as providing need-based computational expansion.” </em></p> \n<h2>Driving higher IT staff productivity</h2> \n<p>While infrastructure savings can be the most obvious driver for moving Apache Spark and Amazon EMR to the public cloud, improved IT staff productivity may have a significant benefit also. As Amazon EMR is a managed service, there is no need for a staff to spend time evaluating, purchasing, installing, provisioning, integrating, maintaining, or supporting their hardware infrastructure. Nor do they need to evaluate, install, patch, upgrade, or troubleshoot their software infrastructure, which in the rapidly innovating open-source software world can be a never-ending task.</p> \n<p>All that effort and associated “soft” costs go away, as the Amazon EMR environment is managed and kept current for customers. IT staff can instead focus their time on assisting data engineers, data scientists, and business analysts in their strategic endeavors, rather than doing infrastructure administration. The IDC White Paper linked these benefits to a 62 percent reduction in staff time cost per year vs. on premises, along with 54 percent reduction of staff costs for the big data environment managers. Respondents also said it helped them be more agile, improve quality, and develop quicker.</p> \n<p>Another customer interviewed by IDC summed this up by saying the following:</p> \n<p><em>“We went with Amazon EMR’s ready-made integration site. It is all about not having to spend time on integration…If we choose another Hadoop technology, then our researchers would have to make that work but if we run into a road block and it doesn’t work, we might learn that the hard way. In a way, we would be doing more testing, which would have meant we needed to hire three more people to do the integration work if we weren’t on Amazon EMR.”</em></p> \n<h2>Providing stronger big data environment availability</h2> \n<p>The third major area of savings cited was improved risk mitigation. Because AWS services are built upon many years of learned lessons in efficient and resilient operations, they deliver against promises of greater than 99.99% availability and durability, <a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/DataDurability.html\" target=\"_blank\" rel=\"noopener\">often with many more 9’s too</a>. Avoiding unplanned downtime was noted by the IDC study to bring a 99% reduction in lost productivity amongst IT and analytics staff.</p> \n<p>A customer noted, <em>“We have made systems much more resilient. It is really all about performance and resiliency.” </em></p> \n<p>There are many other economics benefits of migrating to Amazon EMR. They are often linked to improved staff productivity and delivering not only cost savings, but improved performance and a measurable ROI on analytics. But let’s not spoil the whole <a href=\"https://d1.awsstatic.com/analyst-reports/IDC%20Economic%20Benefits%20of%20Migrating%20to%20EMR%20White%20Paper.pdf\" target=\"_blank\" rel=\"noopener\">IDC White Paper</a>, you can read it for yourself!</p> \n<p> </p> \n<hr>\n<h3>About the Author</h3> \n<p><img class=\"size-full wp-image-6200 alignleft\" src=\"https://d2908q01vomqb2.cloudfront.net/b6692ea5df920cad691c20319a6fffd7a4a766b8/2018/12/20/nrouda1.png\" alt=\"\" width=\"113\" height=\"147\"><strong>Nikki Rouda is the principal product marketing manager for data lakes and big data at AWS</strong>. Nikki has spent 20+ years helping enterprises in 40+ countries develop and implement solutions to their analytics and IT infrastructure challenges. Nikki holds an MBA from the University of Cambridge and an ScB in geophysics and math from Brown University.</p> \n<p> </p> \n<p> </p> \n<p> </p> \n<p> </p> \n<p> </p>\n",
      "enclosure": {},
      "categories": [
        "Amazon EMR"
      ]
    },
    {
      "title": "Best Practices for Securing Amazon EMR",
      "pubDate": "2018-12-14 18:25:54",
      "link": "https://aws.amazon.com/blogs/big-data/best-practices-for-securing-amazon-emr/",
      "guid": "da1c5ebe3031f364e182050ad19adae6b0e4dbc2",
      "author": "Tony Nguyen",
      "thumbnail": "https://d2908q01vomqb2.cloudfront.net/b6692ea5df920cad691c20319a6fffd7a4a766b8/2018/12/13/BestPracticesEMR1.jpg",
      "description": "This post walks you through some of the principles of Amazon EMR security. It also describes features that you can use in Amazon EMR to help you meet the security and compliance objectives for your business. We cover some common security best practices that we see used. We also show some sample configurations to get you started.",
      "content": "\n<p>Whatever your industry, data is central to how organizations function. When we discuss data strategy with customers, it’s often in the context of how do they ingest, store, process, analyze, distribute, and ultimately secure data.</p> \n<p><a href=\"https://aws.amazon.com/emr/\" target=\"_blank\" rel=\"noopener\">Amazon EMR</a> is a managed Hadoop framework that you use to process vast amounts of data. One of the reasons that customers choose Amazon EMR is its security features. For example, customers like <a href=\"https://aws.amazon.com/solutions/case-studies/finra-data-validation/\" target=\"_blank\" rel=\"noopener\">FINRA</a> in regulated industries such as financial services, and in healthcare, choose Amazon EMR as part of their data strategy. They do so to adhere to strict regulatory requirements from entities such as the Payment Card Industry Data Security Standard (PCI) and the Health Insurance Portability and Accountability Act (HIPAA).</p> \n<p>This post walks you through some of the principles of Amazon EMR security. It also describes features that you can use in Amazon EMR to help you meet the security and compliance objectives for your business. We cover some common security best practices that we see used. We also show some sample configurations to get you started. For more in-depth information, see <a href=\"https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-security.html\" target=\"_blank\" rel=\"noopener\">Security</a> in the <em>EMR Management Guide.</em><span></span></p> \n<h2>Encryption, encryption, encryption</h2> \n<p>Our CTO, Werner Vogels, is fond of saying “Dance like nobody’s watching, encrypt like everybody is.” Properly protecting your data at rest and in transit using encryption is a core component of our <a href=\"https://d1.awsstatic.com/whitepapers/architecture/AWS-Security-Pillar.pdf\" target=\"_blank\" rel=\"noopener\">well-architected pillar of security</a>. Amazon EMR <a href=\"https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-create-security-configuration.html\" target=\"_blank\" rel=\"noopener\">security configurations</a> (described in the EMR documentation) make it easy for you to encrypt data. A <em>security configuration</em> is like a template for encryption and other security configurations that you can apply to any cluster when you launch it.</p> \n<h3>Encryption at rest</h3> \n<p>You have multiple options to <a href=\"https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-data-encryption-options.html#emr-encryption-s3\" target=\"_blank\" rel=\"noopener\">encrypt data at rest </a>in your EMR clusters. EMR by default uses the EMR file system (EMRFS) to read from and write data to <a href=\"https://aws.amazon.com/s3/\" target=\"_blank\" rel=\"noopener\">Amazon S3</a>. To encrypt data in Amazon S3, you can specify one of the following options:</p> \n<ul>\n<li>\n<strong>SSE-S3: </strong>Amazon S3 manages the encryption keys for you</li> \n <li>\n<strong>SSE-KMS: </strong>You use an <a href=\"https://aws.amazon.com/kms/\" target=\"_blank\" rel=\"noopener\">AWS Key Management Service</a> (AWS KMS) customer master key (CMK) to encrypt your data server-side on Amazon S3. Be sure to use policies that allow access by Amazon EMR.</li> \n <li>\n<strong>CSE-KMS/CSE-C: </strong>Amazon S3 encryption and decryption takes place client-side on your Amazon EMR cluster. You can use keys provided by AWS KMS (CSE-KMS) or use a custom Java class that provides the master key (CSE-C).</li> \n</ul>\n<p><img class=\"alignnone size-full wp-image-6161\" src=\"https://d2908q01vomqb2.cloudfront.net/b6692ea5df920cad691c20319a6fffd7a4a766b8/2018/12/13/BestPracticesEMR1.jpg\" alt=\"\" width=\"800\" height=\"393\"></p> \n<p>With Amazon EMR, setting up the encryption type for Amazon S3 is easy. You just select it from a list.</p> \n<p>Which option to choose depends on your specific workload requirements. With SSE-S3, the management of the keys is completely taken care of by Amazon. This is the simplest option. Using SSE-KMS or CSE-KMS enables you to have more control over the encryption keys and enables you to provide your own key material for encryption. The encryption is applied to objects within the bucket. The applications running on your cluster don’t need to be modified or made aware of the encryption in any way.</p> \n<p>For even more control, you can use a custom key provider with CSE-C. For more information, see <a href=\"https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-emrfs-encryption-cse.html\" target=\"_blank\" rel=\"noopener\">Amazon S3 Client-Side Encryption</a> in the EMR documentation.</p> \n<p>For local disk encryption, you have two approaches that complement each other. Let’s break them down by the specific encryption target:</p> \n<ul>\n<li>To encrypt your <em>root volume</em> with local disk encryption, create a custom Amazon Machine Image (AMI) for Amazon EMR and specify <a href=\"https://aws.amazon.com/ebs/\" target=\"_blank\" rel=\"noopener\">Amazon EBS</a> volume encryption. We cover details on custom AMIs a bit later in this post.</li> \n <li>To encrypt your <em>storage volumes</em> with local disk encryption, use Amazon EMR <a href=\"https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-data-encryption-options.html#emr-encryption-localdisk\" target=\"_blank\" rel=\"noopener\">security configurations</a>(described in the EMR documentation). Security configurations use a combination of open-source HDFS encryption and LUKS encryption.To use this feature, specify either the Amazon Resource Name (ARN) for an AWS KMS key or provide a custom Java class with the encryption artifacts.</li> \n</ul>\n<p>For more information about using KMS with EMR and S3, see <a href=\"https://docs.aws.amazon.com/kms/latest/developerguide/services-emr.html\" target=\"_blank\" rel=\"noopener\">How Amazon EMR Uses AWS KMS</a> in the EMR documentation.</p> \n<p>If you are using AWS KMS as part of your encryption strategy, see <a href=\"https://docs.aws.amazon.com/kms/latest/developerguide/limits.html\" target=\"_blank\" rel=\"noopener\">AWS KMS Limits</a> in the EMR documentation for information about the request rates supported for your use case.</p> \n<h3>Encryption in transit</h3> \n<p>Amazon EMR security configurations enable you to choose a method for <a href=\"https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-encryption-enable.html#emr-encryption-certificates\" target=\"_blank\" rel=\"noopener\">encrypting data in transit</a> using Transport Layer Security (TLS) (as described in the EMR documentation). You can do either of the following:</p> \n<ul>\n<li>Manually create PEM certificates, zip them in a file, and reference from Amazon S3.</li> \n <li>Implement a certificate custom provider in Java and specify the S3 path to the JAR.</li> \n</ul>\n<p>For more information on how these certificates are used with different big data technologies, check out <a href=\"https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-data-encryption-options.html#emr-encryption-intransit\" target=\"_blank\" rel=\"noopener\">In Transit Data Encryption with EMR</a>.</p> \n<p>Here’s what you’d see when creating a security configuration in the EMR Management Console.</p> \n<p><img class=\"alignnone size-full wp-image-6163\" src=\"https://d2908q01vomqb2.cloudfront.net/b6692ea5df920cad691c20319a6fffd7a4a766b8/2018/12/13/BestPracticesEMR2.jpg\" alt=\"\" width=\"800\" height=\"162\"></p> \n<h3>Expressing encryption as code</h3> \n<p>You can also specify encryption by using the AWS CLI, Boto3, or <a href=\"https://aws.amazon.com/cloudformation/\" target=\"_blank\" rel=\"noopener\">AWS CloudFormation</a>. Here’s an example with Boto3 that uses SSE-S3 for S3 encryption and KMS for local disk encryption:</p> \n<div class=\"hide-language\"> \n <pre><code class=\"lang-python\">import boto3\nclient = boto3.client('emr')\nresponse = client.create_security_configuration(\n    Name='MySecurityConfig'\n    SecurityConfiguration='''{\n        \"EncryptionConfiguration\": {\n            \"EnableInTransitEncryption\" : true,\n            \"EnableAtRestEncryption\" : true,\n            \"AtRestEncryptionConfiguration\" : {\n                \"S3EncryptionConfiguration\" : {\n                    \"EncryptionMode\" : \"SSE-S3\"\n                },\n                \"LocalDiskEncryptionConfiguration\" : {\n                    \"EncryptionKeyProviderType\" : \"AwsKms\",\n                    \"AwsKmsKey\" : \"arn:aws:kms:us-east-1:123456789012:key/12345678-1234-1234-1234-123456789012\"\n                }\n            }\n            \"InTransitEncryptionConfiguration\" : {\n                \"TLSCertificateConfiguration\" : {\n                    \"CertificateProviderType\" : \"PEM\",\n                    \"S3Object\" : \"s3://MyConfigStore/artifacts/MyCerts.zip\"\n                }\n            }\n        }\n    }'''\n)</code></pre> \n</div> \n<p>And the same snippet for AWS CloudFormation:</p> \n<div class=\"hide-language\"> \n <pre><code class=\"lang-yaml\">Resources:\n    MySecurityConfig:\n        Type: AWS::EMR::SecurityConfiguration\n        Properties:\n            Name: MySecurityConfig\n            SecurityConfiguration:\n                EncryptionConfiguration:\n                    EnableInTransitEncryption: true\n                    EnableAtRestEncryption: true\n                    AtRestEncryptionConfiguration:\n                        S3EncryptionConfiguration:\n                            EncryptionMode: SSE-S3\n                        LocalDiskEncryptionConfiguration:\n                            EncryptionKeyProviderType: AwsKms\n                            AwsKmsKey: arn:aws:kms:us-east-1:123456789012:key/12345678-1234-1234-1234-123456789012\n                    InTransitEncryptionConfiguration:\n                        TLSCertificateConfiguration:\n                            CertificateProviderType: PEM\n                            S3Object: arn:aws:s3:::MyConfigStore/artifacts/MyCerts.zip</code></pre> \n</div> \n<p>For more information about setting up security configurations in Amazon EMR, see the AWS Big Data Blog post <a href=\"https://aws.amazon.com/blogs/big-data/secure-amazon-emr-with-encryption/\">Secure Amazon EMR with Encryption</a>.</p> \n<h2>Authentication and authorization</h2> \n<p>Authentication and authorization, otherwise known as AuthN and AuthZ, are two crucial components that need to be considered when controlling access to data. <em>Authentication</em> is the verification of an entity, and <em>authorization</em> is checking whether the entity actually has access to the data or resources it’s asking for. In other words, authentication checks whether you’re really who you say you are. Authorization checks whether you actually have access to what you’re asking for. Alice can be authenticated as indeed being Alice, but this doesn’t necessarily mean that Alice has authorization, or access, to look at Bob’s bank account.</p> \n<h3>Authentication on Amazon EMR</h3> \n<p>Kerberos, a network authentication protocol created by the Massachusetts Institute of Technology (MIT), uses secret-key cryptography to provide strong authentication. It helps you avoid having sensitive information such as passwords or other credentials sent over the network in an unencrypted and exposed format.</p> \n<p>With Kerberos, you can maintain a set of services (known as a <em>realm</em>) and users that need to authenticate (known as <em>principals</em>). You provide a way for these principals to authenticate. You can also integrate your Kerberos setup with other realms. For example, you can have users authenticate from a Microsoft Active Directory domain and have a <em>cross-realm trust</em> set up. Such a setup can allow these authenticated users to be seamlessly authenticated to access your EMR clusters.</p> \n<p>Amazon EMR installs open-source Hadoop-based applications on your cluster, meaning that you can use the existing security features that use Kerberos in these products. For example, you can enable Kerberos authentication for YARN, giving user-level authentication for applications running on YARN such as Apache Spark.</p> \n<p>You can configure Kerberos on an EMR cluster (known as Kerberizing) to provide a means of authentication for cluster users. Before you configure Kerberos on Amazon EMR, we recommend that you become familiar with Kerberos concepts by reading <a href=\"https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-kerberos.html\" target=\"_blank\" rel=\"noopener\">Use Kerberos Authentication</a> in the EMR documentation.</p> \n<p>The following example shows Kerberizing with CloudFormation:</p> \n<div class=\"hide-language\"> \n <pre><code class=\"lang-yaml\">\"SecurityConfiguration\": {\n      \"Type\": \"AWS::EMR::SecurityConfiguration\",\n      \"Properties\": {\n        \"SecurityConfiguration\": {\n          \"AuthenticationConfiguration\": {\n            \"KerberosConfiguration\": {\n              \"ClusterDedicatedKdcConfiguration\": {\n                \"CrossRealmTrustConfiguration\": {\n                  \"Realm\": {\n                    \"Ref\": \"KerberosADdomain\"\n                  },\n                  \"KdcServer\": {\n                    \"Ref\": \"DomainDNSName\"\n                  },\n                  \"Domain\": {\n                    \"Ref\": \"DomainDNSName\"\n                  },\n                  \"AdminServer\": {\n                    \"Ref\": \"DomainDNSName\"\n                  }\n                },\n                \"TicketLifetimeInHours\": 24\n              },\n              \"Provider\": \"ClusterDedicatedKdc\"\n            }\n          }\n        }\n      }\n    }\n...</code></pre> \n</div> \n<h3>Authorization on Amazon EMR</h3> \n<p>Amazon EMR uses AWS Identity and Access Management (IAM) to help you manage access to your clusters. You can use IAM to create policies for principals such as users and groups that control actions that can be performed with Amazon EMR and other AWS resources.</p> \n<p>There are two roles associated with each cluster in Amazon EMR that typically are of interest—the <em>service role</em> and a role for the <a href=\"https://aws.amazon.com/ec2/\" target=\"_blank\" rel=\"noopener\">Amazon EC2</a> <em>instance profile</em>. You use the <em>service role, </em>or <em>EMR role,</em> for any action related to provisioning resources and other service-level tasks that aren’t performed in the context of an EC2 instance. The <em>instance profile</em> role is used by EC2 instances within the cluster. The policies associated with this role apply to processes that run on the cluster instances. For more information on these roles and others, see <a href=\"https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-iam-roles.html\" target=\"_blank\" rel=\"noopener\">Configure IAM Roles for Amazon EMR Permissions to AWS Services</a> in the EMR documentation.</p> \n<p>It’s important to understand how IAM helps you control authorized access to your cluster in relation to these roles and where it does not. IAM controls API-level actions done on other AWS services. IAM helps you with things like controlling access to objects in S3, protecting against cluster modification, and restricting access to keys from AWS KMS.</p> \n<p>This has important implications. By default, any process running in a cluster inherits the access permissions of the IAM role associated with the cluster. In contrast, IAM doesn’t control activity inside of your clusters. You need to use other means to appropriately secure the processes running on each EC2 instance.</p> \n<p>Because of these characteristics, formerly you used to face a particular Amazon EMR authorization challenge. This challenge was to understand how, by default, the <a href=\"http://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles.html\" target=\"_blank\" rel=\"noopener\">IAM role</a> attached to the EC2 instance profile role on your cluster determined the data that can be accessed in Amazon S3. What this effectively meant was that data access to S3 was granular only at the cluster level. This effect made it difficult to have multiple users with potentially different levels of access to data touching the same cluster.</p> \n<p>With Amazon EMR versions 5.10.0 and later, EMRFS fine-grained authorization was introduced. This fine-grained authorization enables the ability to specify the IAM role to assume at the user or group level when EMRFS is accessing Amazon S3. This authorization enables fine-grained access control for Amazon S3 on multitenant EMR clusters and also makes it easier to enable cross-account Amazon S3 access to data.</p> \n<p>The EMRFS authorization feature specifically applies to access by using HiveServer2. If your users are using Spark or other applications that allows for the execution of arbitrary code (for example, Jupyter, Zeppelin, SSH, spark-shell…), your users can bypass the roles that EMRFS has mapped to them.</p> \n<p>For more information on how to configure your security configurations and IAM roles appropriately, read <a href=\"https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-emrfs-iam-roles.html\" target=\"_blank\" rel=\"noopener\">Configure IAM Roles for EMRFS Requests to Amazon S3</a> in the EMR documentation.</p> \n<p>For a great in-depth guide on setting up a multitenant environment with both Kerberos authentication and EMRFS authorization, take a look at the following post on the AWS Big Data blog: <a href=\"https://aws.amazon.com/blogs/big-data/build-a-multi-tenant-amazon-emr-cluster-with-kerberos-microsoft-active-directory-integration-and-emrfs-authorization/\" target=\"_blank\" rel=\"noopener\">Build a Multi-Tenant Amazon EMR Cluster with Kerberos, Microsoft Active Directory Integration and IAM Roles for EMRFS</a>.</p> \n<h2>Network</h2> \n<p>Your network topology is also important when designing for security and privacy. We recommend placing your Amazon EMR clusters in private subnets, and use NAT to perform only outbound internet access.</p> \n<p>Security groups control inbound and outbound access from your individual instances. With Amazon EMR, you can use both <a href=\"https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-man-sec-groups.html\" target=\"_blank\" rel=\"noopener\">Amazon EMR-managed security groups</a> and also your <a href=\"https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-additional-sec-groups.html\" target=\"_blank\" rel=\"noopener\">own security groups</a> to control network access to your instance. By applying the principle of least privilege to your security groups, you can lock down your EMR cluster to only the applications or individuals who need access.</p> \n<p>For more information, see <a href=\"https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-man-sec-groups.html\" target=\"_blank\" rel=\"noopener\">Working With Amazon EMR-Managed Security Groups</a> in the EMR Documentation.</p> \n<p>The following example shows a security group with boto3:</p> \n<div class=\"hide-language\"> \n <pre><code class=\"lang-python\">import boto3\nfrom botocore.exceptions import ClientError\n\nec2 = boto3.client('ec2')\n\nresponse = ec2.describe_vpcs()\nvpc_id = response.get('Vpcs', [{}])[0].get('VpcId', '')\n\ntry:\n    response = ec2.create_security_group(GroupName='SECURITY_GROUP_NAME',\n                                         Description='Allow 80',\n                                         VpcId=vpc_id)\n    security_group_id = response['GroupId']\n\n    data = ec2.authorize_security_group_ingress(\n        GroupId=security_group_id,\n        IpPermissions=[\n            {'IpProtocol': 'tcp',\n             'FromPort': 22,\n             'ToPort': 22,\n             'IpRanges': [{'CidrIp': '0.0.0.0/0'}]}\n        ])\nexcept ClientError as e:\n    print(e)</code></pre> \n</div> \n<p>The following example shows a security group with CloudFormation:</p> \n<div class=\"hide-language\"> \n <pre><code class=\"lang-yaml\">InstanceSecurityGroup:\n  Type: AWS::EC2::SecurityGroup\n  Properties:\n    GroupDescription: Allow 80\n    VpcId:\n      Ref: myVPC\n    SecurityGroupIngress:\n    - IpProtocol: tcp\n      FromPort: '80'\n      ToPort: '80'\n      CidrIp: 0.0.0.0/0\n    SecurityGroupEgress:\n    - IpProtocol: tcp\n      FromPort: '80'\n      ToPort: '80'\n      CidrIp: 0.0.0.0/0\n ...\n</code></pre> \n</div> \n<h2>Minimal IAM policy</h2> \n<p>By default, the IAM policies that are associated with EMR are generally permissive, to enable you to easily integrate EMR with other AWS services. When securing EMR, a best practice is to start from the minimal set of permissions required for EMR to function and add permissions as necessary.</p> \n<p>Following are three policies that are scoped around what EMR minimally requires for basic operation. You can potentially further minimize these policies by removing actions related to spot pricing and autoscaling. For clarity, the policies are annotated with comments—remove the comments before use.</p> \n<p><strong>Minimal EMR service-role policy</strong></p> \n<div class=\"hide-language\"> \n <pre><code class=\"lang-json\">{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Resource\": \"*\",\n            \"Action\": [\n                \"ec2:AuthorizeSecurityGroupEgress\",\n                \"ec2:AuthorizeSecurityGroupIngress\",\n                \"ec2:CancelSpotInstanceRequests\",\n                \"ec2:CreateNetworkInterface\",\n                \"ec2:CreateSecurityGroup\",\n                \"ec2:CreateTags\",\n                \"ec2:DeleteNetworkInterface\", // This is only needed if you are launching clusters in a private subnet. \n                \"ec2:DeleteTags\",\n                \"ec2:DeleteSecurityGroup\", // This is only needed if you are using Amazon managed security groups for private subnets. You can omit this action if you are using custom security groups.\n                \"ec2:DescribeAvailabilityZones\",\n                \"ec2:DescribeAccountAttributes\",\n                \"ec2:DescribeDhcpOptions\",\n                \"ec2:DescribeImages\",\n                \"ec2:DescribeInstanceStatus\",\n                \"ec2:DescribeInstances\",\n                \"ec2:DescribeKeyPairs\",\n                \"ec2:DescribeNetworkAcls\",\n                \"ec2:DescribeNetworkInterfaces\",\n                \"ec2:DescribePrefixLists\",\n                \"ec2:DescribeRouteTables\",\n                \"ec2:DescribeSecurityGroups\",\n                \"ec2:DescribeSpotInstanceRequests\",\n                \"ec2:DescribeSpotPriceHistory\",\n                \"ec2:DescribeSubnets\",\n                \"ec2:DescribeTags\",\n                \"ec2:DescribeVpcAttribute\",\n                \"ec2:DescribeVpcEndpoints\",\n                \"ec2:DescribeVpcEndpointServices\",\n                \"ec2:DescribeVpcs\",\n                \"ec2:DetachNetworkInterface\",\n                \"ec2:ModifyImageAttribute\",\n                \"ec2:ModifyInstanceAttribute\",\n                \"ec2:RequestSpotInstances\",\n                \"ec2:RevokeSecurityGroupEgress\",\n                \"ec2:RunInstances\",\n                \"ec2:TerminateInstances\",\n                \"ec2:DeleteVolume\",\n                \"ec2:DescribeVolumeStatus\",\n                \"ec2:DescribeVolumes\",\n                \"ec2:DetachVolume\",\n                \"iam:GetRole\",\n                \"iam:GetRolePolicy\",\n                \"iam:ListInstanceProfiles\",\n                \"iam:ListRolePolicies\",\n                \"s3:CreateBucket\",\n                \"sdb:BatchPutAttributes\",\n                \"sdb:Select\",\n                \"cloudwatch:PutMetricAlarm\",\n                \"cloudwatch:DescribeAlarms\",\n                \"cloudwatch:DeleteAlarms\",\n                \"application-autoscaling:RegisterScalableTarget\",\n                \"application-autoscaling:DeregisterScalableTarget\",\n                \"application-autoscaling:PutScalingPolicy\",\n                \"application-autoscaling:DeleteScalingPolicy\",\n                \"application-autoscaling:Describe*\"\n            ]\n        },\n        {\n            \"Effect\": \"Allow\",\n            \"Resource\": [\"arn:aws:s3:::examplebucket/*\",\"arn:aws:s3:::examplebucket2/*\"], // Here you can specify the list of buckets which are going to be storing cluster logs, bootstrap action script, custom JAR files, input &amp; output paths for EMR steps\n            \"Action\": [\n                \"s3:GetBucketLocation\",\n                \"s3:GetBucketCORS\",\n                \"s3:GetObjectVersionForReplication\",\n                \"s3:GetObject\",\n                \"s3:GetBucketTagging\",\n                \"s3:GetObjectVersion\",\n                \"s3:GetObjectTagging\",\n                \"s3:ListMultipartUploadParts\",\n                \"s3:ListBucketByTags\",\n                \"s3:ListBucket\",\n                \"s3:ListObjects\",\n                \"s3:ListBucketMultipartUploads\"\n            ]\n        },\n        {\n            \"Effect\": \"Allow\",\n            \"Resource\": \"arn:aws:sqs:*:123456789012:AWS-ElasticMapReduce-*\", // This allows EMR to only perform actions (Creating queue, receiving messages, deleting queue, etc) on SQS queues whose names are prefixed with the literal string AWS-ElasticMapReduce-\n            \"Action\": [\n                \"sqs:CreateQueue\",\n                \"sqs:DeleteQueue\",\n                \"sqs:DeleteMessage\",\n                \"sqs:DeleteMessageBatch\",\n                \"sqs:GetQueueAttributes\",\n                \"sqs:GetQueueUrl\",\n                \"sqs:PurgeQueue\",\n                \"sqs:ReceiveMessage\"\n            ]\n        },\n        {\n            \"Effect\": \"Allow\",  \n            \"Action\": \"iam:CreateServiceLinkedRole\",  // EMR needs permissions to create this service-linked role for launching EC2 spot instances\n            \"Resource\": \"arn:aws:iam::*:role/aws-service-role/spot.amazonaws.com/AWSServiceRoleForEC2Spot*\",\n            \"Condition\": {\n                \"StringLike\": {\n                    \"iam:AWSServiceName\": \"spot.amazonaws.com\"\n                }\n            }\n        },\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": \"iam:PassRole\", // We are passing the custom EC2 instance profile (defined following) which has bare minimum permissions\n            \"Resource\": [\n                \"arn:aws:iam::*:role/Custom_EMR_EC2_role\",\n                \"arn:aws:iam::*:role/EMR_AutoScaling_DefaultRole\"\n            ]\n        }\n    ]\n}</code></pre> \n</div> \n<p><strong>Minimal EMR role for EC2 (instance profile) policy</strong></p> \n<div class=\"hide-language\"> \n <pre><code class=\"lang-json\">{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Resource\": \"*\",\n            \"Action\": [\n                \"ec2:Describe*\",\n                \"elasticmapreduce:Describe*\",\n                \"elasticmapreduce:ListBootstrapActions\",\n                \"elasticmapreduce:ListClusters\",\n                \"elasticmapreduce:ListInstanceGroups\",\n                \"elasticmapreduce:ListInstances\",\n                \"elasticmapreduce:ListSteps\"\n            ]\n        },\n        {\n            \"Effect\": \"Allow\",\n            \"Resource\": [    // Here you can specify the list of buckets which are going to be accessed by applications (Spark, Hive, etc) running on the nodes of the cluster\n                \"arn:aws:s3:::examplebucket1/*\",\n                \"arn:aws:s3:::examplebucket1*\",\n                \"arn:aws:s3:::examplebucket2/*\",\n                \"arn:aws:s3:::examplebucket2*\"\n            ], \n            \"Action\": [\n                \"s3:GetBucketLocation\",\n                \"s3:GetBucketCORS\",\n                \"s3:GetObjectVersionForReplication\",\n                \"s3:GetObject\",\n                \"s3:GetBucketTagging\",\n                \"s3:GetObjectVersion\",\n                \"s3:GetObjectTagging\",\n                \"s3:ListMultipartUploadParts\",\n                \"s3:ListBucketByTags\",\n                \"s3:ListBucket\",\n                \"s3:ListObjects\",\n                \"s3:ListBucketMultipartUploads\",\n                \"s3:PutObject\",\n                \"s3:PutObjectTagging\",\n                \"s3:HeadBucket\",\n                \"s3:DeleteObject\"\n            ]\n        },\n        {\n            \"Effect\": \"Allow\",\n            \"Resource\": \"arn:aws:sqs:*:123456789012:AWS-ElasticMapReduce-*\", // This allows EMR to only perform actions (creating queue, receiving messages, deleting queue, and so on) on SQS queues whose names are prefixed with the literal string AWS-ElasticMapReduce-\n            \"Action\": [\n                \"sqs:CreateQueue\",\n                \"sqs:DeleteQueue\",\n                \"sqs:DeleteMessage\",\n                \"sqs:DeleteMessageBatch\",\n                \"sqs:GetQueueAttributes\",\n                \"sqs:GetQueueUrl\",\n                \"sqs:PurgeQueue\",\n                \"sqs:ReceiveMessage\"\n            ]\n        }\n    ]\n}</code></pre> \n</div> \n<p><strong>Minimal role policy for user launching EMR clusters</strong></p> \n<div class=\"hide-language\"> \n <pre><code class=\"lang-json\">// This policy can be attached to an IAM user who will be launching EMR clusters. It provides minimum access to the user to launch, monitor and terminate EMR clusters\n\n{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Sid\": \"Statement1\",\n            \"Effect\": \"Allow\",\n            \"Action\": \"iam:CreateServiceLinkedRole\",\n            \"Resource\": \"*\",\n            \"Condition\": {\n                \"StringLike\": {\n                    \"iam:AWSServiceName\": [\n                        \"elasticmapreduce.amazonaws.com\",\n                        \"elasticmapreduce.amazonaws.com.cn\"\n                    ]\n                }\n            }\n        },\n        {\n            \"Sid\": \"Statement2\",\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"iam:GetPolicyVersion\",\n                \"ec2:AuthorizeSecurityGroupIngress\",\n                \"ec2:DescribeInstances\",\n                \"ec2:RequestSpotInstances\",\n                \"ec2:DeleteTags\",\n                \"ec2:DescribeSpotInstanceRequests\",\n                \"ec2:ModifyImageAttribute\",\n                \"cloudwatch:GetMetricData\",\n                \"cloudwatch:GetMetricStatistics\",\n                \"cloudwatch:ListMetrics\",\n                \"ec2:DescribeVpcAttribute\",\n                \"ec2:DescribeSpotPriceHistory\",\n                \"ec2:DescribeAvailabilityZones\",\n                \"ec2:CreateRoute\",\n                \"ec2:RevokeSecurityGroupEgress\",\n                \"ec2:CreateSecurityGroup\",\n                \"ec2:DescribeAccountAttributes\",\n                \"ec2:ModifyInstanceAttribute\",\n                \"ec2:DescribeKeyPairs\",\n                \"ec2:DescribeNetworkAcls\",\n                \"ec2:DescribeRouteTables\",\n                \"ec2:AuthorizeSecurityGroupEgress\",\n                \"ec2:TerminateInstances\", //This action can be scoped in similar manner like it has been done following for \"elasticmapreduce:TerminateJobFlows\"\n                \"iam:GetPolicy\",\n                \"ec2:CreateTags\",\n                \"ec2:DeleteRoute\",\n                \"iam:ListRoles\",\n                \"ec2:RunInstances\",\n                \"ec2:DescribeSecurityGroups\",\n                \"ec2:CancelSpotInstanceRequests\",\n                \"ec2:CreateVpcEndpoint\",\n                \"ec2:DescribeVpcs\",\n                \"ec2:DescribeSubnets\",\n                \"elasticmapreduce:*\"\n            ],\n            \"Resource\": \"*\"\n        },\n        {\n            \"Sid\": \"Statement3\",\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"elasticmapreduce:TerminateJobFlows\"\n            ],\n            \"Resource\":\"*\",\n            \"Condition\": {\n                \"StringEquals\": {\n                  \"elasticmapreduce:ResourceTag/custom_key\": \"custom_value\"  // Here you can specify the key value pair of your custom tag so that this IAM user can only delete the clusters which are appropriately tagged by the user\n                }\n              }\n        },\n        {\n            \"Sid\": \"Statement4\",\n            \"Effect\": \"Allow\",\n            \"Action\": \"iam:PassRole\",\n            \"Resource\": [\n                \"arn:aws:iam::*:role/Custom_EMR_Role\",\n                \"arn:aws:iam::*:role/Custom_EMR_EC2_role\",\n                \"arn:aws:iam::*:role/EMR_AutoScaling_DefaultRole\"\n            ]\n        }\n    ]\n}</code></pre> \n</div> \n<h2>Bootstrap actions</h2> \n<p>Bootstrap actions are commonly used to run custom code for setup before the execution of your cluster. You can use them to install software or configure instances in any language already installed on the cluster, including Bash, Perl, Python, Ruby, C++, or Java.</p> \n<p>For more details on how to use bootstrap actions, see <a href=\"https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-plan-bootstrap.html\" target=\"_blank\" rel=\"noopener\">Create Bootstrap Actions to Install Additional Software</a> in the EMR documentation.</p> \n<p>For the purposes of this blog post, we’ll discuss bootstrap actions to harden your cluster, apply security packages you might require, and set up third-party monitoring solutions.</p> \n<h3>When do bootstrap actions run in the cluster lifecycle?</h3> \n<p>Bootstrap actions run on each cluster instance after instance provisioning and before application installation. This approach has implications if you want to use bootstrap actions to modify the security configuration of applications that are provisioned by EMR. If this is the case, you can have your script wait on a particular trigger, like so with a bootstrap action dependent on Presto installation:</p> \n<div class=\"hide-language\"> \n <pre><code class=\"lang-bash\"># Work to do before Presto being installed executes before this line\nwhile [ ! -f /var/run/presto/presto-presto-server.pid ]\ndo\n  sleep 1\ndone\n\n# Continue on with script execution\n</code></pre> \n</div> \n<p>Be sure to run the preceding snippet in a background process so that it doesn’t block the provisioning process. Failing to do so results in a timeout failure.</p> \n<p>For more information about the cluster lifecycle, refer to: <a href=\"https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-overview.html#emr-overview-cluster-lifecycle\" target=\"_blank\" rel=\"noopener\">Understanding the Cluster Lifecycle</a> in the EMR documentation.</p> \n<h2>Custom AMIs and applying CIS controls to harden your AMI</h2> \n<p>Custom Amazon Machine Images (AMIs) provide another approach that you can take to help harden and secure your EMR cluster. Amazon EMR uses an Amazon Linux AMI to initialize Amazon EC2 instances when you create and launch a cluster. The AMI contains the Amazon Linux operating system, other software, and configurations required for each instance to host your cluster applications.</p> \n<p>Specifying a custom AMI is useful for the following use cases:</p> \n<ul>\n<li>Encrypting the EBS root device volumes (boot volumes) of EC2 instances in your cluster, which you can’t do with a security configuration. Security configurations only help you encrypt your storage volumes. For more information, see <a href=\"https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-custom-ami.html#emr-custom-ami-encrypted\" target=\"_blank\" rel=\"noopener\">Creating a Custom AMI with an Encrypted Amazon EBS Root Device Volume</a> in the EMR documentation.</li> \n <li>Pre-installing applications and performing other customizations instead of using bootstrap actions, which can improve cluster start time and streamline the startup work flow. For more information and an example, see <a href=\"https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-custom-ami.html#emr-custom-ami-preconfigure\" target=\"_blank\" rel=\"noopener\">Creating a Custom Amazon Linux AMI from a Preconfigured Instance</a> in the EMR documentation.</li> \n <li>Implementing more sophisticated cluster and node configurations than bootstrap actions enable.</li> \n</ul>\n<p>By using a custom AMI instead of a bootstrap action, you can have your hardening steps prebaked into the images you use, rather than having to run the bootstrap action scripts at instance provision time. You don’t have to choose between the two, either. You can create a custom AMI for the common security characteristics of your cluster that are less likely to change. You can use bootstrap actions to pull the latest configurations and scripts that might be cluster-specific.</p> \n<p>One approach that many of our customers take is to apply the Center for Internet Security (CIS) benchmarks to harden their EMR clusters, found on the <a href=\"https://www.cisecurity.org/benchmark/amazon_linux/\" target=\"_blank\" rel=\"noopener\">Center for Internet Security website</a>. It’s important to verify each and every control for necessity and function test against your requirements when applying these benchmarks to your clusters.</p> \n<p>The following example shows using custom AMIs in CloudFormation:</p> \n<div class=\"hide-language\"> \n <pre><code class=\"lang-yaml\">Resources:\n  cluster:\n    Type: 'AWS::EMR::Cluster'\n    Properties:\n      CustomAmiId: \"ami-7fb3bc69\"\n      Instances:\n        MasterInstanceGroup:\n...</code></pre> \n</div> \n<p>The following example shows using custom AMIs in boto3:</p> \n<div class=\"hide-language\"> \n <pre><code class=\"lang-python\">response = client.run_job_flow(\n    Name='string',\n    LogUri='string',\n    AdditionalInfo='string',\n    CustomAmiId='ami-7fb3bc69',\n...</code></pre> \n</div> \n<h2>Auditing</h2> \n<p>You might want the ability to audit compute environments, which is a key requirement for many customers. There are a variety of ways that you can support this requirement within EMR:</p> \n<ul>\n<li>From EMR 5.14.0 onwards, EMRFS, Amazon EMR’s connector for S3, supports auditing of users who ran queries that accessed data in S3 through EMRFS. This feature is turned on by default and passes on user and group information to audit logs like CloudTrail. It provides you with comprehensive request tracking.</li> \n <li>If it exists, you can configure and implement application-specific auditing on EMR. For example, this AWS Big Data Blog post walks through how to configure a custom event listener on Presto to enable audit logging, debugging, and performance analysis: <a href=\"https://aws.amazon.com/blogs/big-data/custom-log-presto-query-events-on-amazon-emr-for-auditing-and-performance-insights/\" target=\"_blank\" rel=\"noopener\">Custom Log Presto Query Events on Amazon EMR for Auditing and Performance Insights</a>.</li> \n <li>You can use tools such as Apache Ranger to implement another layer of auditing and authorization. For additional information, see this AWS Big Data Blog post: <a href=\"https://aws.amazon.com/blogs/big-data/implementing-authorization-and-auditing-using-apache-ranger-on-amazon-emr/\" target=\"_blank\" rel=\"noopener\">Implementing Authorization and Auditing using Apache Ranger on Amazon EMR</a>\n</li> \n <li>AWS CloudTrail, a service that provides a record of actions taken by a user, role, or an AWS service, is integrated with Amazon EMR. CloudTrail captures all API calls for Amazon EMR as events. The calls captured include calls from the Amazon EMR console and code calls to the Amazon EMR API operations. If you create a trail, you can enable continuous delivery of CloudTrail events to an Amazon S3 bucket, including events for Amazon EMR.</li> \n <li>You can also audit the S3 objects that EMR accesses by using S3 access logs. AWS CloudTrail provides logs only for AWS API calls. Thus, if a user runs a job that reads and writes data to S3, the S3 data that was accessed by EMR doesn’t show up in CloudTrail. By using S3 access logs, you can comprehensively monitor and audit access against your data in S3 from anywhere, including EMR.</li> \n <li>Because you have full control over your EMR cluster, you can always install your own third-party agents or tooling. You do so by using bootstrap actions or custom AMIs to help support your auditing requirements.</li> \n</ul>\n<h2>Verifying your security configuration</h2> \n<p>You also want to verify that your configuration works, of course. Following are some steps that you can take to verify your configuration.</p> \n<p><strong>S3 server-side encryption (SSE) on EMR</strong></p> \n<p>Here, we want to verify a particular case. We want to ensure that if an S3 object is uploaded using EMRFS and server-side encryption (SSE) is enabled in the EMRFS configuration, it has metadata indicating it’s encrypted. For example, calling getSSEAlgorithm on the S3 object should return AES256 if the object is encrypted using an S3 key. It should return aws:kms if the object is encrypted using a KMS key.</p> \n<p>We also want to check that if the file is downloaded using EMRFS, the original file and the downloaded file match in terms of content.</p> \n<p>To verify this, do the following:</p> \n<ol>\n<li>Use Secure Shell (SSH) to connect to the master node as described in <a href=\"https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-connect-master-node-ssh.html\" target=\"_blank\" rel=\"noopener\">Connect to the Master Node Using SSH</a> in the EMR documentation.</li> \n <li>Upload an object to S3 using EMRFS: \n  <ul>\n<li><span><tt>hadoop fs -put &lt;local path&gt; &lt;s3 bucket path&gt;</tt></span></li> \n  </ul>\n</li> \n <li>Check the metadata of the uploaded object directly: \n  <ul>\n<li><span><tt>aws s3api head-object --bucket &lt;s3 bucket path&gt; --key file</tt></span></li> \n  </ul>\n</li> \n <li>Download the file from S3 using EMRFS: \n  <ul>\n<li><span><tt>hadoop fs -get &lt;s3 bucket path&gt; &lt;local path&gt;</tt></span></li> \n  </ul>\n</li> \n <li>Use diff on the original and downloaded file and verify that they are the same.</li> \n</ol>\n<h3>S3 client-side encryption (CSE) on EMR</h3> \n<p>Similarly to verifying server-side encryption, we want to confirm another point. If an object is uploaded using EMRFS and client side encryption is enabled in the EMRFS configuration, the S3 object’s contents should be encrypted. A client that doesn’t possess the proper key shouldn’t be able to see them. If we get the object using an Amazon S3 client, the contents of the object shouldn’t be the same as the contents of the original object that was uploaded.</p> \n<p>We also want to confirm that if we do use EMRFS to download the file, the contents of the downloaded file should match the contents of the original file.</p> \n<p>To verify this, do the following:</p> \n<ol>\n<li>Use SSH to connect to the master node as described in <a href=\"https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-connect-master-node-ssh.html\" target=\"_blank\" rel=\"noopener\">Connect to the Master Node Using SSH</a>.</li> \n <li>Upload an object to S3 using EMRFS: \n  <ul>\n<li><span><tt>hadoop fs -put &lt;local path&gt; &lt;s3 bucket path&gt;</tt></span></li> \n  </ul>\n</li> \n <li>Download the object from S3 using the AWS CLI: \n  <ul>\n<li><span><tt>aws s3 mv &lt;s3 bucket path&gt; &lt;local path&gt;</tt></span></li> \n   <li>This file should NOT match the original file.</li> \n  </ul>\n</li> \n <li>Download the object from S3 using EMRFS: \n  <ul>\n<li><span><tt>hadoop fs -get &lt;s3 bucket path&gt; &lt;local path&gt;</tt></span></li> \n   <li>This file SHOULD match the original file.</li> \n  </ul>\n</li> \n</ol>\n<h3>Local disk encryption</h3> \n<p>We also want to verify that if local disk encryption is enabled, all the block devices on the cluster are of type crypto_LUKS. If there is a partition that is not a LUKS partition, the following command exits with a return code of 1:</p> \n<p><span><tt>! blkid | grep -v crypto_LUKS</tt></span></p> \n<p>In addition, we want to verify that the local disk encryption uses the kind of key that was defined in the configuration.</p> \n<p>Let’s try to use KMS to decrypt our encrypted local disk passphrase. If it is decrypted, we try to use it to open the LUKS partitions. If it doesn’t get decrypted, we assume it is a custom key.</p> \n<p>To do so, we run the following commands:</p> \n<ol>\n<li><span><tt>base64 -d /var/setup-devices/.encrypted-diskKey &gt; (local path for encrypted passphrase)</tt></span></li> \n <li>\n<span><tt>aws kms decrypt --ciphertext-blob fileb://(local path for encrypted passphrase) --query Plaintext &gt; (local path for decrypted passphrase)</tt></span> \n  <ul>\n<li>If this step fails, the key is not from KMS</li> \n  </ul>\n</li> \n <li><span><tt>sudo cryptsetup luksOpen --test-passphrase --key-slot 0 /dev/xvdb1 &lt; (local path for decrypted passphrase) | cut -d '\"' -f 2</tt></span></li> \n</ol>\n<h2>Wrapping it up in AWS CloudFormation</h2> \n<p>We have provided the following AWS CloudFormation template to help you more easily deploy an EMR cluster that aligns to the patterns and practices we describe in this post. The following template spins up an EMR cluster with the following characteristics:</p> \n<ul>\n<li>Encryption at rest enabled for both S3 and the local disk</li> \n <li>Encryption in transit enabled using a certificate bundle that you specify</li> \n <li>A custom AMI CloudFormation parameter that can incorporate the concepts described in the custom AMI section</li> \n <li>Kerberos-enabled</li> \n <li>Fine-grained S3 authorization</li> \n</ul>\n<div class=\"hide-language\"> \n <pre><code class=\"lang-yaml\">---\nDescription: Sample CloudFormation template for creating an EMR cluster\nParameters:\n  KeyName:\n    Description: Name of an existing EC2 KeyPair to enable SSH to the instances\n    Type: AWS::EC2::KeyPair::KeyName\n  Subnet:\n    Description: Subnet ID for creating the EMR cluster\n    Type: AWS::EC2::Subnet::Id\nResources:\n  EMRInstanceProfile:\n    Properties:\n      Roles:\n      - Ref: EMRJobFlowRole\n    Type: AWS::IAM::InstanceProfile\n  EMRJobFlowRole:\n    Properties:\n      AssumeRolePolicyDocument:\n        Statement:\n        - Action:\n          - sts:AssumeRole\n          Effect: Allow\n          Principal:\n            Service:\n            - ec2.amazonaws.com\n      ManagedPolicyArns:\n      - arn:aws:iam::aws:policy/service-role/AmazonElasticMapReduceforEC2Role\n    Type: AWS::IAM::Role\n  EMRSampleCluster:\n    Properties:\n      Applications:\n      - Name: Hadoop\n      - Name: Hive\n      - Name: Spark\n      AutoScalingRole:\n        Ref: EMR_AutoScaling_DefaultRole\n      BootstrapActions:\n      - Name: Dummy bootstrap action\n        ScriptBootstrapAction:\n          Args:\n          - dummy\n          - parameter\n          Path: file:/usr/share/aws/emr/scripts/install-hue\n      Configurations:\n      - Classification: core-site\n        ConfigurationProperties:\n          hadoop.security.groups.cache.secs: '250'\n      - Classification: mapred-site\n        ConfigurationProperties:\n          mapred.tasktracker.map.tasks.maximum: '2'\n          mapreduce.map.sort.spill.percent: '90'\n          mapreduce.tasktracker.reduce.tasks.maximum: '5'\n      Instances:\n        CoreInstanceGroup:\n          EbsConfiguration:\n            EbsBlockDeviceConfigs:\n            - VolumeSpecification:\n                SizeInGB: '10'\n                VolumeType: gp2\n              VolumesPerInstance: '1'\n            EbsOptimized: 'true'\n          InstanceCount: '1'\n          InstanceType: m4.large\n          Name: Core Instance\n        Ec2KeyName:\n          Ref: KeyName\n        Ec2SubnetId:\n          Ref: Subnet\n        MasterInstanceGroup:\n          InstanceCount: '1'\n          InstanceType: m4.large\n          Name: Master Instance\n      JobFlowRole:\n        Ref: EMRInstanceProfile\n      Name: EMR Sample Cluster\n      ReleaseLabel: emr-5.5.0\n      SecurityConfiguration:\n        Ref: EMRSecurityConfiguration\n      ServiceRole:\n        Ref: EMRServiceRole\n      Tags:\n      - Key: Name\n        Value: EMR Sample Cluster\n      VisibleToAllUsers: 'true'\n    Type: AWS::EMR::Cluster\n  EMRSecurityConfiguration:\n    Properties:\n      Name: EMRSampleClusterSecurityConfiguration\n      SecurityConfiguration:\n        EncryptionConfiguration:\n          AtRestEncryptionConfiguration:\n            LocalDiskEncryptionConfiguration:\n              AwsKmsKey: arn:aws:kms:us-east-1:123456789012:key/1234-1234-1234-1234-1234\n              EncryptionKeyProviderType: AwsKms\n            S3EncryptionConfiguration:\n              AwsKmsKey: arn:aws:kms:us-east-1:123456789012:key/1234-1234-1234-1234-1234\n              EncryptionMode: SSE-KMS\n          EnableAtRestEncryption: 'true'\n          EnableInTransitEncryption: 'true'\n          InTransitEncryptionConfiguration:\n            TLSCertificateConfiguration:\n              CertificateProviderType: PEM\n              S3Object: s3://MyConfigStore/artifacts/MyCerts.zip\n    Type: AWS::EMR::SecurityConfiguration\n  EMRServiceRole:\n    Properties:\n      AssumeRolePolicyDocument:\n        Statement:\n        - Action:\n          - sts:AssumeRole\n          Effect: Allow\n          Principal:\n            Service:\n            - elasticmapreduce.amazonaws.com\n      ManagedPolicyArns:\n      - arn:aws:iam::aws:policy/service-role/AmazonElasticMapReduceRole\n    Type: AWS::IAM::Role</code></pre> \n</div> \n<h2>Summary</h2> \n<p>In this post, we provide a set of best practices to consider and follow when securing your EMR clusters. If you have questions or suggestions, leave a comment.</p> \n<p> </p> \n<hr>\n<h3>Additional Reading</h3> \n<p>If you found this post useful, be sure to check out <a href=\"https://aws.amazon.com/blogs/big-data/restrict-access-to-your-aws-glue-data-catalog-with-resource-level-iam-permissions-and-resource-based-policies/\" target=\"_blank\" rel=\"noopener\">Restrict access to your AWS Glue Data Catalog with resource-level IAM permissions and resource-based policies</a>, <a href=\"https://aws.amazon.com/blogs/big-data/implementing-authorization-and-auditing-using-apache-ranger-on-amazon-emr/\" target=\"_blank\" rel=\"noopener\">Implementing Authorization and Auditing using Apache Ranger on Amazon EMR</a>, and <a href=\"https://aws.amazon.com/blogs/big-data/secure-amazon-emr-with-encryption/\" target=\"_blank\" rel=\"noopener\">Secure Amazon EMR with Encryption</a>.</p> \n<p> </p> \n<hr>\n<h3>About the Authors</h3> \n<p><img class=\"size-full wp-image-6169 alignleft\" src=\"https://d2908q01vomqb2.cloudfront.net/b6692ea5df920cad691c20319a6fffd7a4a766b8/2018/12/13/TonyNguyen.png\" alt=\"\" width=\"113\" height=\"154\"><strong>Tony Nguyen is a Senior Consultant with AWS Professional Services. </strong>His specialty is in data and analytics, focused on helping public sector customers with their unique data challenges. He works directly with customers to design, architect, and implement big data and analytics solutions on AWS. When he’s not eyeballs-deep in data, he enjoys playing volleyball, cooking, and occasionally fooling himself into thinking that he’s a half-decent photographer.</p> \n<p> </p> \n<p> </p> \n<p><img class=\"alignleft size-full wp-image-2812\" src=\"https://d2908q01vomqb2.cloudfront.net/b6692ea5df920cad691c20319a6fffd7a4a766b8/2017/08/09/aaron_friedman_100.jpg\" alt=\"\" width=\"100\" height=\"124\"><strong><a href=\"https://aws.amazon.com/blogs/big-data/author/ajfriedm/\" target=\"_blank\" rel=\"noopener noreferrer\">Dr. Aaron Friedman</a> is a Healthcare and Life Sciences Partner Solutions Architect at Amazon Web Services</strong>. He works with ISVs and SIs to architect healthcare solutions on AWS, and bring the best possible experience to their customers. His passion is working at the intersection of science, big data, and software. In his spare time, he’s exploring the outdoors, learning a new thing to cook, or spending time with his wife and his dog, Macaroon.</p>\n",
      "enclosure": {},
      "categories": [
        "Amazon EMR"
      ]
    }
  ]
}